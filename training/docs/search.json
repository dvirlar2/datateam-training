[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Get a tour of the office from Ginger\nFill out required paperwork from Ana (room 325)\nSchedule a headshot with Alex (room 326)\nSet up the remainder of your accounts\n\n\n\n\n\nLDAP - this should be set up prior to your start date in order to help get other accounts set. This account and password control your access to:\n\narcticdata RT queue\nGitHub Enterprise - arctic-data\n\nDatateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal\nORCiD - create an account\n\nLogin to test.arcticdata.io with your ORCID iD\n\nNCEAS Slack - get an invite from slack.nceas.ucsb.edu\n\nChannels to join: #arctica, #arcticbot, #computing, #datateam, #devteam, #social\nIntroduce yourself in #datateam and then use that channel to ask Jeanette to make you an administrator on test.arcticdata.io\n\nArctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request addition to the admin group from Jeanette in Slack\nGitHub - if you do not have a public GitHub account already, please register for one here\nIf you are an intern, fill out anticipated quarterly schedule on the intern google calendar shared with you.\nElectronic Timekeeping - make sure you can log on to electronic timekeeping via your UCSBNetID and password (may not be accessible on the first day, if you continue to have issues please let Ana know). If you are an hourly employee, log your hours for your first day! Under today’s date select ‘Hours Worked’ under the Pay Code column, enter the amount of hours under the Amount column, and finally click the ‘Save’ button in the top right. At the end of every two-week pay period you will also need to click the ‘Approve Timecard’ button in order to submit your timecard.\nDetailed Instructions\nLet Jeanette or Daphne know what email you would like to use for general NCEAS updates from all@nceas.ucsb.edu\n\n\n\n\nNCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the recurring events are:\n\nRoundtable\n\nweekly presentation and discussion of research by a visiting or local scientist\nWednesdays at 12:15 in the lounge\n\nCoffee Klatch\n\ncoffee, socializing, and news updates for NCEAS\nTuesdays at 10:30 in the lounge\n\n\n\n\n\nCheck out their individual calendar entries and channels for more information\n\nEarly Career Researcher Community Forum - #ecr_community\nHacky Hours - #hackyhour\nData Science Chats - #data-science-chats\nNCEAS Diversity Team\nNCEAS Book Club - #bookclub\n\n\n\n\nAs an intern with the data team, there are a few expectations that the Project Coordinators have of you. Overall, we expect you to be communicative and proactive. We want you to learn and grow in this position, but we don’t want you spinning your wheels going nowhere fast! If you’ve spent 10-15 minutes on an issue and you’re not making any progress, reach out to us and your peers for help in the #datateam slack channel. The #datateam slack channel is the main form of communication, and we expect all interns to become comfortable communicating in this space.\nAdditionally, we expect interns to work within the standard business hours of 8am - 5pm (pacific time). We ask that you mark your expected work hours on the shared “Intern” Google Calendar. This is so that the Project Coordinators are aware of who’s working day-to-day and can plan their days accordingly. We also use this to verify time sheets when they are submitted. Ideally, interns would input their proposed hours on the calendar at least one week in advance. During exams and other unusually busy weeks at school, we understand you may need to shift your hours or reduce your workload. When this occurs, please make sure to email either Daphne or Jeanette so that we know not to expect you during your usual schedule.\n\n\n\nTake a moment to review the diversity and inclusion page and code of conduct at NCEAS so that we can foster an environment that is safe, welcoming and inclusive for everyone.\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nClark, Jeanette, Jesse Goldstein, Dominic Mullen, Irene Steves, Mitchell Maier, Stephanie Freund, Sharis Ochs, Bryce Mecum, Peter Slaughter, Emily O’Dean, Jasmine Lai, Daphne Virlar-Knight. 2022 Training Materials for the Arctic Data Center Curation Team. Arctic Data Center. doi:10.18739/A20G3GX8W."
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "1  Introduction to open science",
    "section": "",
    "text": "These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. It also provides an overview on the tools we will be using (remote servers, Rstudio, R, Troubleshooting, Exercises) throughout the training. This document is meant to take multiple days to complete depending on your previous knowledge on some of the topics.\nIf you see anything that needs fixing, submit a issue in the  github issues"
  },
  {
    "objectID": "01_introduction.html#background-reading",
    "href": "01_introduction.html#background-reading",
    "title": "1  Introduction to open science",
    "section": "1.1 Background reading",
    "text": "1.1 Background reading\nRead the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC. We encourage you to follow the links within these pages to gain a deeper understanding.\n\nabout\nsubmission\npreservation\nhistory"
  },
  {
    "objectID": "01_introduction.html#effective-data-management",
    "href": "01_introduction.html#effective-data-management",
    "title": "1  Introduction to open science",
    "section": "1.2 Effective data management",
    "text": "1.2 Effective data management\nRead Matt Jones et al.’s paper on effective data management to learn how we will be organizing datasets prior to archival.\n(Please note that while the tips outlined in this article are best practices, we often do not reformat data files submitted to our repositories unless necessary. It is best to be conservative and not alter other people’s data without good reason.)\nYou may also want to explore the DataONE education resources related to data management."
  },
  {
    "objectID": "01_introduction.html#using-dataone",
    "href": "01_introduction.html#using-dataone",
    "title": "1  Introduction to open science",
    "section": "1.3 Using DataONE",
    "text": "1.3 Using DataONE\nData Observation Network for Earth (DataONE) is a community driven initiative that provides access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data.\nRead more about what DataONE is here and about DataONE member node (MN) guidelines here. Please feel free to ask Jeanette any questions you have about DataONE.\nWe will be applying these concepts in the next chapter."
  },
  {
    "objectID": "01_introduction.html#working-on-a-remote-server",
    "href": "01_introduction.html#working-on-a-remote-server",
    "title": "1  Introduction to open science",
    "section": "1.4 Working on a remote server",
    "text": "1.4 Working on a remote server\nAll of the work that we do at NCEAS is done on our remote server, datateam.nceas.ucsb.edu. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet.\nWe access RStudio on our server through this link. This is the same as your desktop version of RStudio with one main difference is that files are on the server. Please do all your work here. This way you can share your code with the rest of us.\n\n\n\n\n\n\nNote\n\n\n\nIf you R session is frozen and unresponsive check out the guide on how to fix it."
  },
  {
    "objectID": "01_introduction.html#a-note-on-paths",
    "href": "01_introduction.html#a-note-on-paths",
    "title": "1  Introduction to open science",
    "section": "1.5 A note on paths",
    "text": "1.5 A note on paths\nOn the servers, paths to files in your folder always start with /home/yourusername/....\nNote - if you are a more advanced user, you may use the method you prefer as long as it is evident where your file is from.\nWhen you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another data team member needs to run your script, it is not dependent on a working directory."
  },
  {
    "objectID": "01_introduction.html#a-note-on-r",
    "href": "01_introduction.html#a-note-on-r",
    "title": "1  Introduction to open science",
    "section": "1.6 A note on R",
    "text": "1.6 A note on R\nThis training assumes basic knowledge of R and RStudio. If you want a quick R refresher, walk through Jenny Bryan’s excellent materials here.\nThroughout this training we will occasionally use the namespace syntax package_name::function_name() when writing a function. This syntax denotes which package a function came from. For example dataone::getSystemMetadata selects the getSystemMetadata function from the dataone R package. More detailed information on namespaces can be found here."
  },
  {
    "objectID": "01_introduction.html#a-note-on-effective-troubleshooting-in-r",
    "href": "01_introduction.html#a-note-on-effective-troubleshooting-in-r",
    "title": "1  Introduction to open science",
    "section": "1.7 A note on effective troubleshooting in R",
    "text": "1.7 A note on effective troubleshooting in R\nWe suggest using a combination of minimal reproducible examples (MRE) and the package reprex to create reproducible examples. This will allow others to better help you if we can run the code on our own computers.\nA MRE is stripping down your code to only the parts that cause the bug.\nHow to generate a reprex:\n\ncopy the code you want to ask about\ncall reprex()\nfix until everything runs smoothly\ncopy the result to ask your question\n\nWhen copy and paste code slack message or github issues, use three backticks for code blocks and two backticks for a small piece of code will prevent issues with slack formats quotation.\nFor more information and examples check out more of Jenny Bryan’s slides or watch the video starting at about the 10 min mark.\nNote for EML related MREs: - Generating a reprex for these situations (ie. tokens) might be complicated but you can should still follow the MRE principles even if the reprex won’t render fully - You can include a minimal EML to avoid some get_package issues:\nme <- list(individualName = list(givenName = \"Jeanette\", surName = \"Clark\"))\n\nattributes <- data.frame(attributeName = 'length_1',\n                         attributeDefinition = 'def1',\n                         measurementScale = 'ratio',\n                         domain = 'numericDomain',\n                         unit = 'meter',\n                         numberType = 'real')\n\natt_list <- set_attributes(attributes)\n\n\ndoc_ex <- list(packageId = \"id\", system = \"system\", \n            dataset = list(title = \"A Mimimal Valid EML Dataset\",\n                           creator = me,\n                           contact = me,\n                           dataTable = list(entityName = \"data table\", attributeList = att_list))\n)"
  },
  {
    "objectID": "01_introduction.html#a-note-on-exercises",
    "href": "01_introduction.html#a-note-on-exercises",
    "title": "1  Introduction to open science",
    "section": "1.8 A note on Exercises",
    "text": "1.8 A note on Exercises\nThe rest of the training has a series of exercises. These are meant to take you through the process as someone submitting a dataset from scratch. This is slightly different than the usual workflow but important in understanding the underlying system behind the Arctic Data Center.\nPlease note that you will be completing everything on the  site for the training. In the future if you are unsure about doing anything with a dataset. The test site is a good place to try things out!"
  },
  {
    "objectID": "01_introduction.html#exercise-1",
    "href": "01_introduction.html#exercise-1",
    "title": "1  Introduction to open science",
    "section": "1.9 Exercise 1",
    "text": "1.9 Exercise 1\nThis part of the exercise walks you through submitting data through the web form on “test.arcticdata.io”\n\n1.9.1 Part 1\n\nDownload the csv of Table 1 from this paper.\nReformat the table to meet the guidelines outlined in the journal article on effective data management (this might be easier to do in an interactive environment like Excel).\nNote - we usually don’t edit the content in data submissions so don’t stress over this part too much\n\n\n\n1.9.2 Part 2\n\nGo to “test.arcticdata.io” and submit your reformatted file with appropriate metadata that you derive from the text of the paper:\n\nlist yourself as the first ‘Creator’ so your test submission can easily be found,\nfor the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine,\ndirectly copying and pasting sections from the paper (abstract, methods, etc.) is also fine,\nattributes (column names) should be defined, including correct units and missing value codes.\nsubmit the dataset"
  },
  {
    "objectID": "08_using_git.html",
    "href": "08_using_git.html",
    "title": "8  Using Git",
    "section": "",
    "text": "We use git and Github to manage our packages (ie datamgmt, arcticdatautils) and even this document!"
  },
  {
    "objectID": "08_using_git.html#what-is-git",
    "href": "08_using_git.html#what-is-git",
    "title": "8  Using Git",
    "section": "8.2 What is Git?",
    "text": "8.2 What is Git?\nGit is a distributed version control system.\nImportant! If you have never used Git before, or only used it a little, or have no idea what it is, check out this intro to Git put together by the ecodatascience group at UCSB. Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. After you have read through that presentation, come back to this chapter.\n\n8.2.1 So why do I need to use this again?\nThere are several reasons why using the arctic-data GitHub repository is helpful, both for you and for the rest of the data team. Here are a few:\n\nVersioning: Did you accidentally make a change to your code and can’t figure out why it broke? Do you wish you could go back to the version that worked? If you add your code to the GitHub repo you can do this!\nReproducibility: Being able to reproduce how you accomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. As a data center, this is especially important for us with data team members that stay for 6-12 months because we may need to go back and figure out how something was done after the intern or fellow who wrote the code left the team.\nTroubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for others to help you troubleshoot your code if it is on the GitHub repo. We can view, troubleshoot, and fix bugs very easily when code is on the GitHub repo, with the added bonus of being able to go back a version if something should break.\nSolve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on GitHub, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch."
  },
  {
    "objectID": "08_using_git.html#setting-up-git",
    "href": "08_using_git.html#setting-up-git",
    "title": "8  Using Git",
    "section": "8.3 Setting up Git",
    "text": "8.3 Setting up Git\nBefore using git, you need to tell it who you are. The only way to do this is through the command line. When you open RStudio, you should see a Terminal tab located to the right of the Console tab. If the Terminal tab is not visible, you can open it by selecting Tools -> Terminal -> New Terminal at the top of your RStudio window.\nTo tell git who you are, you’re going to set the global options. This includes setting your name, email address, PAT (Personal Access Token), and caching the PAT. Make sure you run the following commands one at a time.\nType the following command in the Terminal window, with your actual name, and press enter. If you do this correctly, it will look as though nothing happened.\n\ngit config --global user.name \"Your Name\"\n\nNext, enter the following line, with the email address associated with your GitHub account.\n\ngit config --global user.email MyEmail@domain.com\n\nAfter running these commands, the Terminal window should look like this: \nNext, we will tell Git to store your PAT. We must do this because of the way our server operating system handles credentials. If you don’t run the next line, your PAT will expire immediately on the server, even though we will set it up on GitHub to be valid for 90 days.\n\ngit config --global credential.helper 'cache -- timeout=10000000'\n\nFinally, check to make sure everything looks correct by entering the following command, which will return the options that you have set.\n\ngit config --global --list"
  },
  {
    "objectID": "08_using_git.html#github-authentication",
    "href": "08_using_git.html#github-authentication",
    "title": "8  Using Git",
    "section": "8.4 GitHub Authentication",
    "text": "8.4 GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. That’s where the PAT comes in.\nIn the tabs at the bottom of your RStudio window, make sure to switch from the Terminal to the Console. Now that, you’ve done that, follow these steps:\n\nRun usethis::create_github_token()\nIn the browser window that pops up, log into GitHub\nIn the “Expiration” drop down menu, select “90 days”\nClick “Generate token” in the green box at the bottom of the page\nOn the next page, copy the token from the green box\nBack in RStudio, run credentials::set_github_pat()\nPaste your token into the dialog box that pops up\n\nYou’ve now successfully set your PAT in RStudio, and connected R, Git, and GitHub. Great job! Now, you’re going to clone the repository we work in so that you can version control your code.\n\n8.4.1 Cloning the arctic-data repo\nTo clone the arctic-data repository, navigate to the repository on GitHub: https://github.nceas.ucsb.edu/KNB/arctic-data/.\nIf you are not already logged on, this will be the enterprise account you use to log in when you go to RT. If you have trouble with this, send a message to Nick Outin (@Nick) in the datateam slack channel.\nOnce you are logged on and can view the above link, click on the green “Code” button, and copy the URL in the gray box.\nIn your RStudio window, click File -> New Project -> Version Control -> Git. Paste the URL you just copied into the “Repository URL” box, and press tab on your keyboard. This should autofill the “Project directory name.” If it does not, then type in arctic-data. Fill it out as shown in the image below to clone the arctic-data repository into the top level of your home directory. Click “Create Project,” and now you should have a directory called arctic-data in your RStudio files window.\nIf you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save’."
  },
  {
    "objectID": "08_using_git.html#working-with-the-repository",
    "href": "08_using_git.html#working-with-the-repository",
    "title": "8  Using Git",
    "section": "8.5 Working with the repository",
    "text": "8.5 Working with the repository\n\n8.5.1 Adding a new script\nIf you have been working on a script that you want to put in the arctic-data GitHub repo, you first need to save it somewhere in the arctic-data folder you cloned to your account on the Datateam server. You can do this by either moving your script into the folder or using the save-as functionality. Note that Git will try and version anything that you save in this folder, so you should be careful about what you save here. For our purposes, things that probably shouldn’t be saved in this folder include:\n\nTokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token in GitHub.\nData files: Git does not version data files very well. You shouldn’t save any .csv files or any other data files (including metadata).\nWorkspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory.\nPlots/Graphics: For the same reasons as data files.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your Git tab, ask for help before committing.\n\n\nAfter you save your script in the appropriate place within the arctic-data folder, it will show up in your Git tab looking like this: \nBefore you commit your changes, you need to click the little box under “Staged”. Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), write your commit message. Always include a commit message. Remember that the commit message should be a concise description of the changes that were made to a file. Your window should look like this: \nPush ‘Commit’, and your commit will be saved to your local repository (this will not push it to the remote repository, yet). Now you want to merge the commits you made with the master version of the remote repository. You do this by using the command “Push.” But before you push, you always need to pull first to avoid merge conflicts. Pulling will merge the current version of the remote repository with your local repository, on your local machine. Click “Pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “Push”.\nAlways remember, the order is commit-pull-push.\n\n\n8.5.2 Editing a script\nIf you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the GitHub website. It is much easier to accidentally overwrite the history of a file this way.\nOne thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done”. Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40”.\nOne final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your Git tab in RStudio that looks like this: \n\n\n8.5.3 Where do I commit?\nThe default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with sub-folders listed by project. Directories can be created as needed but please ask Dom or Jesse first so we can try and maintain some semblance of order in the file structure.\nThe Github workflow diagram from stamminator shows each step and their functions explicitly."
  },
  {
    "objectID": "08_using_git.html#my-git-tab-disappeared",
    "href": "08_using_git.html#my-git-tab-disappeared",
    "title": "8  Using Git",
    "section": "8.6 My Git tab disappeared",
    "text": "8.6 My Git tab disappeared\nSometimes R will crash so hard it loses your project information, causing your Git tab to disappear. Most likely, RStudio has just closed your “project” and all you need to do is reopen it. If your Git tab has disappeared, in the top right of your RStudio session, you should see a little R logo with “Project: (None)” next to it. This means you do not currently have a project open. Clicking the arrow should give you a dropdown menu of recent projects, where you can select “arctic-data” or “sasap-data.” Once you have opened your project, the Git tab should reappear! This is also a convenient way to switch between projects if you are working in multiple repositories."
  },
  {
    "objectID": "09_first_ticket.html",
    "href": "09_first_ticket.html",
    "title": "9  First Ticket",
    "section": "",
    "text": "After completing the previous chapters, Daphne or Jeanette will assign a ticket from RT. Login using your LDAP credentials got get familiarized with RT."
  },
  {
    "objectID": "09_first_ticket.html#navigate-rt",
    "href": "09_first_ticket.html#navigate-rt",
    "title": "9  First Ticket",
    "section": "9.1 Navigate RT",
    "text": "9.1 Navigate RT\nThe RT ticketing system is how we communicate with folks interacting with the Arctic Data Center.\nWe use it for managing submissions, accessing issues, etc. It consists of three separate interfaces:\nFront Page\nAll Tickets\nTicket Page\n\n9.1.1 Front page\n\nThis is what you see first\n\nHome - brings you to this homepage\n\nTickets - to search for tickets (also see number 5)\n\nTools - not needed\n\nNew Ticket - create a new ticket\n\nSearch - Type in the ticket number to quickly navigate to a ticket\n\nQueue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses\n\n\n\nNew = unopened tickets that require attention\n\nOpen = tickets currently open and under investigation and/or being processed by a support team member\n\nStalled = tickets awaiting responses from the PI/ submitter\n\n\n\nTickets I Own - These are the current open tickets that are claimed by me\n\nUnowned Tickets - Newest tickets awaiting claim\n\nTicket Status - Status and how long ago it was created\n\nTake - claim the ticket as yours\n\n\n\n9.1.2 All tickets\n\nThis is the queue interface from number 6 of the Front page\n1. Ticket number and title\n2. Ticket status\n3. Owner - who has claimed the ticket\n\n\n9.1.3 Example ticket\n\n\nTitle - Include the PI’s name for reference\n\nDisplay - homepage of the ticket\n\nHistory - Comment/Email history, see bottom of Display page\n\nBasics - edit the title, status, and ownership here\n\nPeople - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters\n\nLinks - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number\n\n\n\n\n\n\n\nWarning\n\n\n\nVerify that this is indeed the two tickets you want to merge. It is non-reversible.\n\n\n\nActions\n\n\n\nReply - message the submitter/ PI/ all watchers\n\nComment - attach internal message (no submitters, only Data Teamers)\n\nOpen It - Open the ticket\n\nStall - submitter has not responded in greater than 1 month\n\nResolve - ticket completed\n\n\n\nHistory - message history and option to reply (to submitter and beyond) or comment (internal message)\n\n\n\n9.1.4 New data submission\nWhen notified by Arcticbot about a new data submission, here are the typical steps:\n\nUpdate the Requestor under the People section based on the email given in the submission (usually the user/ PI/ submitter). You may have to google for the e-mail address if the PI did not include it in the metadata record.\nTake the ticket (Actions > Take)\nReview the submission based on the checklist\nDraft an email using the template and let others review it via Slack\nSend your reply via Actions\n\nBefore opening a R script first look over the initial checklist first to identify what you will need to update in the metadata."
  },
  {
    "objectID": "09_first_ticket.html#initial-review-checklist",
    "href": "09_first_ticket.html#initial-review-checklist",
    "title": "9  First Ticket",
    "section": "9.2 Initial review checklist",
    "text": "9.2 Initial review checklist\nBefore responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed.\n\n9.2.0.1 Sensitive Data\nIf any of the below is in the dataset, please alert the #arctica team know before proceeding.\n\nCheck if there is any sensitive information or personal identifying information in the data (eg. Names)\nCan the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers)\nDryad Human Subject data guidelines can be a good place to start\n\nCommon Cases:\n\nSocial Science: Any dataset involving human subjects (may include awards awarded by ASSP and topics such as COVID-19)\nArchaeology: archaeological site location information, which is protected from public access by law\nBiology: protected species location coordinates\n\n\n\n9.2.0.2 Data citations\n\nIf the dataset appears to be in a publication please (might be in the abstract) make sure that those citations are registered.\n\n\n\n9.2.0.3 Title\n\nWHAT, WHERE, and WHEN:\n\nIs descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage\nProvides a location of the work from the local to state or country level\nProvides a time frame of the work\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\n\n\n\n\n9.2.0.4 Abstract\n\nDescribes the DATA as well as:\n\nThe motivation (purpose) of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\nAt least 100 words total\n\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\nAny citations to papers can be registered with us\n\n\n\n\n9.2.0.5 Keywords\n\nSome keywords are included\n\n\n\n9.2.0.6 Data\n\nData is normalized (if not suggest to convert the data if possible)\nAt least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program)\nNo xls/xlsx files (or other proprietary files)\nFile contents and relationships among files are clear\nEach file is well NAMED and DESCRIBED and clearly differentiated from all others\nAll attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. Quality control all dimensionless units.\nMissing value codes are explained (WHY are the data absent?)\nIf it is a .rar file  -> scan the file\nIf there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already\n\n\n\n9.2.0.7 People & Parties\n\nAt least one contact and one creator with a name, email address, and ORCID iD\n\n\n\n9.2.0.8 Coverages\n\nIncludes coverages that make sense\n\nTemporal coverage - Start date BEFORE end date\nGeologic time scales are added if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic)\nSpatial coverage matches geographic description (check hemispheres)\nGeographic description is from the local to state or country level, at the least\nTaxonomic coverage if appropriate\n\n\n\n\n9.2.0.9 Project Information\n\nAt least one FUNDING number\nTitle, personnel, and abstract match information from the AWARD (not from the data package)\n\n\n\n9.2.0.10 Methods\n\nThis section is REQUIRED for ALL NSF-FUNDED data packages\nEnough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources\n\n\n\n9.2.0.11 Portals\n\nIf there are multiple submissions from the same people/project let them know about the portals feature\nIf this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette and see the data portals section."
  },
  {
    "objectID": "09_first_ticket.html#processing-templates",
    "href": "09_first_ticket.html#processing-templates",
    "title": "9  First Ticket",
    "section": "9.3 Processing templates",
    "text": "9.3 Processing templates\nWe have developed some partially filled R scripts to get you started on working on your first dataset. They outline common functions used in processing a dataset. However, it will differ depending on the dataset.\nYou can use this template where you can fill in the blanks to get familiar with the functions we use and workflow at first. We also have a more minimal example A filled example as a intermediate step. You can look at the filled example if you get stuck or message the #datateam.\nOnce you have updated the dataset to your satisfaction and reviewed the Final Checklist, post the link to the dataset on #datateam for peer review."
  },
  {
    "objectID": "09_first_ticket.html#final-checklist",
    "href": "09_first_ticket.html#final-checklist",
    "title": "9  First Ticket",
    "section": "9.4 Final Checklist",
    "text": "9.4 Final Checklist\nYou can click on the assessment report on the website to for a general check. Fix anything you see there.\nSend the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive):\n\n9.4.1 Special Datasets\nPlease refer to the dedicated pages for instructions to handle these cases:\n\nMOSAiC\nDBO\n\n\n\n9.4.2 System Metadata\nthe format ids are correct\n\n\n9.4.3 General EML\nIncluded lines for FAIR: ::: {.cell}\ndoc <- eml_add_publisher(doc)\ndoc <- eml_add_entity_system(doc)\n::: ### Title - No abbreviations, should include geographic and temporal coverage\n\n\n9.4.4 Abstract\n\nlonger than 100 words\nno abbreviations or garbled text\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\n\n\n9.4.5 DataTable / OtherEntity / SpatialVectors\n\nin the correct one: DataTable / OtherEntity / SpatialVector / SpatialRaster for the file type\nentityDescription - longer than 5 words and unique\nphysical present and format correct\n\n\n9.4.5.1 Attribute Table\n\ncomplete\nattributeDefinitions longer than 3 words\nVariables match what is in the file\nMeasurement domain - if appropirate (ie dateTime correct)\nMissing Value Code - accounted for if applicable\nSemantic Annotation - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc.\n\n\n\n\n9.4.6 People\n\ncomplete information for each person in each section\n\nincluding ORCID and e-mail address for all contacts\npeople repeated across sections should have consistent information\n\n\n\n\n9.4.7 Geographic region\n\nthe map looks correctand matches the geographic description\ncheck if negatives (-) are missing\n\n\n\n9.4.8 Project\n\nif it is an NSF award you can use the helper function:\n\ndoc$dataset$project <- eml_nsf_to_project(awards)\n\nfor other awards that need to be set manually, see the set project page\n\n\n\n9.4.9 Methods\n\npresent\nno garbled text\n\n\n\n9.4.10 Check EML Version\n\ncurrently using: eml-2.2.0 (as of July 30 2020)\nreview to see if the EML version is set correctly by reviewing the doc$`@context` that it is indeed 2.2.0 under eml\nRe-run your code again and have the lineemld::eml_version(\"eml-2.2.0\") at the top\nMake sure the system metadata is also 2.2.0\n\n\n\n9.4.11 Access\n\nGranted access to PI using set_rights_and_access()\n\nmake sure it is http:// (no s)\n\nnote if it is a part of portals there might be specific access requirements for it to be visible using set_access()\n\n\n\n9.4.12 SFTP Files\n\nif there are files transferred to us via SFTP, delete those files when the ticket is resolved\n\n\n\n9.4.13 Updated datasets\nAll the above applies. These are some areas to do a closer check when users update with a new file:\n\nNew data was added\n\nTemporal Coverage and Title\nand follow usual protocols\n\nFiles were replaced\n\nupdate physical and entityName\ndouble-check attributes are the same\ncheck for any new missing value codes that should be accounted for\n\nWas the dataset published before 2021?\n\nupdate project info , annotations\n\nGlance over entire page for any small mistakes (ie. repeated additionalMetadata, any missed &amps, typos)\n\nAfter all the revisions send the link to the PI in an email through RT. Send the draft of the email to Daphne or Jeanette on Slack."
  },
  {
    "objectID": "09_first_ticket.html#email-templates",
    "href": "09_first_ticket.html#email-templates",
    "title": "9  First Ticket",
    "section": "9.5 Email templates",
    "text": "9.5 Email templates\nThis section covers new data packages submitted. For other inquiries see the PI FAQ templates\nPlease think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible.\nIn your first few months, please run email drafts by the #datateam Slack and get approval before sending.\nRemember to consult the submission guidelines for details of what is expected.\nQuick reference:\n\nInitial email template\nFinal email templates\nAdditional email template\n\n\n\n\n\n9.5.1 Initial email template\n\nHello [NAME OF REQUESTOR], Thank you for your recent submission to the NSF Arctic Data Center!\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n[COMMENTS HERE]\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\nBest,\n[YOUR NAME]\n\n\n\n9.5.2 Comment templates based on what is missing\n\n9.5.2.1 Portals\nMultiple datasets under the same project - suggest data portal feature\n\nI would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project.\n\nIf they ask to nest the dataset\n\nWe are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email.\n\n\n\n9.5.2.2 Dataset citations\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point.\n\n\n\n9.5.2.3 Title\nProvides the what, where, and when of the data\n\nWe would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’.\n\nDoes not use acronyms\n\nWe wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE]\n\n\n\n9.5.2.4 Abstract\nDescribes DATA in package (ideally > 100 words)\n\nWe would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING]\n\n\nThe motivation of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nAll acronyms are defined\nAt least 100 words long\n\nOffer this if submitter is reluctant to change:\n\nIf you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL].\n\n\n\n9.5.2.5 Keywords\n\nWe noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others.\n\n\n\n9.5.2.6 Data\nSensitive Data\nWe will need to ask these questions manually until the fields are added to the webform.\n\nData sensitivity categories\n\nOnce we have the ontology this question can be asked:\n\nBased on our Data sensitivity categories, which of the 3 does your dataset align with most:\n\n\nNon-sensitive data - None of the data includes sensitive or protected information.\nSome or all data is sensitive with minimal risk - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized.\nSome or all data is sensitive with significant risk - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body.\n\n\nEthical research proceedures\n\n\nWe were wondering if you could also address this question specifically on Ethical Research Procedures: Describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. We can help add your answers to the question to the metadata.\n\nAdding provenance\n\nIs the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2\n\nAt least one data file\n\nWe noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process.\n\nOpen formats\nExample using xlsx. Tailor this reponse to the format in question.\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nThe data files can be replaced by going to the green Edit button > Click the black triangle by the Describe button for the data file > Select Replace (attached is also a screenshot on how to get there). \n\nZip files\n\nExcept for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way.\n\nFile contents and relationships among files are clear\n\nCould you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use.\n\nData layout\n\nWould you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent?\n\nWe try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat.\n\nWe would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\n\n9.5.2.7 Attributes\nIdentify which attributes need additional information. If they are common attributes like date and time we do not need further clarification.\nChecklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets):\n\nA name (often the column or row header in the file).\nA complete definition.\nAny missing value codes along with explanations for those codes.\nFor all numeric data, unit information is needed.\nFor all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”).\nFor text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes.\n\nHelpful templates: > We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] > Could you describe ____? > Please define “XYZ”, including the unit of measure. > What are the units of measurement for the columns labeled “ABC” and “XYZ”?\nMissing value codes\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf.\n\n\nWe noticed that the data files contain [blank cells - replace with missing values found]. What do these represent?\n\n\n\n9.5.2.8 Funding\nAll NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization.\n\nWe noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations.\n\n\n\n9.5.2.9 Methods\nWe noticed that methods were missing from the submission. Submissions should include the following:\n\nprovide instrument names (if applicable)\nspecify how sampling locations were chosen\nif citations for sampling methods are used, please provide a brief summary of the methods referenced\nany software used to process the data\n\nNote - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software)\n\nYour methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.]\n\n\nComprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable.\n\nA full example - New Submission: methods, excel to csv, and attributes\n\nThank you for your recent submission to the NSF Arctic Data Center!\n\n\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point.\n\n\nWe would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records.\n\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nWe also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. \n\n\nOur data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice.\n\n\nWe noticed that methods were missing from the submission. Submissions should: - provide instrument names (if applicable) - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail - any software used to process the data\n\n\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n\n\nBest,\n\n\nName\n\n\n\n\n9.5.3 Final email templates\n\n9.5.3.1 Asking for approval\n\nHi [submitter],\n\n\nI have updated your data package and you can view it here after logging in: [URL]\n\n\nPlease review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI.\n\n\nAfter publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version.\n\n\nPlease let us know if you have any questions.\n\n\n\n9.5.3.2 DOI and data package finalization comments\nReplying to questions about DOIs\n\nWe attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version.\n\n\nDOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\nClarification of updating with a DOI and version control\n\nWe definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it).\n\n\n\n9.5.3.3 Resolve the ticket\nSending finalized URL and dataset citation before resolving ticket\n[NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)]\n\nHere is the link and citation to your finalized data package:\n\n\nhttps://doi.org/10.18739/A20X0X\n\n\nFirst Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X.\n\n\nIf in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\nPlease let us know if you need any further assistance.\n\n\n\n\n9.5.4 Additional email templates\n\n9.5.4.1 Deadlines\nIf the PI is checking about dates/timing: > [give rough estimate of time it might take] > Are you facing any deadlines? If so, we may be able to expedite publication of your submission.\n\n\n9.5.4.2 Pre-assigned DOI\nIf the PI needs a DOI right away:\n\nWe can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\n\n9.5.4.3 Sensitive Data\n\nWhich of the following categories best describes the level of sensitivity of your data?\n\n\nA. Non-sensitive data None of the data includes sensitive or protected information. Proceed with uploading data. B. Some or all data is sensitive but has been made safe for open distribution Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Proceed with uploading data, but ensure that only data that are safe for public distribution are uploaded. Address questions about anonymization, aggregation, de-identification, and data embargoes with the data curation support team before uploading data. Describe these approaches in the Methods section. C. Some or all data is sensitive and should not be distributed The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Do NOT upload sensitive data. You should still upload a metadata description of your dataset that omits all sensitive information to inform the community of the dataset’s existence. Contact the data curation support team about possible alternative approaches to safely preserve sensitive or protected data.\n\n\n\nEthical Research Procedures. Please describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Institutional Review Board approvals, consent waivers, procedures for co-production, data sovereignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution.\n\n\n\n\n9.5.4.4 Asking for dataset access\n\nAs a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets.\n\n\n\n9.5.4.5 No response from the researcher\nPlease email them before resolving a ticket like this:\n\nWe are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email.\n\n\n\n9.5.4.6 Recovering Dataset submissions\n\nTo recover dataset submissions that were not successful please do the following:\n\n\n\nGo to https://arcticdata.io/catalog/drafts\nFind your dataset and download the corresponding file\nSend us the file in an email\n\n\n\n\n9.5.4.7 Custom Search Link\n\nYou could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=“your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER\n\n\n\n9.5.4.8 Adding metadata via R\n\nKNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R.\n\n\nHere are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")\n\n\nIf you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well.\n\n\n\n9.5.4.9 Finding multiple data packages\n\nIf linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533\n\n\n\n9.5.4.10 NSF ARC data submission policy\n\nPlease find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp.\n\n\nInvestigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used.\n\n\n\n9.5.4.11 Linking ORCiD and LDAP accounts\n\nFirst create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage.\n\n\nAfter completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface.\n\n\nOr, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage.\n\nOnce the dataset is approved by the PI and there are no further changes, publish the dataset with a doi. ::: {.cell}\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n:::"
  },
  {
    "objectID": "09_first_ticket.html#categorize-datasets",
    "href": "09_first_ticket.html#categorize-datasets",
    "title": "9  First Ticket",
    "section": "9.6 Categorize datasets",
    "text": "9.6 Categorize datasets\nAs a final step we will categorize the dataset you processed. We are trying to categorize datasets so we can have a rough idea of what kinds of datasets we have at the Arctic Data Center. We will grant you access to the google sheet that has all of the categorized datasets\nWe will categorize each dataset into one of the predefined themes (ie. biology, ecology etc.). Definition of the themes can be found in the google sheet\nRun the following line with your doi and themes as a list.\n\ndatamgmt::categorize_dataset(\"your_doi\", c(\"list\", \"of\", \"themes\"), \"your name\")"
  },
  {
    "objectID": "09_first_ticket.html#congrats",
    "href": "09_first_ticket.html#congrats",
    "title": "9  First Ticket",
    "section": "9.7 Congrats!",
    "text": "9.7 Congrats!\nCongratulations on finishing your first ticket! You can head over to the the repository, data-processing to get your ticket processing code reviewed by the team so we can learn from each other!"
  },
  {
    "objectID": "10_advanced.html",
    "href": "10_advanced.html",
    "title": "10  Beyond your first ticket",
    "section": "",
    "text": "This section is meant to be read after you have processed a couple of tickets and you are comfortable with the workflow with a relatively simple dataset with 1-2 files and want to expand your skills and workflows further."
  },
  {
    "objectID": "10_advanced.html#working-with-large-data-packages",
    "href": "10_advanced.html#working-with-large-data-packages",
    "title": "10  Beyond your first ticket",
    "section": "10.1 Working with large data packages",
    "text": "10.1 Working with large data packages"
  },
  {
    "objectID": "10_advanced.html#add-physicals-to-submissions",
    "href": "10_advanced.html#add-physicals-to-submissions",
    "title": "10  Beyond your first ticket",
    "section": "10.2 Add physicals to submissions",
    "text": "10.2 Add physicals to submissions\nNew submissions made through the web editor will not have any physical sections within the otherEntitys. Add them to the EML with the following script:\n\nfor (i in seq_along(doc$dataset$otherEntity)) {\n    otherEntity <- doc$dataset$otherEntity[[i]]\n    id <- otherEntity$id\n    \n    if (!grepl(\"urn-uuid-\", id)) {\n        warning(\"otherEntity \", i, \" is not a pid\")\n        \n    } else {\n        id <- gsub(\"urn-uuid-\", \"urn:uuid:\", id)\n        physical <- arcticdatautils::pid_to_eml_physical(mn, id)\n        doc$dataset$otherEntity[[i]]$physical <- physical\n    }\n}\n\nAs you can see from code above, we use a for loop here to add physical sections. The for loop is a very useful tool to iterate over a list of elements. With for loop, you can repeat a specific block of code without copying and pasting the code over and over again. When processing datasets in Arctic Data Center, there are many places where for loop can be used, such as publishing a bunch of objects with pids, updating formatID for pkg$data, adding physical section like above code, etc.\nA loop is composed of two parts: the sequence and the body. The sequence usually generates indices to locate elements and the body contains the code that you want to iterate for each element.\nHere is an example of adding the same attributeList for all the dataTables in the metadata using for loop. ::: {.cell}\nattributes <- read.csv('attributes.csv')  # attribute table in csv format\nattributeList <- EML::set_attributes(attributes = attributes)\n\nfor (i in 1:length(doc$dataset$dataTable)) { # sequence part\n    doc$dataset$dataTable[[i]]$attributeList <- attributeList # body part\n}\n:::"
  },
  {
    "objectID": "10_advanced.html#use-references",
    "href": "10_advanced.html#use-references",
    "title": "10  Beyond your first ticket",
    "section": "10.3 Use references",
    "text": "10.3 Use references\nReferences are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including:\n\nMaking it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes)\nReducing the size on disk of EML records with highly redundant information\nFaster read/write/validate with the R EML package\n\nYou may want to use EML references if you have the following scenarios (not exhaustive):\n\nOne person has multiple roles in the dataset (creator, contact, etc)\nOne or more entities shares all or some attributes\n\n\n10.3.1 Example with parties\n\n\n\n\n\n\nNote\n\n\n\nDo not use references for creators as it is used for the citation information. The creators will not show up on the top of the dataset if it is a reference. Until this issue is resolved in NCEAS/metacat#926 we will need to keep this in account.\n\n\nIt’s very common to see the contact and creator referring to the same person with XML like this:\n<eml packageId=\"my_test_doc\" system=\"my_system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1 eml.xsd\">\n  <dataset>\n    <creator>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </contact>\n  </dataset>\n</eml>\nSo you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt.\n\ndoc$dataset$creator[[1]]$id  <- \"reference_id\"\ndoc$dataset$contact <- list(references = \"reference_id\") \nprint(doc)\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<eml:eml xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stmml=\"http://www.xml-cml.org/schema/stmml-1.1\" packageId=\"id\" system=\"system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1/ eml.xsd\">\n  <dataset>\n    <title>A Minimal Valid EML Dataset</title>\n    <creator id=\"reference_id\">\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <references>reference_id</references>\n    </contact>\n  </dataset>\n</eml:eml>\n\n\n\n\n\n\nNote\n\n\n\nThe reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that.\n\n\n\n\n10.3.2 Example with attributes\nTo use references with attributes:\n\nAdd an attribute list to a data table\nAdd a reference id for that attribute list\nUse references to add that information into the attributeLists of the other data tables\n\nFor example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest:\n\ndoc$dataset$dataTable[[1]]$attributeList <- attribute_list\ndoc$dataset$dataTable[[1]]$attributeList$id <- \"shared_attributes\" # use any unique name for your id\n\nfor (i in 2:length(doc$dataset$dataTable)) {\n  doc$dataset$dataTable[[i]]$attributeList <- list(references = \"shared_attributes\") # use the id you set above\n}"
  },
  {
    "objectID": "10_advanced.html#annotations",
    "href": "10_advanced.html#annotations",
    "title": "10  Beyond your first ticket",
    "section": "10.4 Annotations",
    "text": "10.4 Annotations\nIf there are multiple tables with similar annotations you can try something like this:\n\n#go through each dataTable\nfor(i in 1:length(doc$dataset$dataTable)){\n  \n  #get all the attibute names\n  an <- eml_get_simple(doc$dataset$dataTable[[i]], \"attributeName\")\n  \n  #go through the attributes figure out what will match\n  for(a in 1:length(an)){\n    annotation <- dplyr::case_when(\n      # the attributeName to match ~ valueLabel\n      an[[a]] == \"Sample ID\" ~ \"Identity\",\n      an[[a]] == \"Location\" ~ \"study location name\",\n      an[[a]] == \"Latitude\" ~ \"latitude coordinate\",\n      an[[a]] == \"Longitude\" ~ \"longitude coordinate\",\n      an[[a]] == \"Elevation (m)\" ~ \"elevation\",\n      str_detect(an[[a]], \"Depth|depth\") ~ \"Depth\")\n    \n    #only run this code when the annotations match\n    if(!is.na(annotation)){\n      #based on the entity Name create a unique id\n      entity <- str_split(doc$dataset$dataTable[[i]]$entityName, \"_\")\n      doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$id <- paste0(entity[[1]][[1]], \"_\", an[[a]])\n      \n      #add the annotation\n      doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$annotation <- eml_ecso_annotation(annotation)\n    }\n  }\n  \n}"
  },
  {
    "objectID": "10_advanced.html#streamlining-your-workflow",
    "href": "10_advanced.html#streamlining-your-workflow",
    "title": "10  Beyond your first ticket",
    "section": "10.5 Streamlining your workflow",
    "text": "10.5 Streamlining your workflow"
  },
  {
    "objectID": "10_advanced.html#code-snippets",
    "href": "10_advanced.html#code-snippets",
    "title": "10  Beyond your first ticket",
    "section": "10.6 Code Snippets",
    "text": "10.6 Code Snippets\nCode snippets help with templating portions of code that you will be using regularly. To add your own, go to the toolbar ribbon at the top of your Rstudio screen and select:\nTools > Global Options... > Code > Edit Snippets > Add these chunks to the end of the file\nMore info can be found in this blog post by Mara Averick on how to add them: https://maraaverick.rbind.io/2017/09/custom-snippets-in-rstudio-faster-tweet-chunks-for-all/\nUsual arcticdatautils ticket workflow ::: {.cell}\nsnippet ticket\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    rm <- \"add your rm\"\n    pkg <- get_package(adc, rm)\n    doc <- EML::read_eml(getObject(adc, pkg\\$metadata))\n    \n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    eml_validate(doc)\n    eml_path <- \"eml.xml\"   \n    write_eml(doc, eml_path)\n\n    #update <- publish_update(adc,\n    #                                               metadata_pid = pkg\\$metadata,\n    #                                               resource_map_pid = pkg\\$resource_map,\n    #                                               metadata_path = eml_path,\n    #                                               data_pids = pkg\\$data,\n    #                                               public = F)\n                                                    \n    #datamgmt::categorize_dataset(update\\$metadata, c(\"theme1\"), \"Your Name\")\n:::\nThe datapack ticket workflow ::: {.cell}\nsnippet datapack\n    library(dataone)\n    library(datapack)\n    library(digest)\n    library(uuid)\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    d1c <- D1Client(\"PROD\", \"urn:node:ARCTIC\")\n    packageId <- \"id here\"\n    dp <- getDataPackage(d1c, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\n    \n    #get metadata id\n    metadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n    \n    #edit the metadata\n    doc <- read_eml(getObject(d1c@mn, metadataId))\n    \n    #add the publisher info\n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    doc\\$dataset\\$project <- eml_nsf_to_project(\"nsf id here\")\n    \n    #check and save the metadata\n    eml_validate(doc)\n    eml_path <- arcticdatautils::title_to_file_name(doc\\$dataset\\$title)\n    write_eml(doc, eml_path)\n    \n    dp <- replaceMember(dp, metadataId, replacement=eml_path)\n    \n    #upload the dataset\n    myAccessRules <- data.frame(subject=\"CN=arctic-data-admins,DC=dataone,DC=org\", permission=\"changePermission\") \n    packageId <- uploadDataPackage(d1c, dp, public=F, accessRules=myAccessRules, quiet=FALSE)\n    #datamgmt::categorize_dataset(\"doi\", c(\"theme1\"), \"Jasmine\")\n:::\nQuick way to give access to submitters to their datasets: ::: {.cell}\nsnippet access\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n\n    rm <- \"rm here\"\n    pkg <- get_package(adc, rm)\n\n    set_access(adc, unlist(pkg), \"orcid here\")\n:::\nQuick access to the usual code for common Solr queries: ::: {.cell}\nsnippet solr\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    result <- query(adc, list(q = \"rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)\",\n                              fl = \"identifier,rightsHolder,formatId, fileName, dateUploaded\",\n                              sort = 'dateUploaded+desc',\n                              start =\"0\",\n                              rows = \"1500\"),\n                         as=\"data.frame\")\n:::"
  },
  {
    "objectID": "10_advanced.html#resources-for-r",
    "href": "10_advanced.html#resources-for-r",
    "title": "10  Beyond your first ticket",
    "section": "10.7 Resources for R",
    "text": "10.7 Resources for R\n\n10.7.1 Learning\nThe following online books are useful for expanding your R knowledge and skills:\n\nthe most recent ADC training materials\n\nThe cleaning and data manipulation section is useful for working with attribute tables\n\nEfficient R Programming\n\nIn particular Chapter 3 Efficient Programming\n\nR for Data Science\n\nSection on Strings\n\nR Packages\n\ncontributing to arcticdatatutils, datamgmt and EML\n\nAdvanced R\n\nObject-oriented programming in R for S4 to understand how datapack and dataone packages are written\n\nbookdown: Authoring Books and Technical Documents with R Markdown\nformatting, troubleshooting and updating the training document\n\nOthers\n\nHands-On Programming with R\nR Programming for Data Science\nExploratory Data Analysis with R\nMastering Software Development in R\nGeocomputation with R\nR Markdown: The Definitive Guide\nThe Tidyverse Style Guide\n\nThe RStudio cheatsheets are also useful references for functions in tidyverse and other packages.\n\n\n10.7.2 Packages\nThe data team uses and develops a number of R packages. Here is a listing and description of the main packages:\n\ndataone\n\nreading and writing data at DataONE member nodes\nhttp://doi.org/10.5063/F1M61H5X\n\ndatapack\n\ncreating and managing data packages\nhttps://github.com/ropensci/datapack\n\nEML\n\ncreating and editing EML metadata documents\nhttps://ropensci.github.io/EML\n\narcticdatautils\n\nutility functions for processing data for the Arctic Data Center\nhttps://nceas.github.io/arcticdatautils/\n\ndatamgmt\n\ndata management utilities for curating, documenting, and publishing data (sandbox package)\nhttps://nceas.github.io/datamgmt/\n\nmetadig\n\nauthoring MetaDIG quality checks\nhttps://github.com/NCEAS/metadig-r\n\nmetajam\n\ndownloading and reading data and metadata from DataONE member nodes\nhttps://nceas.github.io/metajam/"
  },
  {
    "objectID": "03_exploring_eml.html",
    "href": "03_exploring_eml.html",
    "title": "3  Exploring EML",
    "section": "",
    "text": "We use the Ecological Metadata Language (EML) to store structured metadata for all datasets submitted to the Arctic Data Center. EML is written in XML (extensible markup language) and functions for building and editing EML are in the EML R package.\nCurrently the Arctic Data Center website supports editing EML version 2.2.0. There are still some metadata in 2.1.1 that will be converted eventually.\nFor additional background on EML and principles for metadata creation, check out this paper.\nIf you aren’t too familiar with lists and how to navigate them yet take a look at the relevant sections in the Stat 545 class."
  },
  {
    "objectID": "03_exploring_eml.html#navigate-through-eml",
    "href": "03_exploring_eml.html#navigate-through-eml",
    "title": "3  Exploring EML",
    "section": "3.1 Navigate through EML",
    "text": "3.1 Navigate through EML\nThe first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function View allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file.\n\n\n\n\n# Need to be in this member node to explore file\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\ndoc <- read_eml(getObject(d1c_test@mn, \"urn:uuid:558eabf1-1e91-4881-8ba3-ef8684d8f6a1\"))\n\n\nView(doc)\n\n\nThe complex EML document is represented in R as as series of named, nested lists. We use lists all the time in R! A data.frame is one example of a special kind of list that we use all the time. You may be familiar with the syntax dataframe$column_name which allows us to select a particular column of a data.frame. Under the hood, a data.frame is a named list of vectors with the same length. You select one of those vectors using the $ operator, which is called the “list selector operator.”\nJust like you navigate in a data.frame, you can use the $ operator to navigate through the EML structure. The $ operator allows you to go deeper into the EML structure and to see what elements are nested within other elements. However, you have to tell R where you want to go in the structure when you use the $ symbol. For example, if you want to view the dataset element of your EML you would use the command doc$dataset. If you want to view the creators of your data set you would use doc$dataset$creator. Note here that creator is contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing $ and a list of available elements in the structure will appear (e.g., doc$<TAB>):\n\nNote that if you hit tab, and nothing pops up, this most likely implies that you are trying to go into an EML element that can take a series items. For example doc$dataset$creator$<TAB> will not show a pop-up menu. This is because creator is a series-type object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now doc$dataset$creator[[1]]$<TAB> will give you many more options. Note, an empty autocomplete result sometimes means you have reached the end of a branch in the EML structure.\nAt this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering doc$dataset into your console, and print it. Now make the search more specific, for instance: doc$dataset$abstract."
  },
  {
    "objectID": "03_exploring_eml.html#understand-the-eml-schema",
    "href": "03_exploring_eml.html#understand-the-eml-schema",
    "title": "3  Exploring EML",
    "section": "3.2 Understand the EML schema",
    "text": "3.2 Understand the EML schema\nAnother great resource for navigating the EML structure is looking at the schema which defines the structure. The schema diagrams on this page are interactive. Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it.\nFor example, let’s take a look at eml-party. To start off, notice that some elements have bolded lines leading to them.\n\nA bold line indicates that the element is required if the element above it (to the left in the schema) is used, otherwise the element is optional.\nNotice also that next to the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname.\nYou will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema.\nThe eml schema sections you may find particularly helpful include eml-party, eml-attribute and eml-physical.\nFor a more detailed description of the EML schema, see the reference section on exploring EML."
  },
  {
    "objectID": "03_exploring_eml.html#access-specific-elements",
    "href": "03_exploring_eml.html#access-specific-elements",
    "title": "3  Exploring EML",
    "section": "3.3 Access specific elements",
    "text": "3.3 Access specific elements\nThe eml_get() function is a powerful tool for exploring EML (more on that here ). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples:\n\ndoc <- read_eml(system.file(\"example-eml.xml\", package = \"arcticdatautils\"))\neml_get(doc, \"creator\")\n\nindividualName:\n  givenName: Bryce\n  surName: Mecum\norganizationName: National Center for Ecological Analysis and Synthesis\n\neml_get(doc, \"boundingCoordinates\")\n\neastBoundingCoordinate: '-134'\nnorthBoundingCoordinate: '59'\nsouthBoundingCoordinate: '57'\nwestBoundingCoordinate: '-135'\n\neml_get(doc, \"url\")\n\n'':\n  function: download\n  url: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\n'': ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\neml_get_simple() is a simplified alternative to eml_get() that produces a list of the desired EML element.\n\neml_get_simple(doc$dataset$otherEntity, \"entityName\")\n\nTo find an eml element you can use either a combination of which_in_emlfrom the arcticdatautils package or eml_get_simple and which to find the index in an EML list. Use which ever workflow you see fit.\nAn example question you may have: Which creators have a surName “Mecum”?\nExample using which_in_eml:\n\nn <- which_in_eml(doc$dataset$creator, \"surName\", \"Mecum\")\n# Answer: doc$dataset$creator[[n]]\n\nExample using eml_get_simple and which:\n\nent_names <- eml_get_simple(doc$dataset$creator, \"surName\")\ni <- which(ent_names == \"Mecum\")\n# Answer: doc$dataset$creator[[i]]"
  }
]