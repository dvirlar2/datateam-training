{
  "hash": "762923062963b13e0556099c74c3f430",
  "result": {
    "markdown": "---\ntitle: \"First Ticket\"\noutput-dir: docs # wasn't rendering properly w/o this\nfreeze: true\n---\n\n\n\nAfter completing the previous chapters, Daphne or Jeanette will assign a ticket from RT. Login using your LDAP credentials got get familiarized with RT.\n\n## Navigate RT\n\nThe <a href = 'https://support.nceas.ucsb.edu/rt/' target='_blank'>RT ticketing system</a> is how we\ncommunicate with folks interacting with the Arctic Data Center.  \nWe use it for managing submissions, accessing issues, etc. It consists of\nthree separate interfaces:  \n[Front Page](#front-page)  \n[All Tickets](#all-tickets)  \n[Ticket Page](#example-ticket)\n\n### Front page \n\n![](../../images/front_page_rt.png)\n\nThis is what you see first   \n\n1. Home - brings you to this homepage  \n2. Tickets - to search for tickets (also see number 5)  \n3. Tools - not needed  \n4. New Ticket - create a new ticket  \n5. Search - Type in the ticket number to quickly navigate to a ticket  \n6. Queue - Lists all of the tickets currently in a particular queue (such as 'arcticdata') and their statuses  \n + New = unopened tickets that require attention  \n + Open = tickets currently open and under investigation and/or being processed by a support team member  \n + Stalled = tickets awaiting responses from the PI/ `submitter`  \n7. Tickets I Own - These are the current open tickets that are claimed by me  \n8. Unowned Tickets - Newest tickets awaiting claim  \n9. Ticket Status - Status and how long ago it was created  \n10. Take - claim the ticket as yours  \n\n### All tickets\n\n![](../../images/all_tickets_rt.png)   \n\nThis is the queue interface from number 6 of the Front page  \n1. Ticket number and title  \n2. Ticket status  \n3. Owner - who has claimed the ticket  \n\n### Example ticket \n\n![](../../images/example_ticket_rt.png)  \n\n1. Title - Include the PI's name for reference  \n2. Display - homepage of the ticket  \n3. History - Comment/Email history, see bottom of Display page  \n4. Basics - edit the title, status, and ownership here  \n5. People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ `submitter` email addresses should be listed as \"Requestors\". Requestors are only emailed on \"Replys\", not \"Comments\". Ensure your ticket has a Requestor before attempting to contact users/ PIs/ `submitter`s   \n6. Links - option to \"Merge into\" another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number  \n\n:::{.callout-warning}\nVerify that this is indeed the two tickets you want to merge. It is non-reversible.\n:::\n\n\n7. Actions  \n + Reply - message the `submitter`/ PI/ all watchers  \n + Comment - attach internal message (no `submitter`s, only Data Teamers)  \n + Open It - Open the ticket  \n + Stall - `submitter` has not responded in greater than 1 month  \n + Resolve - ticket completed  \n8. History - message history and option to reply (to `submitter` and beyond) or comment (internal message)\n\n### New data submission\nWhen notified by Arcticbot about a new data submission, here are the typical steps:\n\n1. Update the Requestor under the People section based on the email given in the submission (usually the user/ PI/ `submitter`). You may have to google for the e-mail address if the PI did not include it in the metadata record.\n2. Take the ticket (Actions > Take)\n3. Review the submission based on the <a href = 'https://nceas.github.io/datateam-training/reference/initial-review-checklist.html' target='_blank'>checklist</a>\n4. Draft an email using the <a href = 'https://nceas.github.io/datateam-training/reference/email-templates.html' target='_blank'>template</a> and let others review it via Slack\n5. Send your reply via Actions\n\n\n\n\nBefore opening a R script first look over the initial checklist first to identify what you will need to update in the metadata.\n\n## Initial review checklist\n\nBefore responding to a new submission use this checklist to review the submission. When your are ready to respond use the <a href = 'https://nceas.github.io/datateam-training/reference/email-templates.html' target='_blank'>initial email template</a> and insert comments and modify as needed.\n\n#### Sensitive Data\n\nIf any of the below is in the dataset, please alert the #arctica team know before proceeding.\n\n- Check if there is any sensitive information or personal identifying information in the data (eg. Names)\n- Can the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers)\n- [Dryad Human Subject data guidelines](https://datadryad.org/docs/HumanSubjectsData.pdf) can be a good place to start\n    \nCommon Cases:\n\n- **Social Science:** Any dataset involving human subjects (may include awards awarded by ASSP and topics such as COVID-19)\n- **Archaeology:** archaeological site location information, which is protected from public access by law\n- **Biology:** protected species location coordinates\n\n#### Data citations\n- If the dataset appears to be in a publication please (might be in the abstract) make sure that those citations are registered.\n\n#### Title\n  + WHAT, WHERE, and WHEN:  \n    - Is descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage\n    - Provides a location of the work from the local to state or country level\n    - Provides a time frame of the work\n    - NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\n\n#### Abstract\n  + Describes the DATA as well as:\n    - The motivation (purpose) of the study\n    - Where and when the research took place\n    - At least one sentence summarizing general methodologies\n    - NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\n    - At least 100 words total\n    - - tags such as  `<superscript>2</superscript>` and `<subscript>2</subscript>` can be used for nicer formatting\n    - Any citations to papers can be registered with us\n\n#### Keywords\n  + Some keywords are included\n\n#### Data\n  + Data is normalized (if not suggest to convert the data if possible)\n  + At least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program)\n  + No xls/xlsx files (or other proprietary files)\n  + File contents and relationships among files are clear\n  + Each file is well NAMED and DESCRIBED and clearly differentiated from all others\n  + All attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. <a href = 'https://nceas.github.io/datateam-training/reference/assess-attributes.html' target='_blank'>Quality control all `dimensionless` units</a>.\n  + Missing value codes are explained (WHY are the data absent?)\n  + If it is a `.rar` file <a href = 'https://nceas.github.io/datateam-training/reference/scan-rar-files.html' target='_blank'> -> scan the file</a>\n  + If there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already\n\n#### People & Parties\n  + At least one contact and one creator with a name, email address, and ORCID iD\n\n#### Coverages\n  + Includes coverages that make sense\n    - Temporal coverage - Start date BEFORE end date\n    - Geologic time scales <a href = 'https://nceas.github.io/datateam-training/reference/set-coverages.html' target='_blank'>are added</a> if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic)\n    - Spatial coverage matches geographic description (check hemispheres)\n    - Geographic description is from the local to state or country level, at the least\n    - Taxonomic coverage if appropriate\n\n#### Project Information\n  + At least one FUNDING number\n  + Title, personnel, and abstract match information from the AWARD (not from the data package)\n\n#### Methods\n  + This section is REQUIRED for ALL NSF-FUNDED data packages\n  + Enough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources\n\n#### Portals\n  + If there are multiple submissions from the same people/project let them know about the portals feature\n  + If this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette and see the [data portals](https://nceas.github.io/datateam-training/reference/data-portals-1.html) section.\n\n\n\n## Processing templates\n\nWe have developed some partially filled R scripts to get you started on working on your first dataset. They outline common functions used in processing a dataset. However, it will differ depending on the dataset.\n\nYou can use this template where you can [fill in the blanks](data/dataset_processing_example_blanks.R) to get familiar with the functions we use and workflow at first. We also have a more minimal example [A filled example](data/dataset_processing_example_skeleton.R) as a intermediate step. You can look at the [filled example](data/dataset_processing_example_filled.R) if you get stuck or message the #datateam.\n\nOnce you have updated the dataset to your satisfaction and reviewed the Final Checklist, post the link to the dataset on #datateam for peer review.\n\n## Final Checklist\n\nYou can click on the `assessment report` on the website to for a general check. Fix anything you see there.\n\nSend the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive):\n\n### Special Datasets\nPlease refer to the dedicated pages for instructions to handle these cases:\n\n- [MOSAiC](https://nceas.github.io/datateam-training/reference/MOSAiC.html)\n- [DBO](https://nceas.github.io/datateam-training/reference/distributed-biological-observatory-dbo-submissions.html) \n\n### System Metadata\nThe format ids are correct\n\n### General EML\nIncluded lines for FAIR:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc <- eml_add_publisher(doc)\ndoc <- eml_add_entity_system(doc)\n```\n:::\n\n\n### Title\n- No abbreviations, should include geographic and temporal coverage\n\n### Abstract\n- longer than 100 words\n- no abbreviations or garbled text\n- tags such as  `<superscript>2</superscript>` and `<subscript>2</subscript>` can be used for nicer formatting\n\n### DataTable / OtherEntity / SpatialVectors\n- in the correct one: **DataTable / OtherEntity / SpatialVector / SpatialRaster** for the file type\n- **entityDescription** - longer than 5 words and unique\n- **physical** present and format correct\n\n#### Attribute Table\n- complete\n- attributeDefinitions longer than 3 words\n- **Variables** match what is in the file\n- **Measurement domain** - if appropirate (ie dateTime correct)\n- **Missing Value Code** - accounted for if applicable\n- **Semantic Annotation** - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc.\n\n### People\n- complete information for each person in each section\n    + including ORCID and e-mail address for all contacts\n    + people repeated across sections should have consistent information\n\n### Geographic region\n- the map looks correctand matches the geographic description\n- check if negatives (-) are missing\n\n### Project\n- if it is an NSF award you can use the helper function:\n    + `doc$dataset$project <- eml_nsf_to_project(awards)`\n- for other awards that need to be set manually, see the [set project](https://nceas.github.io/datateam-training/reference/set-the-project-section.html) page\n\n### Methods\n- present\n- no garbled text\n\n### Check EML Version\n- currently using: `eml-2.2.0` (as of July 30 2020)\n- review to see if the EML version is set correctly by reviewing the ``doc$`@context` `` that it is indeed 2.2.0 under eml \n- Re-run your code again and have the line`emld::eml_version(\"eml-2.2.0\")` at the top\n- Make sure the system metadata is also 2.2.0\n\n### Access\n- Granted access to PI using `set_rights_and_access()`\n  + make sure it is `http://` (no s)\n- **note** if it is a part of portals there might be specific access requirements for it to be visible using `set_access()`\n\n### SFTP Files\n- if there are files transferred to us via SFTP, delete those files when the ticket is resolved\n\n### Updated datasets\nAll the above applies. These are some areas to do a closer check when users update with a new file:\n\n- New data was added\n  + Temporal Coverage and Title\n  + and follow usual protocols\n- Files were replaced\n  + update physical and entityName\n  + double-check attributes are the same\n  + check for any new missing value codes that should be accounted for\n- Was the dataset published before 2021?\n  + update project info , annotations\n- Glance over entire page for any small mistakes (ie. repeated additionalMetadata, any missed &amps, typos)\n\n\n\n\n\nAfter all the revisions send the link to the PI in an email through RT. Send the draft of the email to Daphne or Jeanette on Slack.\n\n## Email templates\n\nThis section covers new data packages submitted. For other inquiries see the PI FAQ templates\n\nPlease think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible.  \n\nIn your first few months, please run email drafts by the #datateam Slack and get approval before sending.\n\nRemember to consult the <a href = 'https://arcticdata.io/submit/' target='_blank'>submission guidelines</a> for details of what is expected.  \n\nQuick reference:\n\n- [Initial email template](https://nceas.github.io/datateam-training/reference/email-templates.html#initial-email-template)\n- [Final email templates](https://nceas.github.io/datateam-training/reference/email-templates.html#final-email-templates)\n- [Additional email template](https://nceas.github.io/datateam-training/reference/email-templates.html#additional-email-templates)\n\n<!--\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n-->\n\n<!-- {{< include /workflows/pi_correspondence/email_templates/A_initial_email_template.qmd >}}\n-->\n\n### Initial email template\n\n> Hello [NAME OF REQUESTOR],\n> Thank you for your recent submission to the NSF Arctic Data Center!\n> \n> From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n> \n> [COMMENTS HERE]\n>\n> After we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n>\n> Best,\n> \n> [YOUR NAME]\n\n### Comment templates based on what is missing\n\n#### Portals\n\nMultiple datasets under the same project - suggest data portal feature\n\n> I would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project.\n\nIf they ask to nest the dataset\n\n> We are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email.\n\n#### Dataset citations\n\n> If there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point.\n\n#### Title\n\nProvides the *what, where, and when* of the data\n\n> We would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’.\n\nDoes not use acronyms\n\n> We wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE]\n\n#### Abstract\n\nDescribes DATA in package (ideally > 100 words)\n\n> We would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING]\n\n- The motivation of the study\n- Where and when the research took place\n- At least one sentence summarizing general methodologies\n- All acronyms are defined\n- At least 100 words long\n\nOffer this if submitter is reluctant to change:\n\n> If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL].\n\n#### Keywords\n\n> We noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others. \n\n#### Data\n\nSensitive Data\n\nWe will need to ask these questions manually until the fields are added to the webform.\n\n1. Data sensitivity categories\n\nOnce we have the ontology this question can be asked:\n\n> Based on our Data sensitivity categories, which of the 3 does your dataset align with most:\n\n- **Non-sensitive data** - None of the data includes sensitive or protected information.\n\n- **Some or all data is sensitive with minimal risk** - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized.\n\n- **Some or all data is sensitive with significant risk** - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body.\n\n2. Ethical research proceedures\n\n> We were wondering if you could also address this question specifically on Ethical Research Procedures:\nDescribe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution.\nWe can help add your answers to the question to the metadata.\n\nAdding provenance\n\n> Is the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2\n\nAt least one data file\n\n> We noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process.\n\nOpen formats\n\n*Example using xlsx. Tailor this reponse to the format in question.*\n\n> We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n> The data files can be replaced by going to the green Edit button > Click the black triangle by the Describe button for the data file > Select Replace (attached is also a screenshot on how to get there).\n![](../../images/replace_file.png)\n\nZip files\n\n> Except for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way.\n\nFile contents and relationships among files are clear\n\n> Could you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use.\n\nData layout\n\n> Would you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent?\n\nWe try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat.\n\n> We would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (<a href = 'https://arcticdata.io/submit/' target='_blank'>https://arcticdata.io/submit/</a>) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n#### Attributes\n\nIdentify which attributes need additional information. If they are common attributes like date and time we do not need further clarification.\n\nChecklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets):\n\n- A name (often the column or row header in the file).\n- A complete definition.\n- Any missing value codes along with explanations for those codes.\n- For all numeric data, unit information is needed.\n- For all date-time data, a date-time format is needed (e.g. \"DD-MM-YYYY\").\n- For text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes.\n\nHelpful templates:\n> We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form]\n> Could you describe ____? \n> Please define “XYZ”, including the unit of measure. \n> What are the units of measurement for the columns labeled “ABC” and “XYZ”?\n\nMissing value codes\n\n> What do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf.\n\n> We noticed that the data files contain [blank cells - replace with missing values found]. What do these represent? \n\n#### Funding\n\nAll NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization.\n\n> We noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations.\n\n#### Methods\n\nWe noticed that methods were missing from the submission. Submissions should include the following:\n\n- provide instrument names (if applicable)\n- specify how sampling locations were chosen\n- if citations for sampling methods are used, please provide a brief summary of the methods referenced\n- any software used to process the data\n\n*Note* - this includes software submissions as well (see <a href = 'https://arcticdata.io/submit/#metadata-guidelines-for-software' target='_blank'>https://arcticdata.io/submit/#metadata-guidelines-for-software</a>)\n\n> Your methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.]\n\n> Comprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable.\n\n*A full example - New Submission: methods, excel to csv, and attributes*\n\n> Thank you for your recent submission to the NSF Arctic Data Center!\n\n>From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n \n> If there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point.\n\n> We would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records.\n\n> We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n \n> We also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. <some example here>\n\n> Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n> What do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice.\n\n> We noticed that methods were missing from the submission. Submissions should:\n    - provide instrument names (if applicable)\n    - specify how sampling locations were chosen\n    - provide citations for sampling methods that are not explained in detail\n    - any software used to process the data\n\n> After we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n\n> Best,\n\n> Name\n\n\n\n\n### Final email templates\n\n#### Asking for approval\n\n> Hi [submitter],\n\n> I have updated your data package and you can view it here after logging in: [URL]\n\n> Please review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI.\n\n> After publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version.\n\n> Please let us know if you have any questions.\n\n#### DOI and data package finalization comments\n\n*Replying to questions about DOIs*\n\n> We attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version.\n\n> DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\n*Clarification of updating with a DOI and version control*\n\n> We definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it).\n\n#### Resolve the ticket\n\nSending finalized URL and dataset citation before resolving ticket\n\n[NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)] \n\n> Here is the link and citation to your finalized data package:\n\n> <a href = 'https://doi.org/10.18739/A20X0X' target='_blank'>https://doi.org/10.18739/A20X0X</a>\n\n> First Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X. \n\n> If in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n> Please let us know if you need any further assistance.\n\n\n\n### Additional email templates\n\n#### Deadlines\nIf the PI is checking about dates/timing:\n> [give rough estimate of time it might take] \n> Are you facing any deadlines? If so, we may be able to expedite publication of your submission.\n\n#### Pre-assigned DOI\nIf the PI needs a DOI right away:\n\n> We can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n#### Sensitive Data\n\n>Which of the following categories best describes the level of\nsensitivity of your data?\n\n> A. Non-sensitive data None of the data includes sensitive or protected\ninformation. Proceed with uploading data.\nB. Some or all data is sensitive but has been made safe for open\ndistribution Sensitive data has been de-identified, anonymized, aggregated,\nor summarized to remove sensitivities and enable safe data distribution.\nExamples include ensuring that human subjects data, protected species data,\narchaeological site locations and personally identifiable information have\nbeen properly anonymized, aggregated and summarized. Proceed with uploading\ndata, but ensure that only data that are safe for public distribution are\nuploaded. Address questions about anonymization, aggregation,\nde-identification, and data embargoes with the data curation support team\nbefore uploading data. Describe these approaches in the Methods section.\nC. Some or all data is sensitive and should not be distributed The data\ncontains human subjects data or other sensitive data. Release of the data\ncould cause harm or violate statutes, and must remain confidential\nfollowing restrictions from an Institutional Review Board (IRB) or similar\nbody. Do NOT upload sensitive data. You should still upload a metadata\ndescription of your dataset that omits all sensitive information to inform\nthe community of the dataset's existence. Contact the data curation support\nteam about possible alternative approaches to safely preserve sensitive or\nprotected data.\n\n> 2. Ethical Research Procedures. Please describe how and the extent to which\ndata collection procedures followed community standards for ethical\nresearch practices (e.g., CARE Principles). Be explicit about Institutional\nReview Board approvals, consent waivers, procedures for co-production, data\nsovereignty, and other issues addressing responsible and ethical research.\nInclude any steps to anonymize, aggregate or de-identify the dataset, or to\notherwise create a version for public distribution.\n\n#### Asking for dataset access\n> As a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets. \n\n#### No response from the researcher\nPlease email them before resolving a ticket like this:\n\n> We are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email.\n\n#### Recovering Dataset submissions\n> To recover dataset submissions that were not successful please do the following:\n\n> 1. Go to https://arcticdata.io/catalog/drafts\n2. Find your dataset and download the corresponding file\n3. Send us the file in an email\n\n#### Custom Search Link\n> You could also use a permalink like this to direct users to the datasets:\nhttps://arcticdata.io/catalog/data/query=\"your search query here\"\nfor example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER\n\n#### Adding metadata via R\n\n> KNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R.\n\n><a href = 'http://training.arcticdata.io/materials/arctic-data-center-training/programming-metadata-and-data-publishing.html' target='_blank'>Here</a> are some training materials we have that use both the `EML` and `datapack` packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: `d1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")`\n\n> If you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well.\n\n#### Finding multiple data packages\n\n> If linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages.\n\te.g.: <a href = 'https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533' target='_blank'>https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533</a> \n\t\n#### NSF ARC data submission policy\n\n> Please find an overview of our submission guidelines here: <a href = 'https://arcticdata.io/submit/' target='_blank'>https://arcticdata.io/submit/</a>, and NSF Office of Polar Programs policy information here: <a href = 'https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp' target='_blank'>https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp</a>.\n\n> Investigators should upload their data to the Arctic Data Center (<a href = 'https://arcticdata.io' target='_blank'>https://arcticdata.io</a>), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used.\n\n#### Linking ORCiD and LDAP accounts\n\n> First create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: <a href = 'https://knb.ecoinformatics.org/#share' target='_blank'>https://knb.ecoinformatics.org/#share</a>. Next, hover over the icon on the top right and choose \"My Profile\". Then, click the \"Settings\" tab and scroll down to \"Add Another Account\". Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the \"+\". You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click \"have an existing account\", and enter your Morpho credentials with the organization set to \"unaffiliated\") to finalize the linkage between the two accounts. Navigate to \"My Profile\" and \"Settings\" to confirm the linkage.\n\n> After completing this, all of your previously submitted data pacakges should show up on your KNB \"My Profile\" page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface.\n\n> Or, try reversing my instructions - log in first using your Morpho account (by clicking the \"existing account\" button and selecting organization \"unaffiliated\"), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage.\n\n\n\n\n\nOnce the dataset is approved by the PI and there are no further changes, publish the dataset with a doi.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement = eml_path, newId = doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public = TRUE, quiet = FALSE)\n```\n:::\n\n\n\n## Categorize datasets\n\nAs a final step we will categorize the dataset you processed. We are trying to categorize datasets so we can have a rough idea of what kinds of datasets we have at the Arctic Data Center. We will grant you access to the google sheet that has all of the [categorized  datasets](https://docs.google.com/spreadsheets/d/1S_7iW0UBZLZoJBrHXTW5fbHH-NOuOb6xLghraPA4Kf4/edit#gid=1479370118)\n\nWe will categorize each dataset into one of the predefined themes (ie. biology, ecology etc.). Definition of the themes can be found in the [google sheet](https://docs.google.com/document/d/1rqHk9SCH9JvdSnIJovYVE1YF4IF4_yNxFfCOJMI8xjk/edit)\n\nRun the following line with your doi and themes as a list.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatamgmt::categorize_dataset(\"your_doi\", c(\"list\", \"of\", \"themes\"), \"your name\")\n```\n:::\n\n\n\n## Congrats!\n\nCongratulations on finishing your first ticket! You can head over to the the repository, [data-processing](https://github.com/NCEAS/data-processing) to get your ticket processing code reviewed by the team so we can learn from each other!\n",
    "supporting": [
      "09_first_ticket_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}