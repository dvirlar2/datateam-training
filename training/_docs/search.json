[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NCEAS!",
    "section": "",
    "text": "Get a tour of the office from Ginger\nFill out required paperwork from Ana (room 325)\nSchedule a headshot with Alex (room 326)\nSet up the remainder of your accounts"
  },
  {
    "objectID": "index.html#account-information",
    "href": "index.html#account-information",
    "title": "Welcome to NCEAS!",
    "section": "Account information",
    "text": "Account information\n\nLDAP - this should be set up prior to your start date in order to help get other accounts set. This account and password control your access to:\n\narcticdata RT queue\nGitHub Enterprise - arctic-data\n\nDatateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal\nORCiD - create an account\n\nLogin to test.arcticdata.io with your ORCID iD\n\nNCEAS Slack - get an invite from slack.nceas.ucsb.edu\n\nChannels to join: #arctica, #arcticbot, #computing, #datateam, #devteam, #social\nIntroduce yourself in #datateam and then use that channel to ask Jeanette to make you an administrator on test.arcticdata.io\n\nArctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request addition to the admin group from Jeanette in Slack\nGitHub - if you do not have a public GitHub account already, please register for one here\nIf you are an intern, fill out anticipated quarterly schedule on the intern google calendar shared with you.\nElectronic Timekeeping - make sure you can log on to electronic timekeeping via your UCSBNetID and password (may not be accessible on the first day, if you continue to have issues please let Ana know). If you are an hourly employee, log your hours for your first day! Under today’s date select ‘Hours Worked’ under the Pay Code column, enter the amount of hours under the Amount column, and finally click the ‘Save’ button in the top right. At the end of every two-week pay period you will also need to click the ‘Approve Timecard’ button in order to submit your timecard.\nDetailed Instructions\nLet Jeanette or Daphne know what email you would like to use for general NCEAS updates from all@nceas.ucsb.edu"
  },
  {
    "objectID": "index.html#nceas-events",
    "href": "index.html#nceas-events",
    "title": "Welcome to NCEAS!",
    "section": "NCEAS events",
    "text": "NCEAS events\nNCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the recurring events are:\n\nRoundtable\n\nweekly presentation and discussion of research by a visiting or local scientist\nWednesdays at 12:15 in the lounge\n\nCoffee Klatch\n\ncoffee, socializing, and news updates for NCEAS\nTuesdays at 10:30 in the lounge"
  },
  {
    "objectID": "index.html#nceas-community-meetings-groups",
    "href": "index.html#nceas-community-meetings-groups",
    "title": "Welcome to NCEAS!",
    "section": "NCEAS Community Meetings/ Groups",
    "text": "NCEAS Community Meetings/ Groups\nCheck out their individual calendar entries and channels for more information\n\nEarly Career Researcher Community Forum - #ecr_community\nHacky Hours - #hackyhour\nData Science Chats - #data-science-chats\nNCEAS Diversity Team\nNCEAS Book Club - #bookclub"
  },
  {
    "objectID": "index.html#internship-expectations",
    "href": "index.html#internship-expectations",
    "title": "Welcome to NCEAS!",
    "section": "Internship Expectations",
    "text": "Internship Expectations\nAs an intern with the data team, there are a few expectations that the Project Coordinators have of you. Overall, we expect you to be communicative and proactive. We want you to learn and grow in this position, but we don’t want you spinning your wheels going nowhere fast! If you’ve spent 10-15 minutes on an issue and you’re not making any progress, reach out to us and your peers for help in the #datateam slack channel. The #datateam slack channel is the main form of communication, and we expect all interns to become comfortable communicating in this space.\nAdditionally, we expect interns to work within the standard business hours of 8am - 5pm (pacific time). We ask that you mark your expected work hours on the shared “Intern” Google Calendar. This is so that the Project Coordinators are aware of who’s working day-to-day and can plan their days accordingly. We also use this to verify time sheets when they are submitted. Ideally, interns would input their proposed hours on the calendar at least one week in advance. During exams and other unusually busy weeks at school, we understand you may need to shift your hours or reduce your workload. When this occurs, please make sure to email either Daphne or Jeanette so that we know not to expect you during your usual schedule."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Welcome to NCEAS!",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nTake a moment to review the diversity and inclusion page and code of conduct at NCEAS so that we can foster an environment that is safe, welcoming and inclusive for everyone."
  },
  {
    "objectID": "index.html#citation-information",
    "href": "index.html#citation-information",
    "title": "Welcome to NCEAS!",
    "section": "Citation Information",
    "text": "Citation Information\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nClark, Jeanette, Jesse Goldstein, Dominic Mullen, Irene Steves, Mitchell Maier, Stephanie Freund, Sharis Ochs, Bryce Mecum, Peter Slaughter, Emily O’Dean, Jasmine Lai, Daphne Virlar-Knight. 2022 Training Materials for the Arctic Data Center Curation Team. Arctic Data Center. doi:10.18739/A20G3GX8W."
  },
  {
    "objectID": "04_editing_eml.html",
    "href": "04_editing_eml.html",
    "title": "Editing EML",
    "section": "",
    "text": "This chapter is a practical tutorial for using R to read, edit, write, and validate EML documents. Much of the information here can also be found in the vignettes for the R packages used in this section (e.g. the EML package).\nMost of the functions you will see in this chapter will use the arcticdatautils and EML packages."
  },
  {
    "objectID": "04_editing_eml.html#edit-an-eml-element",
    "href": "04_editing_eml.html#edit-an-eml-element",
    "title": "Editing EML",
    "section": "Edit an EML element",
    "text": "Edit an EML element\nThere are multiple ways to edit an EML element.\n\nEdit EML with strings\nThe most basic way to edit an EML element would be to navigate to the element and replace it with something else. Easy!\nFor example, to change the title one could use the following command:\n\ndoc$dataset$title <- \"New Title\"\n\nIf the element you are editing allows for multiple values, you can pass it a list of character strings. Since a dataset can have multiple titles, we can do this:\n\ndoc$dataset$title <- list(\"New Title\", \"Second New Title\")\n\nHowever, this isn’t always the best method to edit the EML, particularly if the element has sub-elements.\n\n\nEdit EML with the “EML” package\nTo edit a section where you are not 100% sure of the sub-elements, using the eml$elementName() helper functions from the EML package will pre-populate the options for you if you utilize the RStudio autocomplete functionality. The arguments in these functions show the available slots for any given EML element. For example, typing doc$dataset$abstract <- eml$abstract()<TAB> will show you that the abstract element can take either the section or para sub-elements.\n\n\n\n\ndoc$dataset$abstract <- eml$abstract(para = \"A concise but thorough description of the who, what, where, when, why, and how of a dataset.\")\n\nThis inserts the abstract with a para element in our dataset, which we know from the EML schema is valid.\nNote that the above is equivalent to the following generic construction:\n\ndoc$dataset$abstract <- list(para = \"A concise but thorough description of the who, what, where, when, why, and how of a dataset.\")\n\nThe eml() family of functions provides the sub-elements as arguments, which is extremely helpful, but functionally all it is doing is creating a named list, which you can also do using the list function.\n\n\nEdit EML with objects\nA final way to edit an EML element would be to build a new object to replace the old object. To begin, you might create an object using an eml helper function. Let’s take keywords as an example. Sometimes keyword lists in a metadata record will come from different thesauruses, which you can then add in series (similar to the way we added multiple titles) to the element keywordSet.\nWe start by creating our first set of keywords and saving it to an object.\n\nkw_list_1 <- eml$keywordSet(keywordThesaurus = \"LTER controlled vocabulary\",\n                            keyword = list(\"bacteria\", \"carnivorous plants\", \"genetics\", \"thresholds\"))\n\nWhich returns:\n$keyword\n$keyword[[1]]\n[1] \"bacteria\"\n\n$keyword[[2]]\n[1] \"carnivorous plants\"\n\n$keyword[[3]]\n[1] \"genetics\"\n\n$keyword[[4]]\n[1] \"thresholds\"\n\n\n$keywordThesaurus\n[1] \"LTER controlled vocabulary\"\nWe create the second keyword list similarly:\n\nkw_list_2 <- eml$keywordSet(keywordThesaurus = \"LTER core area\", \n                            keyword =  list(\"populations\", \"inorganic nutrients\", \"disturbance\"))\n\nFinally, we can insert our two keyword lists into our EML document just like we did with the title example above, but rather than passing character strings into list(), we will pass our two keyword set objects.\n\ndoc$dataset$keywordSet <- list(kw_list_1, kw_list_2)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you must use the function list here and not the c() function. The reasons for this are complex, and due to some technical subtlety in R - but the gist of the issue is that the c() function can behave in unexpected ways with nested lists, and frequently will collapse the nesting into a single level, resulting in invalid EML."
  },
  {
    "objectID": "04_editing_eml.html#fair-data-practices",
    "href": "04_editing_eml.html#fair-data-practices",
    "title": "Editing EML",
    "section": "FAIR data practices",
    "text": "FAIR data practices\nThe result of these function calls won’t show up on the webpage but they will add a publisher element to the dataset element and a system to all of the entities based on what the PID is. This will help make our metadata more FAIR (Findable, Accessible, Interoperable, Reusable).\nThese two functions come from the arcticatautils package, an R package we wrote to help with some very specific data processing tasks.\nAdd these function calls to all of your EML processing scripts.\n\n  library(arcticdatautils)\n\n  doc <- eml_add_publisher(doc)\n  doc <- eml_add_entity_system(doc)"
  },
  {
    "objectID": "04_editing_eml.html#edit-attributelists",
    "href": "04_editing_eml.html#edit-attributelists",
    "title": "Editing EML",
    "section": "Edit attributeLists",
    "text": "Edit attributeLists\nAttributes are descriptions of variables, typically columns or column names in tabular data. Attributes are stored in an attributeList. When editing attributes in R, you need to create one to three objects:\n\nA data.frame of attributes\nA data.frame of custom units (if applicable)\n\nThe attributeList is an element within one of 4 different types of entity objects. An entity corresponds to a file, typically. Multiple entities (files) can exist within a dataset. The 4 different entity types are dataTable (most common for us), spatialVector, spatialRaster, and otherEntity\nPlease note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector object for spatial data.\nTo edit or examine an existing attribute table already in an EML file, you can use the following commands, where i represents the index of the series element you are interested in. Note that if there is only one item in the series (ie there is only one dataTable), you should just call doc$dataset$dataTable, as in this case doc$dataset$dataTable[[1]] will return the first sub-element of the dataTable (the entityName)\n\n# If they are stored in an otherEntity (submitted from the website by default)\nattributeList <- EML::get_attributes(doc$dataset$otherEntity[[i]]$attributeList)\n\n# Or if they are stored in a dataTable (usually created by a datateam member)\nattributeList <- EML::get_attributes(doc$dataset$dataTable[[i]]$attributeList)\n\n# Or if they are stored in a spatialVector (usually created by a datateam member)\nattributeList <- EML::get_attributes(doc$dataset$spatialVector[[i]]$attributeList)\n\nattributes <- attributeList$attributes\nprint(attributes)\n\n\nEdit attributes\nAttribute information should be stored in a data.frame with the following columns:\n\nattributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp”\nattributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)”\nattributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.”\nmeasurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required.\n\nnominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River)\nordinal: ordered categories. e.g.: Low, Medium, High\ndateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001\nratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second.\ninterval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius.\n\ndomain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required.\n\ntextDomain: text that is free-form, or matches a pattern\nenumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake\ndateTimeDomain: dateTime attributes\nnumericDomain: attributes that are numbers (either ratio or interval)\n\nformatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”.\ndefinition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases.\nunit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found using: standardUnits <- EML::get_unitList() then running View(standardUnits$units).\nnumberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer.\n\nreal: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…)\nnatural: non-zero positive integers (1,2,3…)\nwhole: positive integers and zero (0,1,2,3…)\ninteger: positive and negative integers and zero (…-2,-1,0,1,2…)\n\nmissingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’\nmissingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists.\n\nYou can create attributes manually by typing them out in R following a workflow similar to the one below:\n\nattributes <- data.frame(\n    \n    attributeName = c('Date', 'Location', 'Region','Sample_No', 'Sample_vol', \n                      'Salinity', 'Temperature', 'sampling_comments'),\n    \n    attributeDefinition = c('Date sample was taken on', \n                            'Location code representing location where sample was taken',\n                            'Region where sample was taken', 'Sample number', 'Sample volume', \n                            'Salinity of sample in PSU', 'Temperature of sample', \n                            'comments about sampling process'),\n    \n    measurementScale = c('dateTime', 'nominal','nominal', 'nominal', 'ratio', \n                         'ratio', 'interval', 'nominal'),\n    \n    domain = c('dateTimeDomain', 'enumeratedDomain','enumeratedDomain', \n               'textDomain', 'numericDomain', 'numericDomain', \n               'numericDomain', 'textDomain'),\n    \n    formatString = c('MM-DD-YYYY', NA,NA,NA,NA,NA,NA,NA),\n    \n    definition = c(NA,NA,NA,'Sample number', NA, NA, NA, \n                   'comments about sampling process'),\n    \n    unit = c(NA, NA, NA, NA,'milliliter', 'dimensionless', 'celsius', NA),\n    \n    numberType = c(NA, NA, NA,NA, 'real', 'real', 'real', NA),\n    \n    missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, 'NA'),\n    \n    missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, \n                                    'no sampling comments'))\n\nHowever, typing this out in R can be a major pain. Luckily, there’s a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you) to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes (these commands will launch a Shiny app in your web browser):\n\n#first download the CSV in your data package from Exercise #2\ndata_pid <- selectMember(dp, name = \"sysmeta@fileName\", value = \".csv\")\ndata <- read.csv(text=rawToChar(getObject(d1c_test@mn, data_pid)))\n\n\n# From data (recommended)\nEML::shiny_attributes(data = data)\n\n# From an existing attribute table\nattributeList <- get_attributes(doc$dataset$dataTable[[i]]$attributeList)\nEML::shiny_attributes(data = NULL, attributes = attributeList$attributes)\n\n# From scratch\natts <- EML::shiny_attributes()\n\nOnce you are done editing a table in the app, quit the app and the tables will be assigned to the atts variable as a list of data frames (one for attributes, factors, and units). Alternatively, each table can be to exported to a csv file by clicking the Download button.\nIf you downloaded the table, read the table back into your R session and assign it to a variable in your script (e.g. attributes <- data.frame(...)), or just use the variable that shiny_attributes returned.\nFor simple attribute corrections, datamgmt::edit_attribute() allows you to edit the slots of a single attribute within an attribute list. To use this function, pass an attribute through datamgmt::edit_attribute() and fill out the parameters you wish to edit/update. An example is provided below where we are changing attributeName, domain, and measurementScale in the first attribute of a dataset. After completing the edits, insert the new version of the attribute back into the EML document.\n\nnew_attribute <- datamgmt::edit_attribute(doc$dataset$dataTable[[1]]$attributeList$attribute[[1]], \n                          attributeName = 'date_and_time', \n                          domain = 'dateTimeDomain', \n                          measurementScale = 'dateTime')\n\ndoc$dataset$dataTable[[1]]$attributeList$attribute[[1]] <- new_attribute\n\n\n\nEdit custom units\nEML has a set list of units that can be added to an EML file. These can be seen by using the following code:\n\nstandardUnits <- EML::get_unitList()\nView(standardUnits$units)\n\nSearch the units list for your unit before attempting to create a custom unit. You can search part of the unit you can look up part of the unit ie meters in the table to see if there are any matches.\nIf you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields:\n\nid: The unit id (ids are camelCased)\nunitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes)\nparentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”)\nmultiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000)\nname: Unit abbreviation (e.g. for kilometer name = “km”)\ndescription: Text defining the unit (e.g. for kilometer description = “1000 meters”)\n\nTo manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template:\n\ncustom_units <- data.frame(   \n  id = c('siemensPerMeter', 'decibar'),\n  unitType = c('resistivity', 'pressure'),\n  parentSI = c('ohmMeter', 'pascal'),\n  multiplierToSI = c('1','10000'),\n  abbreviation = c('S/m','decibar'),\n  description = c('siemens per meter', 'decibar'))\n\nUsing EML::get_unit_id for custom units will also generate valid EML unit ids. Custom units are then added to additionalMetadata using the following command:\n\nunitlist <- set_unitList(custom_units, as_metadata = TRUE)\ndoc$additionalMetadata <-  list(metadata = list(unitList = unitlist))\n\n\n\nEdit factors\nFor attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition.\n\nattributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute.\ncode should contain all unique values of the given attributeName that exist within the actual data.\ndefinition should contain a plain text definition that describes each code.\n\nTo build factors by hand, you use the named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”.\n\nLocation <- c(CASC = 'Cascade Lake', CHIK = 'Chikumunik Lake', \n              HEAR = 'Heart Lake', NISH = 'Nishlik Lake' )\n\nRegion <- c(W_MTN = 'West region, locations West of Eagle Mountain', \n            E_MTN = 'East region, locations East of Eagle Mountain')\n\nThe definitions are then written into a data.frame using the names of the named character vectors and their definitions.\n\nfactors <- rbind(data.frame(attributeName = 'Location', \n                            code = names(Location), \n                            definition = unname(Location)),\n                  data.frame(attributeName = 'Region', code = names(Region), \n                             definition = unname(Region)))\n\n\n\nFinalize attributeList\nOnce you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using the following command:\n\nattributeList <- EML::set_attributes(attributes = attributes,\n                                     factors = factors) \n\nThis attributeList must then be added to a dataTable.\n\n\n\n\n\n\nNote\n\n\n\nRemember to use: d1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\") d1c_test@mn"
  },
  {
    "objectID": "04_editing_eml.html#set-physical",
    "href": "04_editing_eml.html#set-physical",
    "title": "Editing EML",
    "section": "Set physical",
    "text": "Set physical\nTo set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io!\n\n\n\n\n\n\nNote\n\n\n\nThe word ‘physical’ derives from database systems, which distinguish the ‘logical’ model (e.g., what attributes are in a table, etc) from the physical model (how the data are written to a physical hard disk (basically, the serialization). so, we grouped metadata about the file (eg. dataformat, file size, file name) as written to disk in physical.\n\n\n\ndata_pid <- selectMember(dp, name = \"sysmeta@fileName\", \n                         value = \"your_file_name.csv\")\n\nphysical <- arcticdatautils::pid_to_eml_physical(d1c@mn, data_pid)\n\nThe physical must then be assigned to the data object.\nNote that the above workflow only works if your data object already exists on the member node."
  },
  {
    "objectID": "04_editing_eml.html#edit-datatables",
    "href": "04_editing_eml.html#edit-datatables",
    "title": "Editing EML",
    "section": "Edit dataTables",
    "text": "Edit dataTables\nTo edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable using the eml$dataTable() helper function as below:\n\ndataTable <- eml$dataTable(entityName = \"A descriptive name for the data (does not need to be the same as the data file)\",\n                           entityDescription = \"A description of the data\",\n                           physical = physical,\n                           attributeList = attributeList)\n\nThe dataTable must then be added to the EML. How exactly you do this will depend on whether there are dataTable elements in your EML, and how many there are. To replace whatever dataTable elements already exist, you could write:\n\ndoc$dataset$dataTable <- dataTable\n\nIf there is only one dataTable in your dataset, the EML package will usually “unpack” these, so that it is not contained within a list of length 1 - this means that to add a second dataTable, you cannot use the syntax doc$dataset$dataTable[[2]], since when unpacked this will contain the entityDescription as opposed to pointing to the second in a series of dataTable elements. Confusing - I know. Not to fear though - this syntax will get you on your way, should you be trying to add a second dataTable.\n\ndoc$dataset$dataTable <- list(doc$dataset$dataTable, dataTable)\n\nIf there is more than one dataTable in your dataset, you can return to the more straightforward construction of:\n\ndoc$dataset$dataTable[[i]] <- dataTable \n\nWhere i is the index that you wish insert your dataTable into.\nTo add a list of dataTables to avoid the unpacking problem above you will need to create a list of dataTables\n\ndts <- list() # create an empty list\nfor(i in seq_along(tables_you_need)){\n  # your code modifying/creating the dataTable here\n  dataTable <- eml$dataTable(entityName = dataTable$entityName,\n                             entityDescription = dataTable$entityDescription,\n                             physical = physical,\n                             attributeList = attributeList)\n  \n  dts[[i]] <- dataTable # add to the list\n}\n\nAfter getting a list of dataTables, assign the resulting list to dataTable EML.\n\ndoc$dataset$dataTable <- dts\n\nBy default, the online submission form adds all entities as otherEntity, even when most should probably be dataTable. You can use eml_otherEntity_to_dataTable to easily move items in otherEntity over to dataTable. Most tabular data or data that contain variables should be listed as a dataTable. Data that do not contain variables (eg: plain text readme files, pdfs, jpegs) should be listed as otherEntity.\n\neml_otherEntity_to_dataTable(doc, \n                             1, # which otherEntities you want to convert, for multiple use - 1:5\n                             validate_eml = F) # set this to False if the physical or attributes are not added"
  },
  {
    "objectID": "04_editing_eml.html#edit-otherentities",
    "href": "04_editing_eml.html#edit-otherentities",
    "title": "Editing EML",
    "section": "Edit otherEntities",
    "text": "Edit otherEntities\n\nRemove otherEntities\nTo remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable.\n\ndoc$dataset$otherEntity[[i]] <- NULL\n\n\n\nCreate otherEntities\nIf you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity.\n\notherEntity <- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]])\n\nAlternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID.\n\notherEntity <- arcticdatautils::pid_to_eml_entity(mn, \"your_data_pid\", entityType = \"otherEntity\", entityName = \"Entity Name\", entityDescription = \"Description about entity\")\n\nThe otherEntity must then be set to the EML, like so:\n\ndoc$dataset$otherEntity <- otherEntity\n\nIf you have more than one otherEntity object in the EML already, you can add the new one like this:\n\ndoc$dataset$otherEntity[[i]] <- otherEntity\n\nWhere i is set to the number of existing entities plus one. Remember the warning from the last section, however. If you only have one otherEntity, and you are trying to add another, you have to run:\n\ndoc$dataset$otherEntity <- list(otherEntity, doc$dataset$otherEntity)"
  },
  {
    "objectID": "04_editing_eml.html#semantic-annotations",
    "href": "04_editing_eml.html#semantic-annotations",
    "title": "Editing EML",
    "section": "Semantic annotations",
    "text": "Semantic annotations\nFor a brief overview of what a semantic annotation is, and why we use them check out this video.\nEven more information on how to add semantic annotations to EML 2.2.0 can be found here. Currently metacatUI does not support the editing of semantic annotations on the website so all changes will have to be done in R.\nThere are several elements in the EML 2.2.0 schema that can be annotated:\n\ndataset\nentity (eg: otherEntity or dataTable)\nattribute\n\nOn the datateam, we will only be adding annotations to attributes for now.\n\nHow annotations are used\nThis is a dataset that has semantic annotations included.\nOn the website you can see annotations in each of the attributes.\n\n\nYou can click on any one of them to search for more datasets with that same annotation.\n\n\nAttribute-level annotations\nTo add annotations to the attributeList you will need information about the propertyURI and valueURI\nAnnotations are essentially composed of a sentence, which contains a subject (the attribute), predicate (propertyURI), and object (valueURI). Because of the way our search interface is built, for now we will be using attribute annotations that have a propertyURI label of “contains measurements of type”.\nHere is what an annotation for an attribute looks like in R. Note that both the propertyURI and valueURI have both a label, and the URI itself.\n\ndoc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$annotation\n\n$id\n[1] \"ODBcOyaTsg\"\n\n$propertyURI\n$propertyURI$label\n[1] \"contains measurements of type\"\n\n$propertyURI$propertyURI\n[1] \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType\"\n\n\n$valueURI\n$valueURI$label\n[1] \"Distributed Biological Observatory region identifier\"\n\n$valueURI$valueURI\n[1] \"http://purl.dataone.org/odo/ECSO_00002617\"\n\n\n\n\n\n\nNote\n\n\n\nSemantic attribute annotations can be applied to spatialRasters, spatialVectors and dataTables\n\n\n\n\n\nHow to add an annotation\n1. Decide which variable to annotate\nThe goal for the datateam is to start annotating every dataset that comes in. Please make sure to add semantic annotations to spatial and temporal features such as latitude, longitude, site name and date and aim to annotate as many attributes as possible.\n2. Find an appropriate valueURI\nThe next step is to find an appropriate value to fill in the blank of the sentence: “this attribute contains measurements of _____.”\nThere are several ontologies to search in. In order of most to least likely to be relevant to the Arctic Data Center they are:\n\nThe Ecosystem Ontology (ECSO)\n\nthis was developed at NCEAS, and has many terms that are relevant to ecosystem processes, especially those involving carbon and nutrient cycling\n\nThe Environment Ontology (EnVO)\n\nthis is an ontology for the concise, controlled description of environments\n\nNational Center for Biotechnology Information (NCBI) Organismal Classification (NCBITAXON)\n\nThe NCBI Taxonomy Database is a curated classification and nomenclature for all of the organisms in the public sequence databases.\n\nInformation Artifact Ontology (IAO)\n\nthis ontology contains terms related to information entities (eg: journals, articles, datasets, identifiers)\n\n\nTo search, navigate through the “classes” until you find an appropriate term. When we are picking terms, it is important that we not just pick a similar term or a term that seems close - we want a term that is totally “right”. For example, if you have an attribute for carbon tetroxide flux and an ontology with a class hierarchy like this:\n– carbon flux\n|—- carbon dioxide flux\nOur exact attribute, carbon tetroxide flux is not listed. In this case, we should pick “carbon flux” as it’s completely correct and not “carbon dioxide flux” because it’s more specific but not quite right.\n\n\n\n\n\n\nNote\n\n\n\nFor general attributes (such as ones named depth or length), it is important to be as specific as possible about what is being measured.\ne.g. selecting the lake area annotation for the area attribute in this dataset\n\n\n3. Build the annotation in R\n\nManually Annotating\nthis method is great for when you are inserting 1 annotation, fixing an existing annotation or programmatically updating annotations for multiple attributeLists\nFirst you need to figure out the index of the attribute you want to annotate.\n\neml_get_simple(doc$dataset$dataTable[[3]]$attributeList, \"attributeName\")\n\n [1] \"prdM\"         \"t090C\"        \"t190C\"        \"c0mS/cm\"      \"c1mS/cm\"      \"sal00\"        \"sal11\"        \"sbeox0V\"      \"flECO-AFL\"\n[10] \"CStarTr0\"     \"cpar\"         \"v0\"           \"v4\"           \"v6\"           \"v7\"           \"svCM\"         \"altM\"         \"depSM\"    \n[19] \"scan\"         \"sbeox0ML/L\"   \"sbeox0dOV/dT\" \"flag\"    \nNext, assign an id to the attribute. It should be unique within the document, and it’s nice if it is human readable and related to the attribute it is describing. One format you could use is entity_x_attribute_y which should be unique in scope, and is nice and descriptive.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$id <- \"entity_ctd_attribute_salinity\"\n\nNow, assign the propertyURI information. This will be the same for every annotation you build.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$propertyURI <- list(label = \"contains measurements of type\",\n                                                                                       propertyURI = \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType\")\n\nFinally, add the valueURI information from your search.\n You should see an ID on the Bioportal page that looks like a URL - this is the valueURI. Use the value to populate the label element.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$valueURI <- list(label = \"Water Salinity\",\n     valueURI = \"http://purl.dataone.org/odo/ECSO_00001164\")\n\n\n\nShiny Attributes\nthis method is great for when you are updating many attributes\nOn the far right of the table of shiny_attributes there are 4 columns: id, propertyURI, propertyLabel, valueURI, valueLabel that can be filled out.\n\n\n\nAnnotating sensitive data\nSensitive datasets that might cover protected characteristics (human subjects data, endangered species locations, etc) should be annotated using the data sensitivity ontology: https://bioportal.bioontology.org/ontologies/SENSO/?p=classes&conceptid=root.\n\nDataset Annotations\nAs a final step in the data processing pipeline, we will categorize the dataset. We are trying to categorize datasets so we can have a general idea of what kinds of data we have at the Arctic Data Center.\nDatasets will be categorized using the Academic Ontology. These annotations will be seen at the top of the landing page, and can be thought of as “themes” for the dataset. In reality, they are dataset-level annotations.\nBe sure to ask your peers in the #datateam slack channel whether they agree with the themes you think best fit your dataset. Once there is consensus, use the following line of code:\n\ndoc <- datamgmt::eml_categorize_dataset(doc, c(\"list\", \"of\", \"themes\"))"
  },
  {
    "objectID": "04_editing_eml.html#exercise-3a",
    "href": "04_editing_eml.html#exercise-3a",
    "title": "Editing EML",
    "section": "Exercise 3a",
    "text": "Exercise 3a\nThe metadata for the dataset created earlier in Exercise 2 was not very complete. Here we will add a attribute and physical to our entity (the csv file).\n\nMake sure your package from before is loaded into R.\nConvert otherEntity into dataTable.\nReplace the existing dataTable with a new dataTable object with an attributelist you write in R using the above commands.\nWe will continue using the objects created and updated in this exercise in 3b.\n\nBelow is some pseudo-code for how to accomplish the above steps. Fill in the dots according to the above sections to complete the exercise.\n\n# get the latest version of the resource map identifier from your dataset on the arctic data center\nresource_map_pid <- ...\ndp <- getDataPackage(d1c_test, identifier=resource_map_pid, lazyLoad=TRUE, quiet=FALSE)\n\n# get metadata pid\nmo <- selectMember(...)\n\n# read in EML\ndoc <- read_eml(getObject(...))\n\n# convert otherEntity to dataTable\ndoc <- eml_otherEntity_to_dataTable(...)\n\n# write an attribute list using shiny_attributes based on the data in your file\nex_data <- read.csv(...)\natts <- shiny_attributes(data = ex_data)\n\n# set the attributeList\ndoc$dataset$dataTable$attributeList <- set_attributes(...)"
  },
  {
    "objectID": "04_editing_eml.html#validate-eml-and-update-package",
    "href": "04_editing_eml.html#validate-eml-and-update-package",
    "title": "Editing EML",
    "section": "Validate EML and update package",
    "text": "Validate EML and update package\nTo make sure that your edited EML is valid against the EML schema, run eml_validate() on your EML. Fix any errors that you see.\n\neml_validate(doc)\n\nYou should see something like if everything passes: >[1] TRUE >attr(,“errors”) >character(0)\nThen save your EML to a path of your choice or a temp file. You will later pass this path as an argument to update the package.\n\neml_path <- \"path/to/save/eml.xml\"\nwrite_eml(doc, eml_path)"
  },
  {
    "objectID": "04_editing_eml.html#exercise-3b",
    "href": "04_editing_eml.html#exercise-3b",
    "title": "Editing EML",
    "section": "Exercise 3b",
    "text": "Exercise 3b\n\nMake sure you have everything from before in R.\n\nAfter adding more metadata, we want to publish the dataset onto test.arcticdata.io. Before we publish updates we need to do a couple checks before doing so.\n\nValidate your metadata using eml_validate.\nUse the checklist to review your submission.\nMake edits where necessary\n\nOnce eml_validate returns TRUE go ahead and run write_eml, replaceMember, and uploadDataPackage. There might be a small lag for your changes to appear on the website. This part of the workflow will look roughly like this:\n\n# validate and write the EML\neml_validate(...)\nwrite_eml(...)\n\n# replace the old metadata file with the new one in the local package\ndp <- replaceMember(dp, ...)\n\n# upload the data package\npackageId <- uploadDataPackage(...)"
  },
  {
    "objectID": "06_editing_sysmeta.html",
    "href": "06_editing_sysmeta.html",
    "title": "Editing system metadata",
    "section": "",
    "text": "Every object on the ADC (or the KNB) has “system metadata”. An object’s system metadata have information about the file itself, such as the name of the file (fileName), the format (formatId), who the rightsHolder is, what the accessPolicy is, and more. Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension.\nAlthough the majority of system metadata changes that need to be made are done automatically, sometimes we need to change aspects of the system metadata (or ‘sysmeta’ for short) manually."
  },
  {
    "objectID": "06_editing_sysmeta.html#edit-sysmeta",
    "href": "06_editing_sysmeta.html#edit-sysmeta",
    "title": "Editing system metadata",
    "section": "Edit sysmeta",
    "text": "Edit sysmeta\nTo edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command:\n\nsysmeta <- getSystemMetadata(d1c@mn, pid)\n\nThen edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command:\n\nsysmeta@fileName <- 'NewFileName.csv'\n\nNote that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule() (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access(), which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata() to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats.\n\n# Example of setting the formatId slot\nsysmeta@formatId <- \"eml://ecoinformatics.org/eml-2.1.1\"\n\nAfter you have changed the necessary slot, you can update the system metadata using the following command:\n\nupdateSystemMetadata(d1c@mn, pid, sysmeta)\n\n\nIdentifiers and sysmeta\nImportantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published).\n\n\nAdditional resources\nFor a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation:\n\nSystem Metadata\nData Types in CICore"
  },
  {
    "objectID": "06_editing_sysmeta.html#exercise-5",
    "href": "06_editing_sysmeta.html#exercise-5",
    "title": "Editing system metadata",
    "section": "Exercise 5",
    "text": "Exercise 5\nSometimes the system doesn’t recognize the file types properly. For example you have a csv file but the File type on the website says Microsoft Excel\n\nRead the system metadata in from the data file you uploaded previously.\nCheck to make sure the fileName and formatId are set correctly (the extension in fileName should match the formatId).\nUpdate the system metadata if necessary."
  },
  {
    "objectID": "02_creating_a_data_package.html",
    "href": "02_creating_a_data_package.html",
    "title": "Creating a data package",
    "section": "",
    "text": "This chapter will teach you how to create and submit a data package to a DataONE MN via R. But first, please read this paper on the value of structured metadata, namely the Ecological Metadata Language (EML)."
  },
  {
    "objectID": "02_creating_a_data_package.html#what-is-in-a-package",
    "href": "02_creating_a_data_package.html#what-is-in-a-package",
    "title": "Creating a data package",
    "section": "What is in a package?",
    "text": "What is in a package?\nA data package generally consists of at least 3 components.\n\nMetadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata are information that describe data (e.g. who made the data, how were the data made, etc.). The metadata file will be in an XML format, and have the extension .xml (extensible markup language). We often refer to this file as the EML, which is the metadata standard that it uses. This is also what you see when you click on a page in the Arctic Data Center.\nData: Other objects in a package are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, NetCDF files, plain text files, PDF documents, image files, etc.\nResource Map: The final object is the resource map. This object is a plain text file with the extension .rdf (Resource Description Framework) that defines the relationships between all of the other objects in the data package. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website with the metadata file and all of the data files together in the correct place. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R.\n\n\n\n\nFrom the DataOne Community Meeting (Session 7)"
  },
  {
    "objectID": "02_creating_a_data_package.html#packages-on-the-website",
    "href": "02_creating_a_data_package.html#packages-on-the-website",
    "title": "Creating a data package",
    "section": "Packages on the Website",
    "text": "Packages on the Website\nAll of the package information is represented when you go to the landing page for a dataset. When you make changes through R those published changes will be reflected here. Although you can edit the metadata directly from the webpage but we recommend to use R in most cases."
  },
  {
    "objectID": "02_creating_a_data_package.html#about-identifiers",
    "href": "02_creating_a_data_package.html#about-identifiers",
    "title": "Creating a data package",
    "section": "About identifiers",
    "text": "About identifiers\nEach object (metadata files, data files, resource maps) on the ADC or the KNB (another repo) has a unique identifier, also sometimes called a “PID” (persistent identifier). When you look at the landing page for a dataset, for example here, you can find the resource map identifier listed under the title in the gray bar after the words “Files in this dataset Package:” (resource_map_doi:10.18739/A2836Z), the metadata identifier in the “General > Identifier” section of the metadata record or after the title with blue font (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “Online Distribution Info” section (arctic-data.9546.1).\nNote, all datasets submitted are given a preliminary identifier (usually starting with urn:uuid:). When the dataset is finalized, a doi will be issued.\n\nDifferent versions of a package are linked together by what we call the “version chain” or “obsolescence chain”. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use datapack::uploadDataPackage() for updating the entire package on the latest versions of all objects to ensure that the version chain is maintained."
  },
  {
    "objectID": "02_creating_a_data_package.html#upload-a-package",
    "href": "02_creating_a_data_package.html#upload-a-package",
    "title": "Creating a data package",
    "section": "Upload a package",
    "text": "Upload a package\nWe will be using R to connect to the NSF Arctic Data Center (ADC) data repository to push and pull edits in actual datasets. To identify yourself as an admin you will need to pass a ‘token’ into R. Do this by signing in to the ADC with your ORCid and password, then hovering over your name in the top right corner and clicking on “My profile”, then navigating to “Settings” and “Authentication Token”, copying the “Token for DataONE R”, and finally pasting and running it in your R console.\n\n\n\n\n\n\nWarning\n\n\n\nThis token is your identity on these sites, please treat it as you would a password (i.e. don’t paste into scripts that will be shared). The easiest way to do this is to always run the token in the console. There’s no need to keep it in your script since it’s temporary anyway.\n\n\nYou will need to retrieve a new one after it either expires or you quit your R session.\nSometimes you’ll see a placeholder in scripts to remind users to get their token, such as:\n\noptions(dataone_test_token = \"...\")\n\n\n\n\n\n\n\nNote\n\n\n\nSince we will be working on the test site and not the production site, please remember to get your token from test.arcticdata.io\n\n\nNext, please be sure these packages are loaded for the training (these should already exist if you are working on the server):\n\nlibrary(devtools)\nlibrary(dataone)\nlibrary(datapack)\nlibrary(EML)\nlibrary(remotes)\nlibrary(XML)\nlibrary(uuid)\n\nIf any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package, then load them.\n\ninstall.packages(\"package_name\")\n\nNow you’ll install the arcticdatautils and datamgmt packages with the code below. If prompted to update packages during the installation process, skip the updates. Now, run the following code to install and load the libraries.\n\nremotes::install_github(\"nceas/arcticdatautils\")\nlibrary(arcticdatautils)\nremotes::install_github(\"nceas/datamgmt\")\nlibrary(datamgmt)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you are usually working with data packages, you will only need the following:\n\nlibrary(dataone) \nlibrary(datapack)\nlibrary(EML)\nlibrary(arcticdatautils)\n\n\n\nFor this training, we will be working exclusively on the Arctic test site, or “node.” In many of the functions you will use this will be the first argument. It is often referred to in documentation as mn, short for member node. More information on the other nodes can be found in the reference section under Set DataONE nodes Set DataONE nodes\nFor example, if we are using the test site, set the node to the test Arctic node:\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nOnce all set up you can first publish an object (data). If you are curious how everything magically works, here is a handy diagram:\n\n\n\nFrom the DataOne Community Meeting (Session 7)"
  },
  {
    "objectID": "02_creating_a_data_package.html#datapack-background",
    "href": "02_creating_a_data_package.html#datapack-background",
    "title": "Creating a data package",
    "section": "datapack Background",
    "text": "datapack Background\nadapted from the dataone and datapack vingettes\ndatapack is written differently than most R packages you may have encountered in the past. This is because it uses the S4 system instead.\n\nlibrary(dataone)\nlibrary(datapack)\nlibrary(uuid)\n\nData packages\nData packages are a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata.\n\nNavigating data packages\nNodes\nUsing this example on arcticdata.io\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nTo use the member node information, use the mn slot\n\nd1c_test@mn\n\n\n\n\n\n\n\nNote\n\n\n\nTo access the various slots using objects created by datapack and dataone (e.g. getSystemMetadata) requires the @ which is different from what you might have seen in the past. This is because these use the S4 system.\n\n\nGet an existing package from the Arctic Data Center. Make sure you know as you go through this training whether you are reading or writing to test or production. We don’t want to upload any of your test datasets to production!\n\nd1c <- dataone::D1Client(\"PROD\", \"urn:node:ARCTIC\")\ndp <- dataone::getDataPackage(d1c, \"resource_map_urn:uuid:1f9eee7e-2d03-43c4-ad7f-f300e013ab28\")\n\n\n\nData Objects\nCheck out the objects slot\n\ndp@objects\n\nGet the number for data and metadata files associated with this data package:\n\ngetSize(dp)\n\nGet the file names and corresponding pids\n\ngetValue(dp, name=\"sysmeta@fileName\")\n\nGet identifiers\nYou can search by any of the sysmeta slots such as fileName and formatId and get the corresponding identifier(s):\n\nmetadataId <- selectMember(dp, name=\"sysmeta@ADD THE NAME OF THE SLOT\", \n                           value=\"PATTERN TO SEARCH BY\")\n\nExample:\n\nselectMember(dp, name=\"sysmeta@formatId\", value=\"image/tiff\")\nselectMember(dp, name=\"sysmeta@fileName\", value=\"filename.csv\")\n\n\n\nProvenance\nView the provenance as a dataTable. We will get into detail in the Building provenance chapter.\n\ndp@relations$relations"
  },
  {
    "objectID": "02_creating_a_data_package.html#exercise-2a",
    "href": "02_creating_a_data_package.html#exercise-2a",
    "title": "Creating a data package",
    "section": "Exercise 2a",
    "text": "Exercise 2a\nSelect a dataset from the catalog on the Arctic Data Center. Observe the number of data files in the dataset. Try to find identifiers for the metadata file and resource map on the landing page for the dataset based on the screenshot shown above."
  },
  {
    "objectID": "02_creating_a_data_package.html#create-a-new-data-package",
    "href": "02_creating_a_data_package.html#create-a-new-data-package",
    "title": "Creating a data package",
    "section": "Create a new data package",
    "text": "Create a new data package\nadapted from the dataone and datapack vingettes\n\nlibrary(dataone)\nlibrary(datapack)\nlibrary(uuid)\n\nTo create a new data package, follow the code below. Remember, a data package is a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata.\n\ndp <- new(\"DataPackage\")\n\n\nUpload new data files\n\nCreate and add a metadata file\nIn this example we will use this previously written EML metadata. Here we are getting the file path from the dataone package and saving that as the object emlFile.\nThis is a bit of an unusual way to reference a local file path, but all this does is looks within the R package dataone and grabs the path to a metadata document stored within that package. If you print the value of emlFile you’ll see it is just a file path, but it points to a special place on the server where that package is installed. Usually you will just reference EML paths that are stored within your user file system.\n\nemlFile <- system.file(\"extdata/strix-pacific-northwest.xml\", \n                       package = \"dataone\")\n\nCreate a new DataObject and add it to the package. In the case below, our new DataObject will be a metadata file.\n\nmetadataObj <- new(\"DataObject\", \n                   format = \"https://eml.ecoinformatics.org/eml-2.2.0\", \n                   filename = emlFile)\n\ndp <- addMember(dp, metadataObj)\n\nCheck the dp object to see if the DataObject was added correctly.\n\ndp\n\n\n\nAdd some additional data files\n\nsourceData <- system.file(\"extdata/OwlNightj.csv\", package = \"dataone\")\n\nsourceObj <- new(\"DataObject\", format = \"text/csv\", filename = sourceData)\n\ndp <- addMember(dp, sourceObj, metadataObj)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to change the formatId please use updateSystemMetadata (more on this later in the book)\n\n\n\n\n\nUpload the package\n\nd1c <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nMake sure to give access privileges to the ADC admins. Although you may be tempted to edit the format of the string in the subject argument, you must keep it exactly as is. Otherwise you’ll run into error messages!\n\nmyAccessRules <- data.frame(subject = \"CN=arctic-data-admins,DC=dataone,DC=org\", \n                            permission = \"changePermission\") \n\nGet necessary token from test.arcticdata.io to upload the dataset prior uploading the datapackage:\n\npackageId <- uploadDataPackage(d1c, dp, public = TRUE, \n                               accessRules = myAccessRules, quiet = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to preserve folder structures, you can use this method\nIn this example, adding the csv files to a folder named data and scripts:\n\noutputData <- system.file(\"extdata/Strix-occidentalis-obs.csv\", package=\"dataone\") \n\noutputObj <- new(\"DataObject\", format = \"text/csv\", filename = outputData,\n                 targetPath = \"data\")\n\ndp <- addMember(dp, outputObj, metadataObj)\n\nprogFile <- system.file(\"extdata/filterObs.R\", package = \"dataone\")\n\nprogObj <- new(\"DataObject\", format = \"application/R\", filename = progFile, \n               targetPath = \"scripts\", mediaType = \"text/x-rsrc\")\n\ndp <- addMember(dp, progObj, metadataObj)"
  },
  {
    "objectID": "02_creating_a_data_package.html#exercise-2b",
    "href": "02_creating_a_data_package.html#exercise-2b",
    "title": "Creating a data package",
    "section": "Exercise 2b",
    "text": "Exercise 2b\nThis exercise will take you through how to do the submission process through R instead of the webform (exercise 1).\n\nPart 1 - Gather your data files\nFor our convenience, we will be grabbing the metadata and data files from the file we published earlier:\n\nLocate the data package you published in Exercise 1 by navigating to the “My Profile > My Data” section on test.arcticdata.io.\nDownload the metadata and data files and transfer them to the Datateam server.\n\n\n\nPart 2 - Working in R\nNow we want to publish the metadata and data files we downloaded again to test.arcticdata.io\n\nObtain a token and please note that for this exercise please make sure you grab the token from the  arcticdata test site\nPublish your metadata and data file to the site.\n\n\n#set the node\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\ndp <- new(\"DataPackage\")\n\n#add your metadata\nmetadataObj <- new(...)\ndp <- addMember(...)\n\n#add your data files\nsourceObj <- new(...)\ndp <- addMember(...)\n\n#upload your package\nmyAccessRules <- data.frame(...) \npackageId <- uploadDataPackage(...)\n\n\nView your new data set by appending the metadata PID to the end of the URL test.arcticdata.io/#view/…\nIf you are successful it should look the same as the dataset you created in exercise 1"
  },
  {
    "objectID": "workflows/pi_correspondence/large_file_transfer.html",
    "href": "workflows/pi_correspondence/large_file_transfer.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Often if the researcher has many files to upload or the files are large in size, we will need to provide alternative methods of upload. We currently have two options available for large file transfer where sFTP is easier to set up but slower while GLOBUS takes a bit more setup but faster.\nBefore providing access, try to get a sense of how big the transfer will be. df -h on Arctic server and datateam server - look at current space available\n\n\nAs a team member for login info.\n\nAfter you login, you can add a folder with your last name to add your files. If you need more detailed access instructions, I have included the link here for your convenience: https://help.nceas.ucsb.edu/NCEAS/help/remote_file_access\n\nOption 1: SSH via the Terminal https://pages.github.nceas.ucsb.edu/NCEAS/help/connecting_to_linux_using_ssh.html\nOption 2: Cyberduck instructions\nTo use Cyberduck to transfer local files onto the Datateam server:\n\nOpen Cyberduck.\nCheck that you have the latest version (Cyberduck -> Check for Update…). If not, download and install the latest (you may need Jeanette to enter a password).\nClick “Open Connection”.\nFrom the drop-down, choose “SFTP (Secure File Transfer Protocol)”.\nEnter “datateam.nceas.ucsb.edu” for Server.\nEnter your username and password.\nConnect.\n\nFrom here, you can drag and drop files to and from the server.\n\n\n\n\narctic-data-center\n#arctic-data-center\nlogin using your credentials on the datateam server\n\nRemember to remove the files on the datateam server after you finish a ticket to free up space."
  },
  {
    "objectID": "workflows/pi_correspondence/replicate_dataset.html",
    "href": "workflows/pi_correspondence/replicate_dataset.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Sometimes we get requests to have their datasets replicated to ADC. Usually it is a dataset on EDI where you can ge the Digital Object Identifier: which serves as the resource map on DataOne\n\n\n\n\n\n\nNote\n\n\n\nIf a link isn’t given, you can use the package ID (i.e. knb-lter-arc.20129.1) and add it to the end of this link: https://portal.edirepository.org/nis/mapbrowse?packageid=\n\n\nIf we are unsure about the identifer you can try querying for it on the CN:\n\ncn <- CNode('PROD')\n\n#find the DOIs\nresult <- query(cn, list(q = paste0(\"(id:*10.6073/pasta/d4f567844673857239eec0cb61c6f543\",\"* *:* NOT obsoletedBy:*)\"),\n                         fl = \"identifier,rightsHolder,formatId, fileName, dateUploaded, authoritativeMN, replicaMN\",\n                         sort = 'dateUploaded+desc',\n                         start =\"0\",\n                         rows = \"1500\"),\n                as=\"data.frame\")\n\n\n\n\nTry cloning to the test node first to see how it will look. Set up your nodes\n\n\nfrom <- dataone::D1Client(\"PROD\", \"urn:node:LTER\")\nto <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\n\n\n\n\n\n\nNote\n\n\n\nFor LTER datasets on EDI, there are two possibilities for the mn: \"urn:node:LTER\" and \"urn:node:EDI\". Try the other one if one isn’t working\n\n\n\nUse clone_package to copy it over to the test node\n\n\nclone_package(\"doi:10.6073/pasta/d4f567844673857239eec0cb61c6f543\", #example doi to replicate\n              from = from,\n              to = to,\n              add_access_to = \"http://orcid.org/0000-0001-8888-547X\",\n              change_auth_node = F,\n              new_pid = T)\n\n\nOnce you have verified that it clones properly to the test node, do the same in production but with a couple of minor modifications:\n\n\nfrom <- dataone::D1Client(\"PROD\", \"urn:node:LTER\")\nto <- dataone::D1Client(\"PROD\", \"urn:node:ARCTIC\")\n\nclone_package(\"doi:10.6073/pasta/0af82d3c3d9d1710775cf9b1464ce70b\",\n              from = from,\n              to = to,\n              add_access_to = \"http://orcid.org/0000-0001-8888-547X\",\n              change_auth_node = F,\n              new_pid = F) #uses the same pid\n\n\n\n\n\n\n\nNote\n\n\n\nmake sure the new_pid argument is set to F when you publish to production"
  },
  {
    "objectID": "workflows/pi_correspondence/final_review_checklist.html",
    "href": "workflows/pi_correspondence/final_review_checklist.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "You can click on the assessment report on the website to for a general check. Fix anything you see there.\nSend the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive):\n\n\nPlease refer to the dedicated pages for instructions to handle these cases:\n\nMOSAiC\nDBO\n\n\n\n\nThe format ids are correct\n\n\n\nIncluded lines for FAIR:\n\ndoc <- eml_add_publisher(doc)\ndoc <- eml_add_entity_system(doc)\n\n\n\n\n\nNo abbreviations, should include geographic and temporal coverage\n\n\n\n\n\nlonger than 100 words\nno abbreviations or garbled text\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\n\n\n\n\nin the correct one: DataTable / OtherEntity / SpatialVector / SpatialRaster for the file type\nentityDescription - longer than 5 words and unique\nphysical present and format correct\n\n\n\n\ncomplete\nattributeDefinitions longer than 3 words\nVariables match what is in the file\nMeasurement domain - if appropirate (ie dateTime correct)\nMissing Value Code - accounted for if applicable\nSemantic Annotation - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc.\n\n\n\n\n\n\ncomplete information for each person in each section\n\nincluding ORCID and e-mail address for all contacts\npeople repeated across sections should have consistent information\n\n\n\n\n\n\nthe map looks correctand matches the geographic description\ncheck if negatives (-) are missing\n\n\n\n\n\nif it is an NSF award you can use the helper function:\n\ndoc$dataset$project <- eml_nsf_to_project(awards)\n\nfor other awards that need to be set manually, see the set project page\n\n\n\n\n\npresent\nno garbled text\n\n\n\n\n\ncurrently using: eml-2.2.0 (as of July 30 2020)\nreview to see if the EML version is set correctly by reviewing the doc$`@context` that it is indeed 2.2.0 under eml\nRe-run your code again and have the lineemld::eml_version(\"eml-2.2.0\") at the top\nMake sure the system metadata is also 2.2.0\n\n\n\n\n\nGranted access to PI using set_rights_and_access()\n\nmake sure it is http:// (no s)\n\nnote if it is a part of portals there might be specific access requirements for it to be visible using set_access()\n\n\n\n\n\nif there are files transferred to us via SFTP, delete those files when the ticket is resolved\n\n\n\n\nAll the above applies. These are some areas to do a closer check when users update with a new file:\n\nNew data was added\n\nTemporal Coverage and Title\nand follow usual protocols\n\nFiles were replaced\n\nupdate physical and entityName\ndouble-check attributes are the same\ncheck for any new missing value codes that should be accounted for\n\nWas the dataset published before 2021?\n\nupdate project info , annotations\n\nGlance over entire page for any small mistakes (ie. repeated additionalMetadata, any missed &amps, typos)"
  },
  {
    "objectID": "workflows/pi_correspondence/navigate_rt.html",
    "href": "workflows/pi_correspondence/navigate_rt.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center.\nWe use it for managing submissions, accessing issues, etc. It consists of three separate interfaces:\nFront Page\nAll Tickets\nTicket Page\n\n\n\nThis is what you see first\n\nHome - brings you to this homepage\n\nTickets - to search for tickets (also see number 5)\n\nTools - not needed\n\nNew Ticket - create a new ticket\n\nSearch - Type in the ticket number to quickly navigate to a ticket\n\nQueue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses\n\n\n\nNew = unopened tickets that require attention\n\nOpen = tickets currently open and under investigation and/or being processed by a support team member\n\nStalled = tickets awaiting responses from the PI/ submitter\n\n\n\nTickets I Own - These are the current open tickets that are claimed by me\n\nUnowned Tickets - Newest tickets awaiting claim\n\nTicket Status - Status and how long ago it was created\n\nTake - claim the ticket as yours\n\n\n\n\n\nThis is the queue interface from number 6 of the Front page\n1. Ticket number and title\n2. Ticket status\n3. Owner - who has claimed the ticket\n\n\n\n\n\nTitle - Include the PI’s name for reference\n\nDisplay - homepage of the ticket\n\nHistory - Comment/Email history, see bottom of Display page\n\nBasics - edit the title, status, and ownership here\n\nPeople - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters\n\nLinks - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number\n\n\n\n\n\n\n\nWarning\n\n\n\nVerify that this is indeed the two tickets you want to merge. It is non-reversible.\n\n\n\nActions\n\n\n\nReply - message the submitter/ PI/ all watchers\n\nComment - attach internal message (no submitters, only Data Teamers)\n\nOpen It - Open the ticket\n\nStall - submitter has not responded in greater than 1 month\n\nResolve - ticket completed\n\n\n\nHistory - message history and option to reply (to submitter and beyond) or comment (internal message)\n\n\n\n\nWhen notified by Arcticbot about a new data submission, here are the typical steps:\n\nUpdate the Requestor under the People section based on the email given in the submission (usually the user/ PI/ submitter). You may have to google for the e-mail address if the PI did not include it in the metadata record.\nTake the ticket (Actions > Take)\nReview the submission based on the checklist\nDraft an email using the template and let others review it via Slack\nSend your reply via Actions"
  },
  {
    "objectID": "workflows/pi_correspondence/email_templates/C_additional_email_templates.html",
    "href": "workflows/pi_correspondence/email_templates/C_additional_email_templates.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "If the PI is checking about dates/timing: > [give rough estimate of time it might take] > Are you facing any deadlines? If so, we may be able to expedite publication of your submission.\n\n\n\nIf the PI needs a DOI right away:\n\nWe can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\n\n\n\nWhich of the following categories best describes the level of sensitivity of your data?\n\n\nA. Non-sensitive data None of the data includes sensitive or protected information. Proceed with uploading data. B. Some or all data is sensitive but has been made safe for open distribution Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Proceed with uploading data, but ensure that only data that are safe for public distribution are uploaded. Address questions about anonymization, aggregation, de-identification, and data embargoes with the data curation support team before uploading data. Describe these approaches in the Methods section. C. Some or all data is sensitive and should not be distributed The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Do NOT upload sensitive data. You should still upload a metadata description of your dataset that omits all sensitive information to inform the community of the dataset’s existence. Contact the data curation support team about possible alternative approaches to safely preserve sensitive or protected data.\n\n\n\nEthical Research Procedures. Please describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Institutional Review Board approvals, consent waivers, procedures for co-production, data sovereignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution.\n\n\n\n\n\n\nAs a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets.\n\n\n\n\nPlease email them before resolving a ticket like this:\n\nWe are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email.\n\n\n\n\n\nTo recover dataset submissions that were not successful please do the following:\n\n\n\nGo to https://arcticdata.io/catalog/drafts\nFind your dataset and download the corresponding file\nSend us the file in an email\n\n\n\n\n\n\nYou could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=“your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER\n\n\n\n\n\nKNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R.\n\n\nHere are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")\n\n\nIf you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well.\n\n\n\n\n\nIf linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533\n\n\n\n\n\nPlease find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp.\n\n\nInvestigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used.\n\n\n\n\n\nFirst create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage.\n\n\nAfter completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface.\n\n\nOr, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage."
  },
  {
    "objectID": "workflows/pi_correspondence/email_templates/B_final_email_templates.html",
    "href": "workflows/pi_correspondence/email_templates/B_final_email_templates.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Hi [submitter],\n\n\nI have updated your data package and you can view it here after logging in: [URL]\n\n\nPlease review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI.\n\n\nAfter publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version.\n\n\nPlease let us know if you have any questions.\n\n\n\n\nReplying to questions about DOIs\n\nWe attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version.\n\n\nDOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\nClarification of updating with a DOI and version control\n\nWe definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it).\n\n\n\n\nSending finalized URL and dataset citation before resolving ticket\n[NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)]\n\nHere is the link and citation to your finalized data package:\n\n\nhttps://doi.org/10.18739/A20X0X\n\n\nFirst Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X.\n\n\nIf in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\nPlease let us know if you need any further assistance."
  },
  {
    "objectID": "workflows/pi_correspondence/email_templates/A_initial_email_template.html",
    "href": "workflows/pi_correspondence/email_templates/A_initial_email_template.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Comment templates based on what is missing\n\nPortals\nMultiple datasets under the same project - suggest data portal feature\n\nI would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project.\n\nIf they ask to nest the dataset\n\nWe are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email.\n\n\n\nDataset citations\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point.\n\n\n\nTitle\nProvides the what, where, and when of the data\n\nWe would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’.\n\nDoes not use acronyms\n\nWe wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE]\n\n\n\nAbstract\nDescribes DATA in package (ideally > 100 words)\n\nWe would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING]\n\n\nThe motivation of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nAll acronyms are defined\nAt least 100 words long\n\nOffer this if submitter is reluctant to change:\n\nIf you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL].\n\n\n\nKeywords\n\nWe noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others.\n\n\n\nData\nSensitive Data\nWe will need to ask these questions manually until the fields are added to the webform.\n\nData sensitivity categories\n\nOnce we have the ontology this question can be asked:\n\nBased on our Data sensitivity categories, which of the 3 does your dataset align with most:\n\n\nNon-sensitive data - None of the data includes sensitive or protected information.\nSome or all data is sensitive with minimal risk - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized.\nSome or all data is sensitive with significant risk - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body.\n\n\nEthical research proceedures\n\n\nWe were wondering if you could also address this question specifically on Ethical Research Procedures: Describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. We can help add your answers to the question to the metadata.\n\nAdding provenance\n\nIs the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2\n\nAt least one data file\n\nWe noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process.\n\nOpen formats\nExample using xlsx. Tailor this reponse to the format in question.\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nThe data files can be replaced by going to the green Edit button > Click the black triangle by the Describe button for the data file > Select Replace (attached is also a screenshot on how to get there). \n\nZip files\n\nExcept for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way.\n\nFile contents and relationships among files are clear\n\nCould you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use.\n\nData layout\n\nWould you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent?\n\nWe try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat.\n\nWe would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\n\nAttributes\nIdentify which attributes need additional information. If they are common attributes like date and time we do not need further clarification.\nChecklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets):\n\nA name (often the column or row header in the file).\nA complete definition.\nAny missing value codes along with explanations for those codes.\nFor all numeric data, unit information is needed.\nFor all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”).\nFor text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes.\n\nHelpful templates: > We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] > Could you describe ____? > Please define “XYZ”, including the unit of measure. > What are the units of measurement for the columns labeled “ABC” and “XYZ”?\nMissing value codes\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf.\n\n\nWe noticed that the data files contain [blank cells - replace with missing values found]. What do these represent?\n\n\n\nFunding\nAll NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization.\n\nWe noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations.\n\n\n\nMethods\nWe noticed that methods were missing from the submission. Submissions should include the following:\n\nprovide instrument names (if applicable)\nspecify how sampling locations were chosen\nif citations for sampling methods are used, please provide a brief summary of the methods referenced\nany software used to process the data\n\nNote - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software)\n\nYour methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.]\n\n\nComprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable.\n\nA full example - New Submission: methods, excel to csv, and attributes\n\nThank you for your recent submission to the NSF Arctic Data Center!\n\n\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point.\n\n\nWe would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records.\n\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nWe also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. \n\n\nOur data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice.\n\n\nWe noticed that methods were missing from the submission. Submissions should: - provide instrument names (if applicable) - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail - any software used to process the data\n\n\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n\n\nBest,\n\n\nName"
  },
  {
    "objectID": "workflows/pi_correspondence/A_email_templates.html",
    "href": "workflows/pi_correspondence/A_email_templates.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This section covers new data packages submitted. For other inquiries see the PI FAQ templates\nPlease think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible.\nIn your first few months, please run email drafts by the #datateam Slack and get approval before sending.\nRemember to consult the submission guidelines for details of what is expected.\nQuick reference:\n\nInitial email template\nFinal email templates\nAdditional email template\n\n\n\n\n\n\nHello [NAME OF REQUESTOR], Thank you for your recent submission to the NSF Arctic Data Center!\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n[COMMENTS HERE]\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\nBest,\n[YOUR NAME]\n\n\n\n\n\n\nMultiple datasets under the same project - suggest data portal feature\n\nI would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project.\n\nIf they ask to nest the dataset\n\nWe are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email.\n\n\n\n\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point.\n\n\n\n\nProvides the what, where, and when of the data\n\nWe would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’.\n\nDoes not use acronyms\n\nWe wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE]\n\n\n\n\nDescribes DATA in package (ideally > 100 words)\n\nWe would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING]\n\n\nThe motivation of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nAll acronyms are defined\nAt least 100 words long\n\nOffer this if submitter is reluctant to change:\n\nIf you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL].\n\n\n\n\n\nWe noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others.\n\n\n\n\nSensitive Data\nWe will need to ask these questions manually until the fields are added to the webform.\n\nData sensitivity categories\n\nOnce we have the ontology this question can be asked:\n\nBased on our Data sensitivity categories, which of the 3 does your dataset align with most:\n\n\nNon-sensitive data - None of the data includes sensitive or protected information.\nSome or all data is sensitive with minimal risk - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized.\nSome or all data is sensitive with significant risk - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body.\n\n\nEthical research proceedures\n\n\nWe were wondering if you could also address this question specifically on Ethical Research Procedures: Describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. We can help add your answers to the question to the metadata.\n\nAdding provenance\n\nIs the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2\n\nAt least one data file\n\nWe noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process.\n\nOpen formats\nExample using xlsx. Tailor this reponse to the format in question.\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nThe data files can be replaced by going to the green Edit button > Click the black triangle by the Describe button for the data file > Select Replace (attached is also a screenshot on how to get there). \n\nZip files\n\nExcept for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way.\n\nFile contents and relationships among files are clear\n\nCould you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use.\n\nData layout\n\nWould you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent?\n\nWe try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat.\n\nWe would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\n\n\nIdentify which attributes need additional information. If they are common attributes like date and time we do not need further clarification.\nChecklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets):\n\nA name (often the column or row header in the file).\nA complete definition.\nAny missing value codes along with explanations for those codes.\nFor all numeric data, unit information is needed.\nFor all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”).\nFor text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes.\n\nHelpful templates: > We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] > Could you describe ____? > Please define “XYZ”, including the unit of measure. > What are the units of measurement for the columns labeled “ABC” and “XYZ”?\nMissing value codes\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf.\n\n\nWe noticed that the data files contain [blank cells - replace with missing values found]. What do these represent?\n\n\n\n\nAll NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization.\n\nWe noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations.\n\n\n\n\nWe noticed that methods were missing from the submission. Submissions should include the following:\n\nprovide instrument names (if applicable)\nspecify how sampling locations were chosen\nif citations for sampling methods are used, please provide a brief summary of the methods referenced\nany software used to process the data\n\nNote - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software)\n\nYour methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.]\n\n\nComprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable.\n\nA full example - New Submission: methods, excel to csv, and attributes\n\nThank you for your recent submission to the NSF Arctic Data Center!\n\n\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point.\n\n\nWe would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records.\n\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nWe also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. \n\n\nOur data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice.\n\n\nWe noticed that methods were missing from the submission. Submissions should: - provide instrument names (if applicable) - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail - any software used to process the data\n\n\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n\n\nBest,\n\n\nName\n\n\n\n\n\n\n\n\nHi [submitter],\n\n\nI have updated your data package and you can view it here after logging in: [URL]\n\n\nPlease review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI.\n\n\nAfter publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version.\n\n\nPlease let us know if you have any questions.\n\n\n\n\nReplying to questions about DOIs\n\nWe attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version.\n\n\nDOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\nClarification of updating with a DOI and version control\n\nWe definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it).\n\n\n\n\nSending finalized URL and dataset citation before resolving ticket\n[NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)]\n\nHere is the link and citation to your finalized data package:\n\n\nhttps://doi.org/10.18739/A20X0X\n\n\nFirst Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X.\n\n\nIf in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\nPlease let us know if you need any further assistance.\n\n\n\n\n\n\n\nIf the PI is checking about dates/timing: > [give rough estimate of time it might take] > Are you facing any deadlines? If so, we may be able to expedite publication of your submission.\n\n\n\nIf the PI needs a DOI right away:\n\nWe can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\n\n\n\nWhich of the following categories best describes the level of sensitivity of your data?\n\n\nA. Non-sensitive data None of the data includes sensitive or protected information. Proceed with uploading data. B. Some or all data is sensitive but has been made safe for open distribution Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Proceed with uploading data, but ensure that only data that are safe for public distribution are uploaded. Address questions about anonymization, aggregation, de-identification, and data embargoes with the data curation support team before uploading data. Describe these approaches in the Methods section. C. Some or all data is sensitive and should not be distributed The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Do NOT upload sensitive data. You should still upload a metadata description of your dataset that omits all sensitive information to inform the community of the dataset’s existence. Contact the data curation support team about possible alternative approaches to safely preserve sensitive or protected data.\n\n\n\nEthical Research Procedures. Please describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Institutional Review Board approvals, consent waivers, procedures for co-production, data sovereignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution.\n\n\n\n\n\n\nAs a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets.\n\n\n\n\nPlease email them before resolving a ticket like this:\n\nWe are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email.\n\n\n\n\n\nTo recover dataset submissions that were not successful please do the following:\n\n\n\nGo to https://arcticdata.io/catalog/drafts\nFind your dataset and download the corresponding file\nSend us the file in an email\n\n\n\n\n\n\nYou could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=“your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER\n\n\n\n\n\nKNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R.\n\n\nHere are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")\n\n\nIf you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well.\n\n\n\n\n\nIf linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533\n\n\n\n\n\nPlease find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp.\n\n\nInvestigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used.\n\n\n\n\n\nFirst create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage.\n\n\nAfter completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface.\n\n\nOr, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage."
  },
  {
    "objectID": "workflows/pi_correspondence/email_templates_pi_faqs.html",
    "href": "workflows/pi_correspondence/email_templates_pi_faqs.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Q: I would like to / have already uploaded data to ____ repository\n\nReview the repository, does it issue persistent identifiers (ie. DOIs)?\nDo they say how long they will be commiting to storing the data?\n\nIf the answer is no to either questions, we will need to document and store any files in our own system. If any of this is unclear from what you can find online, you can message Matt to discuss with NSF if a particular repository will be allowed.\nPossible response for a generalist repository\n\nIf your data doesn’t require features that only ____ provides, we would encourage you to consider publishing both the data and the metadata at the Arctic Data Center. Some benefits to submitting to us includes:\n\n\nWe provide a DOI that can be cited in your published papers\nWe track where datasets are being cited\nBy submitting both the data and metadata to us, you can fulfill your NSF reporting requirements at the same time. This will reduce the work needed on your part to get any additional metadata requirements for NSF.\n\nQ: Can I replace data that have already been uploaded and keep the DOI?\n\nA: Once you have published your data with the Arctic Data Center, it can still be updated by providing an additional version which can replace the original, while still preserving the original and making it available to anyone who might have cited it. To update your data, return to the data submission tool used to submit it, and provide an update.\n\n\nAny update to a data package qualifies as a new version and therefore requires a new DOI. This is because each DOI represents a unique, immutable version, just like for a journal article. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to the latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\n\nDatateam: please review this dataset again to make sure it follows current standards\n\nQ: Why don’t I see my data package on the ADC?\n\nPossible Answer #1: The data package is still private because we are processing it or awaiting your approval to publish it. Please login with your ORCID iD to view private data packages.\n\n\nPossible Answer #2: The data package is still private and you do not have access because you were not the submitter. If you need access please have the submitter send us a message from his/her email address confirming this, along with your ORCID iD. Once we receive that confirmation we will be happy to grant you permission to view and edit the data package.\n\n\nPossible Answer #3: The data package is still private and we accidentally failed to grant you access. We apologize for the mistake. We have since updated the access policy. Please let us know if you are still having trouble viewing the data package here: [URL]. Remember to login with your ORCID iD.\n\n\nPossible Answer #4: The URL was garbled by their email server (eg. URL Defense or the url was split into two lines). You can navigate to your dataset by going to your name in the top right and selecting my datasets.\n\nIssue: MANY files to upload (100s or 1000s) or large cumulative size.\n\nA: Datateam - Prior to accepting large uploads, take the time to make sure there is enough space available and uploading all the files is necessary. If it is not a NSF dataset see the section on Scope.\n\n\nWe have a secure FTP you can access. Details are available here: https://help.nceas.ucsb.edu/remote_file_access . Please access our server at datateam.nceas.ucsb.edu with the username “visitor”. Let us know if you would like to use our SFTP and we will send you the password and the path to which directory to upload to.\n\n\nIf you have files to transfer to us that total several terabytes it may be best to arrange a shipment of an external hard drive.\n\nQ: Can I add another data file to an existing submission without having to fill out another metadata form?\n\nA: Yes. Navigate to the data package after being sure to login. Then click the green “Edit” button. The form will populate with the already existing metadata so there is no need to fill it out again. Click “Add Files” and browse to the file you wish to add.\n\n\nBe aware that the DOI will change after you add this file (or make any changes to a data package) as, just like for a journal article, a DOI represents a unique and immutable version. The original URL and DOI will remain functional and valid, but clearly display a message at the top of that page stating that “A newer version of this dataset exists” with a link to the latest version. Only the newest version wil be discoverable via a search.\n\n\nWe can also copy the metadata from existing dataset. Please send us the URL of the dataset you would like to copy and the title of the new dataset and we will be able to do that on your behalf.\n\nQ: I want to organize the order of the files/metadata entities\n\nA: We can reorganize entities within their entities. This is a limitation with our tools (EML).\n\n\nWhile re-ordering the groups of entities is possible manually, it isn’t possible the way write_eml and publish_update is written. We do not recommended reordering entities this way.\n\nQ: Can we submit data as an Excel file?\n\nA: While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. For instance, while Microsoft Excel files are commonplace, it is better to export these spreadsheets to Comma Separated Values (CSV) text files, which can be read on any computer without needing to have Microsoft products installed. So, yes, you are free to submit an Excel workbook, however we strongly recommend converting each sheet to a CSV. The goal is not only for users to be able to read data files, but to be able to analyze them with software, such as R Studio. Typically, we would extract any plots and include them as separate image files.\n\n[ONLY SAY THIS NEXT PART IF THE REQUESTOR CONTINUES TO INSIST and then USE PROV TO POINT FROM THE XLS TO THE CSVs]\n\nI understand that having the plots in the same file as the data they are built from simplifies organization. If you definitely prefer to have the Excel workbook included, we ask that you allow us to document the data in both formats and include a note in the metadata clarifying that the data are indeed duplicated (but in different formats).\n\nQ: Can we submit model output?\n\nA: Yes you can submit your model output to the Arctic Data Center. We also recommend including model source code directly in the dataset as it is critical to understand the model output dataset and will allow other researchers to re-produce your output. If the model is complex or it’s important to maintain file structure then it can submitted as a zip file. We also recommend included a detailed readme file that documents: version information, licensing information, a list of software/hardware used in development, a list of software/hardware dependencies needed to run the software, information detailing source data for models, any mathematical/physical explanations needed to understand models, any methods used to evaluate models, and instructions how to run the model if not including in the source code as comments.\nPlease read our software submission guidelines for more detailed information: https://arcticdata.io/submit/#metadata-guidelines-for-software\n\n\n\n\nQ: May another person (e.g. my student) submit data using my ORCID iD so that it is linked to me?\n\nA: We recommend instead that the student set up their own ORCiD account at https://ORCiD.org/register and submit data packages from that account. Submissions are processed by our team and, at that point, we can grant you full rights to the metadata and data files even though another person submitted them.\n\nIssue: Web form not cooperating.\n\nA: To help us diagnose the problem, could you let us know the following:\n\n\nWhich operating system (including the version) and browser (with version #) are you using?\nAt which exact step did the issue arise?\nWhat error message did you receive?\nDo you have any reason to believe that you may be using a slow internet connection?\n\n\nPlease provide us with any relevant screenshots and we will troubleshoot from there.\n\nIssue: Can’t log in to the Arctic Data Center\n\nA: Please accept our apologies that you are experiencing difficulty logging in. We suggest trying these troubleshooting steps:\n\n\nTry a different internet browser (Chrome, Firefox, Internet Explorer etc.)\nTry clearing your cache and then re-logging in with orcid Id\nPrivacy blockers (like EFF’s privacy badger https://www.eff.org/privacybadger) block the cookies needed for the login to work. To enable the cookies you need to disable your privacy tracker for arcticdata.io.\n\n\nIf the problem persists please let us know what step the error occured or if there were error messages you have recieved.\n\n\n\n\nQ: Can I submit a non-NSF funded dataset?\nSample text 1:\n\nThe Arctic Data Center is open to all arctic-related data. For larger data packages, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Please let us know the size of your dataset in terms of total number and total size of all files. We look forward to receiving your submission!\n\nSample text 2:\n\nYes, you can submit non-NSF-funded Arctic data if you are willing to submit under the licensing terms of the Arctic Data Center (CC-0 or CC-BY), the data are moderately sized (with exact limits open to discussion), and a lot of support time to curate the submission is not required (i.e., you submit a complete metadata record and well formatted, open format data files). For larger data packages, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Also, please note that NSF-funded projects take priority when it comes to processing. Information on best practices for data and metadata organization is available here: https://arcticdata.io/submit/#organizing-your-data.\n\nQ: Is ______ considered within the arctic\n\nWe don’t have a precise geographic cutoff. We definitely will preserve any dataset that is funded by the Arctic section of the National Science Foundation, which includes many sub-Arctic studies. So, as long as the dataset is related to Arctic research defined broadly, we would consider it. We will prioritize NSF-funded projects because that is our main mandate, but we will also preserve non-NSF data about the Arctic as time allows. If you send more details about the study, its subject, location, and funding, we could be more definitive about whether we could archive it."
  },
  {
    "objectID": "workflows/pi_correspondence/initial_review_checklist.html",
    "href": "workflows/pi_correspondence/initial_review_checklist.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Before responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed.\n\n\nIf any of the below is in the dataset, please alert the #arctica team know before proceeding.\n\nCheck if there is any sensitive information or personal identifying information in the data (eg. Names)\nCan the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers)\nDryad Human Subject data guidelines can be a good place to start\n\nCommon Cases:\n\nSocial Science: Any dataset involving human subjects (may include awards awarded by ASSP and topics such as COVID-19)\nArchaeology: archaeological site location information, which is protected from public access by law\nBiology: protected species location coordinates\n\n\n\n\n\nIf the dataset appears to be in a publication please (might be in the abstract) make sure that those citations are registered.\n\n\n\n\n\nWHAT, WHERE, and WHEN:\n\nIs descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage\nProvides a location of the work from the local to state or country level\nProvides a time frame of the work\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\n\n\n\n\n\n\nDescribes the DATA as well as:\n\nThe motivation (purpose) of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\nAt least 100 words total\n\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\nAny citations to papers can be registered with us\n\n\n\n\n\n\nSome keywords are included\n\n\n\n\n\nData is normalized (if not suggest to convert the data if possible)\nAt least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program)\nNo xls/xlsx files (or other proprietary files)\nFile contents and relationships among files are clear\nEach file is well NAMED and DESCRIBED and clearly differentiated from all others\nAll attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. Quality control all dimensionless units.\nMissing value codes are explained (WHY are the data absent?)\nIf it is a .rar file  -> scan the file\nIf there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already\n\n\n\n\n\nAt least one contact and one creator with a name, email address, and ORCID iD\n\n\n\n\n\nIncludes coverages that make sense\n\nTemporal coverage - Start date BEFORE end date\nGeologic time scales are added if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic)\nSpatial coverage matches geographic description (check hemispheres)\nGeographic description is from the local to state or country level, at the least\nTaxonomic coverage if appropriate\n\n\n\n\n\n\nAt least one FUNDING number\nTitle, personnel, and abstract match information from the AWARD (not from the data package)\n\n\n\n\n\nThis section is REQUIRED for ALL NSF-FUNDED data packages\nEnough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources\n\n\n\n\n\nIf there are multiple submissions from the same people/project let them know about the portals feature\nIf this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette and see the data portals section."
  },
  {
    "objectID": "workflows/pi_correspondence/example_data_packages.html",
    "href": "workflows/pi_correspondence/example_data_packages.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Modeling\n\nModel with output:\n\nhttps://doi.org/10.18739/A24J09X55\n\nModel code archived as a zip if preverving directory structure is important\n\nhttps://doi.org/10.18739/A2JS9H795\n\nModel with provenance:\n\nhttps://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Ae6390181-85e9-46a2-b2ba-e352ece51cc6\n\nCode and output:\n\nNote: We can only archive the code if the submitter has the rights to redistribute it (like if they wrote it themselves, or if the code has a license that allows for that)\nhttps://doi.org/10.18739/A2XS5JH4N\n\nGuidelines for large outputs:\n\nhttps://arcticdata.io/submit/#guidelines-for-large-models\n\n\nSpatial data\n\nVector\n\nhttps://doi.org/10.18739/A2TB0XV89\n\nRaster\n\nhttps://doi.org/10.18739/A2GT5FG0B\n\n\nPortals\n\nhttps://arcticdata.io/catalog/portals/DBO\nhttps://arcticdata.io/catalog/portals/CALM"
  },
  {
    "objectID": "workflows/data_packages_arcticdatautils/reorder_entities.html",
    "href": "workflows/data_packages_arcticdatautils/reorder_entities.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This is easier to accomplish using arcticdatautils\nAn example reorganizing alphabetically\n\nMetadata\n\n\ndoc$dataset$otherEntity <- doc$dataset$otherEntity[order(entity_names)]\n\n\nData files\n\n\npkg <- get_package(adc, rm, file_names = T)\ndoc <- EML::read_eml(getObject(adc, pkg$metadata))\nreordered <- arcticdatautils::reorder_pids(pkg$data, doc)"
  },
  {
    "objectID": "workflows/data_packages_arcticdatautils/get_package_and_eml.html",
    "href": "workflows/data_packages_arcticdatautils/get_package_and_eml.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Before we look more in depth at EML, we first need to load your data package into R. After setting your node, use the following commands to load the package:\n\nrm_pid <- \"your_resource_map_pid\"\n\npkg <- get_package(adc_test,\n                   rm_pid,\n                   file_names = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\nKeeping track of PIDs\nIn all subsequent sections, if they ask for the pid of the metadata, reasource map or data you can refer to the object from get_package().\nFor example, if you assigned the value from get_package() as above as pkg then you can refer to the following by using the corresponding commands:\n\n\n\nmetadata_pid <-  pkg$metadata\ndata_pid <-  pkg$data\nresource_pid <-  pkg$resource_map\n\nAfter loading the package, you can also load the EML file into R using the following command:\n\ndoc <- read_eml(getObject(adc_test, pkg$metadata))\n\nNote that we named the object doc. This is a good generic name to use for EML documents. The generic name eml should not be used - as the EML package ships with an eml function, which can cause namespace issues in your environment if you have an object also called eml.\n\n\n\n\n\n\nNote\n\n\n\nTip to always have the most recent resource map.\nWhen editing data packages, you always want to be working with the most recent update. To ensure you have the most recent resource map, you can use the following commands:\nrm_pid_original <- \"your_original_resource_map_pid\"\nall_rm_versions <- get_all_versions(adc_test, rm_pid_original)\nrm_pid <- all_rm_versions[length(all_rm_versions)]\nprint(rm_pid)"
  },
  {
    "objectID": "workflows/data_packages_arcticdatautils/publish_an_object.html",
    "href": "workflows/data_packages_arcticdatautils/publish_an_object.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Objects (data files, xml metadata files) can be published to a DataONE node using the function publish_object() from the arcticdatautils R Package. To publish an object, you must first get the formatId of the object you want to publish. A few common formatIds are listed below.\n\n# .csv file\nformatId <- \"text/csv\"\n\n# .txt file\nformatId <- \"text/plain\"\n\n# metadata file\nformatId <- \"https://eml.ecoinformatics.org/eml-2.2.0\"\n# OR\nformatId <- format_eml(\"2.2.0\")\n\nMost objects have registered formatIds that can be found on the DataONE Object Format List here. Always use the “Id:” (2nd line) from the DataONE Object Format List and ensure it is copied EXACTLY. Metadata files (as shown above) use a special function to set the formatId. If the formatId is not listed at the DataONE website, you can set formatId <- \"application/octet-stream\".\n\n\n\n\n\n\nNote\n\n\n\nIf you want to change the formatId please use updateSystemMetadata instead.\n\n\nOnce you know the appropriate formatId you can publish an object using these commands:\n\npath <- \"path/to/your/file\"\nformatId <- \"your/formatId\"\n\npid <- publish_object(adc_test,\n                      path = path,\n                      format_id = formatId) # note that the output of this function is the PID of the newly published object\n\nAfter publishing the object, the PID will need to be added to a resource map by updating or creating a resource map. Additionally, the rights and access for the object must be set. However, you only need to give other people rights and access to objects to objects that are not yours, for the training you don’t need to do this."
  },
  {
    "objectID": "workflows/data_packages_arcticdatautils/create_a_resource_map.html",
    "href": "workflows/data_packages_arcticdatautils/create_a_resource_map.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "If you are creating a new data package, you must create a resource map. Resource maps provide information about the resources in the data package (i.e. which data files are associated with a particular metadata file (xml)).\nBefore creating a new resource map, it is best to ensure that all the objects of interest have already been published to the node. Therefore, if you’ve only run publish_object() on the xml (in section 2.4 above), you will also need to publish all the data objects you wish to associate with the xml. The resource map you are about to create controls these linkages.\nOnce all the objects that ought to belong to the package have been published, create a resource map using their respective PIDs (or the variables you saved the PIDs to) via the arcticdatautils::create_resource_map() function, like so:\n\nresource_map_pid <- create_resource_map(adc_test,\n                                        metadata_pid = metadata_pid,\n                                        data_pids = data_pids)"
  },
  {
    "objectID": "workflows/wrangle_data/clean_column_names.html",
    "href": "workflows/wrangle_data/clean_column_names.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "You might have read that column names should not include spaces or special characters. Inevitably, you’ll encounter data that are not so tidy. For example:\n\n\n  First Name Last Name date of birth\n1      Homer   Simpson      1/1/1960\n2      Marge   Simpson    10/20/1965\n3        Ned  Flanders     3/22/1961\n\n\nTo tidy it up, you can use the clean_names() function from the janitor package:\n\njanitor::clean_names(df)"
  },
  {
    "objectID": "workflows/wrangle_data/fix_excel_dates.html",
    "href": "workflows/wrangle_data/fix_excel_dates.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Do you see something that looks like 43134, 43135, 43136 even though the column header is Dates? You may have encountered an Excel date/time problem. To fix it, the janitor package has a handy function:\n\njanitor::excel_numeric_to_date(c(43134, 43135, 43136))\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you check that the dates make sense!"
  },
  {
    "objectID": "workflows/miscellaneous/code_snippets.html",
    "href": "workflows/miscellaneous/code_snippets.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Code snippets help with templating portions of code that you will be using regularly. To add your own, go to the toolbar ribbon at the top of your Rstudio screen and select:\nTools > Global Options... > Code > Edit Snippets > Add these chunks to the end of the file\nMore info can be found in this blog post by Mara Averick on how to add them: https://maraaverick.rbind.io/2017/09/custom-snippets-in-rstudio-faster-tweet-chunks-for-all/\nUsual arcticdatautils ticket workflow:\n\nsnippet ticket\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    rm <- \"add your rm\"\n    pkg <- get_package(adc, rm)\n    doc <- EML::read_eml(getObject(adc, pkg\\$metadata))\n    \n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    eml_validate(doc)\n    eml_path <- \"eml.xml\"   \n    write_eml(doc, eml_path)\n\n    #update <- publish_update(adc,\n    #                                               metadata_pid = pkg\\$metadata,\n    #                                               resource_map_pid = pkg\\$resource_map,\n    #                                               metadata_path = eml_path,\n    #                                               data_pids = pkg\\$data,\n    #                                               public = F)\n                                                    \n    #datamgmt::categorize_dataset(update\\$metadata, c(\"theme1\"), \"Your Name\")\n\n The datapack ticket workflow:\n\nsnippet datapack\n    library(dataone)\n    library(datapack)\n    library(digest)\n    library(uuid)\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    d1c <- D1Client(\"PROD\", \"urn:node:ARCTIC\")\n    packageId <- \"id here\"\n    dp <- getDataPackage(d1c, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\n    \n    #get metadata id\n    metadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n    \n    #edit the metadata\n    doc <- read_eml(getObject(d1c@mn, metadataId))\n    \n    #add the publisher info\n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    doc\\$dataset\\$project <- eml_nsf_to_project(\"nsf id here\")\n    \n    #check and save the metadata\n    eml_validate(doc)\n    eml_path <- arcticdatautils::title_to_file_name(doc\\$dataset\\$title)\n    write_eml(doc, eml_path)\n    \n    dp <- replaceMember(dp, metadataId, replacement=eml_path)\n    \n    #upload the dataset\n    myAccessRules <- data.frame(subject=\"CN=arctic-data-admins,DC=dataone,DC=org\", permission=\"changePermission\") \n    packageId <- uploadDataPackage(d1c, dp, public=F, accessRules=myAccessRules, quiet=FALSE)\n    #datamgmt::categorize_dataset(\"doi\", c(\"theme1\"), \"Jasmine\")\n\n Quick way to give access to submitters to their datasets:\n\nsnippet access\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n\n    rm <- \"rm here\"\n    pkg <- get_package(adc, rm)\n\n    set_access(adc, unlist(pkg), \"orcid here\")\n\n Quick access to the usual code for common Solr queries:\n\nsnippet solr\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    result <- query(adc, list(q = \"rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)\",\n                              fl = \"identifier,rightsHolder,formatId, fileName, dateUploaded\",\n                              sort = 'dateUploaded+desc',\n                              start =\"0\",\n                              rows = \"1500\"),\n                         as=\"data.frame\")"
  },
  {
    "objectID": "workflows/miscellaneous/metacat_version.html",
    "href": "workflows/miscellaneous/metacat_version.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Check which version of metacat is currently deployed to:\n\narcticdata.io here,\n\nand to test.arcticdata.io here."
  },
  {
    "objectID": "workflows/miscellaneous/file_paths.html",
    "href": "workflows/miscellaneous/file_paths.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This section contains some basic tips about file paths.\n\n\nIf you’re working on the datateam server (RStudio in an internet browser), you’re usually working in the home directory under your username. File paths generally follow this pattern: /home/dmullen/folder/file. Note: These files are not stored on your local machine, they are on a remote server. You can access them through remote SFTP software like Cyberduck.\nIf you’re working with your local RStudio app then your file paths generally follow this pattern: /Users/datateam/folder/file. These files are stored on your local machine and you can access them with Finder.\n\n\n\nUsing . in a file path refers to your current working directory. You can print your working directory with the call: getwd(). If we assume that my current working directory is home/dmullen, then the following two calls are equivalent:\n\nfile.exists('/home/dmullen/myFile.R')\nfile.exists('./myFile.R')\n\nUsing ~ in a file path does not always refer to your current working directory. This is a common misconception. It is actually set to the global R USER path. You can print this with the call: Sys.getenv('USER'). Let’s assume that I’m working on the arcticdatautils package which I open using a .RProj file. This will update my current working directory to: /home/dmullen/arcticdatautils. Now the following calls are equivalent:\n\nfile.exists('/home/dmullen/arcticdatautils/myFile.R')\nfile.exists('~/arcticdatautils/myFile.R')\nfile.exists('./myFile.R')"
  },
  {
    "objectID": "workflows/miscellaneous/resources_for_r.html",
    "href": "workflows/miscellaneous/resources_for_r.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The following online books are useful for expanding your R knowledge and skills:\n\nthe most recent ADC training materials\n\nThe cleaning and data manipulation section is useful for working with attribute tables\n\nEfficient R Programming\n\nIn particular Chapter 3 Efficient Programming\n\nR for Data Science\n\nSection on Strings\n\nR Packages\n\ncontributing to arcticdatatutils, datamgmt and EML\n\nAdvanced R\n\nObject-oriented programming in R for S4 to understand how datapack and dataone packages are written\n\nbookdown: Authoring Books and Technical Documents with R Markdown\nformatting, troubleshooting and updating the training document\n\nOthers\n\nHands-On Programming with R\nR Programming for Data Science\nExploratory Data Analysis with R\nMastering Software Development in R\nGeocomputation with R\nR Markdown: The Definitive Guide\nThe Tidyverse Style Guide\n\nThe RStudio cheatsheets are also useful references for functions in tidyverse and other packages.\n\n\n\nThe data team uses and develops a number of R packages. Here is a listing and description of the main packages:\n\ndataone\n\nreading and writing data at DataONE member nodes\nhttp://doi.org/10.5063/F1M61H5X\n\ndatapack\n\ncreating and managing data packages\nhttps://github.com/ropensci/datapack\n\nEML\n\ncreating and editing EML metadata documents\nhttps://ropensci.github.io/EML\n\narcticdatautils\n\nutility functions for processing data for the Arctic Data Center\nhttps://nceas.github.io/arcticdatautils/\n\ndatamgmt\n\ndata management utilities for curating, documenting, and publishing data (sandbox package)\nhttps://nceas.github.io/datamgmt/\n\nmetadig\n\nauthoring MetaDIG quality checks\nhttps://github.com/NCEAS/metadig-r\n\nmetajam\n\ndownloading and reading data and metadata from DataONE member nodes\nhttps://nceas.github.io/metajam/"
  },
  {
    "objectID": "workflows/nesting_data/nesting_a_data_package.html",
    "href": "workflows/nesting_data/nesting_a_data_package.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Data packages on member nodes can exist as independent packages or in groups (nested data packages). Much like we can group multiple data files together with a common metadata file, we can group related data packages together with a common “parent” data package.\nThe structure of nested data packages resembles a pyramid. There is one top level, or “parent”, with one or more data packages, or “child packages”, nested beneath it. There is no limit to how many nested levels can be created, but packages do not generally exceed 3 levels. This “grandparent” has 5 child packages (nested datasets), all of which have child packages of their own.\nHere are some common uses for nesting:\n\ncollected data vary by year\nan NSF award funds several related projects\ndata collection is still ongoing\ndata files exceed the 1000 file limit per data package"
  },
  {
    "objectID": "workflows/nesting_data/nesting_a_data_package.html#add-children-to-an-existing-parent",
    "href": "workflows/nesting_data/nesting_a_data_package.html#add-children-to-an-existing-parent",
    "title": "NCEAS Data Team Training",
    "section": "Add children to an existing parent",
    "text": "Add children to an existing parent\nA new package is published with a DOI and needs to be nested underneath a pre-existing parent. Nest the new child using the child_pids argument in publish_update().\n\nresource_map_child_new <- \"some_child_resource_map_pid\"\npkg_parent <- get_package(mn, 'resource_map_parent')\n\npublish_update(mn,\n               resource_map_pid = pkg_parent$resource_map,\n               metadata_pid = pkg_parent$metadata,\n               data_pids = pkg_parent$data_pids,  # parents usually don't contain data, but better to be safe\n               child_pids = c(pkg_parent$child_packages, \n                              resource_map_child_new))\n# include the resource map PIDs of ALL the childs* in the `child_pids` argument, otherwise the nesting relationships between any omitted childs and the parent will be deleted\n\n\n\n\n\n\n\nWarning\n\n\n\nCheck through all arguments carefully before you publish to production! Do you need to update the metadata? Does the parent include data objects? Does the parent have a parent?\nParents can be tricky to fix and work with (especially if they have serial identifiers (SIDs)), so if you’re not sure how something works, try it on a test node."
  },
  {
    "objectID": "workflows/nesting_data/nesting_a_data_package.html#create-a-new-parent-package",
    "href": "workflows/nesting_data/nesting_a_data_package.html#create-a-new-parent-package",
    "title": "NCEAS Data Team Training",
    "section": "Create a new parent package",
    "text": "Create a new parent package\nIn some cases, a parent package already exists. Search the ADC for the NSF award number to see if there are already exisiting packages. Parents usually have a UUID rather than a DOI and often start with a title like “Collaborative research:”, but not always. More typically, you will need to create a new parent by editing the existing metadata. The parent package should contain a generalized summary for the metadata of each of its childs.\nTo create a new parent, you will need to:\n\nCreate parent metadata. It’s often easiest to start with a child’s metadata and generalize them.\n\nAbstract/title: Remove dates and other details that are specific to the child package. Sometimes the NSF award abstract/ title will work.\nData tables/other entities: Generally, top-level parents do not include data objects, so these sections can be removed.\nGeographic coverage: Expand to include geographic coverage of all childs, if needed.\nTemporal coverage: Expand to include temporal ranges of all childs, if needed. If the study is ongoing, include the most recent end date; the parent can be updated when additional childs are added.\nMethods: Often not needed, but may be included if all childs use the same methods.\n\nPublish the parent metadata to the member node (ADC) using publish_object().\nCreate a resource map to link the parent and childs together using create_resource_map() and the child_pids argument.\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you use the childs’ resource map PIDs when you create the resource map! If you forgot to do so, consult Jeanette for help fixing it."
  },
  {
    "objectID": "workflows/nesting_data/nesting_a_data_package.html#example",
    "href": "workflows/nesting_data/nesting_a_data_package.html#example",
    "title": "NCEAS Data Team Training",
    "section": "Example",
    "text": "Example\nWe can start by creating two data packages on the test node to nest beneath a parent. These data packages contain measurements taken from Lake E1 in Alaska in 2013 and 2014.\nFirst, load the Arctic Data Center Test Node and libraries.\n\nlibrary(dataone)\nlibrary(arcticdatautils)\nlibrary(EML)\n\ncn_staging <- CNode('STAGING')\nadc_test <- getMNode(cn_staging,'urn:node:mnTestARCTIC')\n\ncn <- CNode('PROD')\nadc <- getMNode(cn, 'urn:node:ARCTIC')\n\nWe will re-create the following parent package: https://arcticdata.io/catalog/#view/urn:uuid:799b7a86-cb1c-497c-a05a-d73492915cad on the test node with two of its children. First we will copy two of the children to the test node, make sure your token for the test node is not expired.\n\nfrom <- dataone::D1Client(\"PROD\", \"urn:node:ARCTIC\")\nto <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nchild_pkg_1 <- datamgmt::clone_package('resource_map_doi:10.18739/A2KS1R',\n                                       from = from, to = to,\n                                       add_access_to = arcticdatautils:::get_token_subject(),\n                                       change_auth_node = TRUE, new_pid = TRUE)\n\nchild_pkg_2 <- datamgmt::clone_package('resource_map_doi:10.18739/A2QK29',\n                                       from = from, to = to,\n                                       add_access_to = arcticdatautils:::get_token_subject(),\n                                       change_auth_node = TRUE, new_pid = TRUE)\n\nThese two packages correspond to data from the same study, varying only by year; however, they currently exist on the test node as independent entities. We will associate them with each other by nesting them underneath a parent.\nNow, let’s create a parent metadata file. Read in one of the childs’ metadata files (EML). We can download object from a node in binary format using dataone::getObject(). Once it’s downloaded we just need to convert to it to the proper format: in this case to EML format using EML::read_eml().\n\ndoc_parent <- read_eml(getObject(adc_test, child_pkg_1$metadata))\n\n\n## View the title \ndoc_parent$dataset$title\n\nThe title of this child contains “2012-2013”. This is too specific for the parent, as the temporal range of both childs is 2012-2014. The parent should encompass this larger time range.\n\ndoc_parent$dataset$title <- 'Time series of water temperature, specific conductance, and oxygen from Lake E1, North Slope, Alaska, 2012-2014'\n\nLike the title, the temporal coverage elements in this EML need to be adjusted.\n\nnew_end_date <- \"2014-09-20\"\ndoc_parent$dataset$coverage$temporalCoverage$rangeOfDates$endDate$calendarDate <- new_end_date\n\nRemove dataTables and otherEntitys from the metadata. If you recall from previous chapters, dataTables contain metadata associated with data files (generally CSVs) and otherEntitys contain metadata about any other files in the data package (for instance a README or coding script). Because the parent does not contain any data objects, we want to remove dataTables and otherEntitys from the metdata file. In this instance, the E1 2013 metadata only contain dataTables. We can remove these by setting the dataTable element in the EML to a new blank object.\n\ndoc_parent$dataset$dataTable <- NULL\n\nIn this case, the abstract, contacts, creators, geographicDescription, and methods are already generalized and do not require changes.\nBefore writing your parent EML make sure that it validates. This is just a check to make sure everything is in the correct format.\n\neml_validate(doc_parent)\n\nAfter your EML validates we need to save, or “write”, it as a new file. Write your parent EML to a directory in your home folder. You can view this process like using “Save as” in Microsoft Word. We opened a file (“E1_2013.xml”), made some changes, and “saved it as” a new file called “doc_parent.xml”.\n\n# We can save the eml in a temporary file \neml_path <- file.path(tempdir(), 'science_metadata.xml')\nwrite_eml(doc_parent, path)\n\nNext, we will publish the parent metadata to the test node.\n\nmetadata_parent <- publish_object(adc_test, \n                                  path = eml_path, \n                                  format_id = format_eml())\n\nFinally, we create a resource map for the parent package. We nest the two child data packages using the child_pids argument in create_resource_map(). Note that these child_pids are PIDs for the resource maps of the child packages, NOT the metadata PIDs.\n\nresource_map_parent <- create_resource_map(adc_test, \n                                           metadata_pid = metadata_parent,\n                                           child_pids = c(child_pkg_1$resource_map,\n                                                          child_pkg_2$resource_map))\n\nThe child packages are now nested underneath the parent."
  },
  {
    "objectID": "workflows/data_portals/mosaic.html",
    "href": "workflows/data_portals/mosaic.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Look out for datasets that are part of the MOSAiC expedition from 2019 -2020. There are a couple of special steps for datasets from this project.\nWhen working on MOSAiC datasets:\n\nFind out the EventId from the researcher to annotate (Campaign can be derived from the eventLabel). Ask for it in your email:\n\n\nWe would like to ask for the event label associated with this dataset (see https://www.pangaea.de/expeditions/events/PS122%2F4).\n\n\nFind the appropriate dataset and attribute level annotations\n\n\nThere are functions in arcticdatautils to help with annotating: mosaic_annotate_dataset and mosaic_annotate_attribute\nThe custom ontology for the datasets are currently in the sem-prov-ontologies repository but you won’t need to use it directly if the function works.\nUse EML::shiny_attributes only when no annotations have been added. The function does not work well with multiple annotations\nPlease continue to annotate the measurement types using containst measurements of type for all other attributes\n\n\nThe portal is on DataONE. Make sure the dataset(s) show up in the portal after you add the annotations.\n\nHere are two fully annotated datasets for reference:\n\nSimple example\nExample with MOSAiC and ECSO annotations\nExample with non-polarstern vessels\n\nThe following shows how to add the annotations using arcticdatautils and manually in the case the functions fail to work:\n\n\nThere are 5 main campaigns in the MOSAiC expedition. The main campaigns follow the pattern PS122/#. For the full campaign list it is easiest to see on the PANGAEA website\narcticdatautils\n\ndoc$dataset$id <- \"your id here\"\ndoc$dataset$annotation <- NULL #make sure it is empty\n\ndoc$dataset$annotation <- mosaic_annotate_dataset(c(\"PS122/1\", \"PS122/2\"))\n\nManual\nMajority of the annotations will be the same across all of the datasets. Pay attention to what needs to be changed for the hasBasis and isPartOfCampaign valueURIs.\nMore than one Campaign and Basis might be needed in rare cases.\n\ndoc$dataset$id <- \"your id here\"\ndoc$dataset$annotation <- NULL #make sure it is empty here\n\ndoc$dataset$annotation <- list(\n  #Basis\n  list(\n    propertyURI = list(label = \"hasBasis\",\n                       propertyURI = \"https://purl.dataone.org/odo/MOSAIC_00000034\"),\n    valueURI = list(label = \"Polarstern\", # this depends on your event ID, most of them should be Polarstern\n                    valueURI = \"https://purl.dataone.org/odo/MOSAIC_00000030\")\n  ),\n  #Project\n  list(\n    propertyURI = list(label = \"hasProjectLabel\",\n                       propertyURI = \"https://purl.dataone.org/odo/MOSAIC_00000025\"),\n    valueURI = list(label = \"MOSAiC20192020\",\n                    valueURI = \"https://purl.dataone.org/odo/MOSAIC_00000023\")\n  ),\n  #Campaign\n  list(\n    propertyURI = list(label = \"isPartOfCampaign\",\n                       propertyURI = \"https://purl.dataone.org/odo/MOSAIC_00000032\"),\n    valueURI = list(label = \"PS122/2\", #*** this changes depending on the campaign\n                    valueURI = \"https://purl.dataone.org/odo/MOSAIC_00000018\")\n  ) #*** this changes depending on the campaign\n)\n\n\n\nApply these to the attribute level of the dataset. Event IDs should have already be identified earlier on in the dataset review. Check the ticket overview for details.\narcticdatautils\n\ndoc$dataset$dataTable[[1]]$attributeList$attribute[[1]]$id <- \"some id here\"\ndoc$dataset$dataTable[[1]]$attributeList$attribute[[1]]$annotation <- mosaic_annotate_attribute(\"PS122/2_14-270\")\n\nManual\n\nevent_annotation <- list(#Event Label\n  list(\n    propertyURI = list(label = \"wasGeneratedBy\",\n                       propertyURI = \"http://www.w3.org/ns/prov#wasGeneratedBy\"),\n    valueURI = list(label = \"PS122/2_14-270\",\n                    valueURI = \"https://purl.dataone.org/odo/MOSAIC_00004550\")\n  ),\n  #Method/Device - Use the long name!\n  list(\n    propertyURI = list(label = \"deployedSystem\",\n                       propertyURI = \"https://purl.dataone.org/odo/MOSAIC_00002201\"),\n    valueURI = list(label = \"Ultra-Wideband Software-defined Microwave Radiometer (0.5-2GHZ)\",\n                    valueURI = \"https://purl.dataone.org/odo/MOSAIC_00001163\")\n  ))\n\ndoc$dataset$dataTable$attributeList$attribute[[1]]$id <- \"eventId\"\ndoc$dataset$dataTable$attributeList$attribute[[1]]$annotation <-\n  event_annotate\n\nAdding annotations\nIf there are already annotations applied (ie. using ECSO), the MOSAiC annotations can be added as so:\n\nget the index of all of the attributes we want to add this annotation\n\n\nn <- which(stringr::str_detect(eml_get_simple(doc$dataset$dataTable$attributeList, \"attributeName\"),\"Brightness\"))\n\n\nloop through and add the annotation(s)\n\n\nfor(i in n) {\n  doc$dataset$dataTable$attributeList$attribute[[i]]$annotation <-\n    append(\n      list(\n        doc$dataset$dataTable$attributeList$attribute[[i]]$annotation\n      ),\n      event_annotation\n    )\n}\n\n\n\n\n\nThe filters are based on the MOSAiC ontology. The Campaign and Basis filters should remain relatively the same. However there are over 500 Method and Devices. So the decision was made to only show the ones we have annotated.\nWe can query for all of them and output them formatted as a choice:\n\nmosaic_portal_filter(\"Method/Device\")\n\nThen update the portal document using the steps outlines in the advanced data portal customizations"
  },
  {
    "objectID": "workflows/data_portals/data_portals.html",
    "href": "workflows/data_portals/data_portals.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Create a custom, branded portal for your research topic or lab group that spans datasets in the Arctic Data Center – available for all users. Branded portals provide a convenient, readily customized way to communicate your science, your team, your data, and related data from within the Arctic Data Center. The portals feature makes highlighting and sharing related datasets easy for researchers.\n\nGeneral information on data portals can be found on the main page: https://arcticdata.io/data-portals/\n\n\nMy portals allows members of the Arctic Data Center Team to view all the portals currently created. This can be found in Settings under the My Portals section.\n\n\n\nUseful search terms to use:\n\nNSF award number\nGeographic Region\nKeywords\nAccess permissions\n\n### Set Access Permissions For some portals such as PerCS portal, will need to be added by running set_access(). Please see the specific portal for the exact arguments to pass.\n\n\n\nThese requires the manual update of the portal document for features that are not available on the website yet. Please ask Jeanette for more assistance. For an example of a portal with all the features, see the SASAP Portal.\n\n\n\nMost os the time copy and pasting desired from the SASAP portal is a good place to start\nTips - try to collapse the specific section and copy that to make sure you copy everything is needed\n\nFilter Groups\nGrouping multiple filters under one tab\n\nPossible fontawesome icons\n\n<filterGroup>\n  <label>MOSAiC Terms</label>\n  <description>Terms in the MOSAiC ontology</description>\n  <icon>book</icon>\n  <choiceFilter>...</choiceFilter>\n</filterGroup>\nIndividual Filters\n<choiceFilter>\n  <label>Campaign</label>\n  <!-- ====== equivalent to the fields available in the solr query ====== -->\n  <field>text</field> \n  <operator>OR</operator>\n  <filterOptions>\n      <placeholder>Choose a campaign</placeholder><icon>sitemap</icon>\n      <description>The campaign number</description>\n  </filterOptions>\n  <!-- ====== change these for different options ====== -->\n      <choice><label>AF-MOSAiC-1</label><value>https://purl.dataone.org/odo/MOSAIC_00000020</value></choice> \n      <choice><label>AT-MOSAiC-1</label><value>https://purl.dataone.org/odo/MOSAIC_00011201</value></choice>\n      <chooseMultiple>true</chooseMultiple>\n</choiceFilter>\n\n\n\nLook for this section in the xml and replace the hex codes:\n <option>\n        <optionName>primaryColor</optionName>\n        <optionValue>#3774b9</optionValue>\n    </option>\n    <option>\n        <optionName>secondaryColor</optionName>\n        <optionValue>#82cdf5</optionValue>\n    </option>\n    <option>\n        <optionName>accentColor</optionName>\n        <optionValue>#EB5638</optionValue>\n    </option>\n\n\n\n\nalso a formatted page - see the SASAP portal for details\n\n\n\n\n\nFor portals on DataOne - use the ucsb node. Portals made on the ADC use the same node as the datasets.\n\nlibrary(dataone)\nlibrary(EML)\n\ncn <- CNode(\"PROD\")\nucsb <- getMNode(cn, \"urn:node:mnUCSB1\")\n\nLook for portals using query()so you can get the appropriate seriesID\n\ndf <- query(ucsb, list(q =\"formatId:*purl.dataone.org/portals-1.0.0 AND (*:* NOT obsoletedBy:*)\",\n                       rows = \"300\",\n                       fl = \"identifier,dateUploaded,formatId,obsoletedBy,origin,formatType,title, seriesId\",\n                       sort = 'dateUploaded+desc'),\n            as = \"data.frame\")\n\nDownload the file and open to edit and add the filters and other options.\n\nid <- df[df$title == \"Multidisciplinary drifting Observatory for the Study of Arctic Climate expedition\", 2] #series ID\nwriteBin(getObject(ucsb, id), \"file_mosaic.xml\") #download\n\nMake whatever edits you need and upload the file. The identifier should be updated if successful but if there is an issue with the xml document go back and fix the issue before re-uploading again.\n\nsys <- getSystemMetadata(ucsb,  id)\narcticdatautils::update_object(ucsb, sys@identifier, path = \"file_mosaic.xml\", format_id = sys@formatId, sid = sys@seriesId)"
  },
  {
    "objectID": "workflows/data_portals/dbo_packages.html",
    "href": "workflows/data_portals/dbo_packages.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "DBO packages are organized under a project page (viewable here). In order to make sure that DBO datasets appear in the special DBO data search correctly, there are a few additional metadata requirements.\n\ngive read and write access to the entire data package (using set_access) to the group: \"CN=DBO,DC=dataone,DC=org\"\n\neg: `set_access(mn, pkg, “CN=DBO,DC=dataone,DC=org”, permissions = c(“read”, “write”, “changePermission”))\n\nensure that the name of the ship the data was collected from appears in the metadata record. This will likely be one of the following ship names. If the ship name is not in that list, let Jeanette or Dominic know\n\nAnnika Marie\nFairweather\nHealy\nKromov\nMirai\nOscar Dyson\nOshoro-Maru\nSir Wilfrid Laurier\nWestward Wind\nXue Long\n\nthe geographic coverage should be one coverage per DBO transect sampled. The geographicDescription and bounding coordinates should be from the controlled vocabulary given in the code snippet below, which will create a data.frame with these values in your R environment\n\n\ngeo_locs <- dplyr::tribble(~siteNew, ~westBoundingCoordinate, ~eastBoundingCoordinate, ~northBoundingCoordinate,~ southBoundingCoordinate,\n\"DBO 1 - South of St. Lawrence Island, Bering Sea\", -176.147,   -172.187,   63.769, 61.847,\n\"DBO 2 - Chirikov Basin, Northern Bering Sea\",  -170.492,   -167.86,    65.111, 64.482,\n\"DBO 3 - Southern Chukchi Sea\", -171.419,   -166.481,   68.609, 66.752,\n\"DBO 4 - Northeast Chukchi Sea\",    -164.553,   -160.507,   71.867, 70.682,\n\"DBO 5 - Barrow Canyon, Chukchi Sea\",   -158.848,   -155.931,   71.808, 71.111,\n\"DBO 6 - Beaufort Sea - 152 W\", -153.865,   -150.976,   72.119, 70.89)\n\n\none of the following key phrases referencing the data type should exist in the dataset. If the dataset doesn’t seem to fit in one of these categories, ask Jeanette or Dominic\n\nADCP\nbenthic macroinfaunal\nchlorophyll\nCTD\nCPUE (referencing fish catch per unit effort)\nmarine mammal\nnutrient\nsediment\nzooplankton\n\n\nAfter you have updated your dataset, navigate to https://arcticdata.io/catalog/projects/DBO/data and make sure that the different filters work as expected. Your dataset should be discoverable in some way through all of the filters."
  },
  {
    "objectID": "workflows/adc_web_submissions/recover_failed_submission.html",
    "href": "workflows/adc_web_submissions/recover_failed_submission.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "A submission may result in an error when the EML cannot be validated through EML::eml_validate, preventing it from being written to an indicated path. This usually occurs when the metacatUI detects a submission error and uploads the EML as a data object instead of text. When this happens, arcticdatautils::recover_failed_submission can be used to retrieve the original EML file. The function removes the error message to get a valid EML document.\nFor the example below, pid is the metadata pid containing the failed submission. The path parameter indicates where you want to write the recovered EML version to on your server’s local working directory.\n\npath <- \"path/to/save/eml.xml\"\n\npid <- \"your_metadata_pid\"\n\narcticdatautils::recover_failed_submission(adc, pid, path)\n\nThe output of this function is a valid EML file written to your chosen path. After recovering the document, make sure to set the rights and access to the correct submitter.\nNote that arcticdatautils::recover_failed_submission may not always work. Its functionality depends on the error message since they can vary."
  },
  {
    "objectID": "workflows/adc_web_submissions/add_physicals_to_submissions.html",
    "href": "workflows/adc_web_submissions/add_physicals_to_submissions.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "New submissions made through the web editor will not have any physical sections within the otherEntitys. Add them to the EML with the following script:\n\nfor (i in seq_along(doc$dataset$otherEntity)) {\n    otherEntity <- doc$dataset$otherEntity[[i]]\n    id <- otherEntity$id\n    \n    if (!grepl(\"urn-uuid-\", id)) {\n        warning(\"otherEntity \", i, \" is not a pid\")\n        \n    } else {\n        id <- gsub(\"urn-uuid-\", \"urn:uuid:\", id)\n        physical <- arcticdatautils::pid_to_eml_physical(mn, id)\n        doc$dataset$otherEntity[[i]]$physical <- physical\n    }\n}\n\nAs you can see from code above, we use a for loop here to add physical sections. The for loop is a very useful tool to iterate over a list of elements. With for loop, you can repeat a specific block of code without copying and pasting the code over and over again. When processing datasets in Arctic Data Center, there are many places where for loop can be used, such as publishing a bunch of objects with pids, updating formatID for pkg$data, adding physical section like above code, etc.\nA loop is composed of two parts: the sequence and the body. The sequence usually generates indices to locate elements and the body contains the code that you want to iterate for each element.\nHere is an example of adding the same attributeList for all the dataTables in the metadata using for loop.\n\nattributes <- read.csv('attributes.csv')  # attribute table in csv format\nattributeList <- EML::set_attributes(attributes = attributes)\n\nfor (i in 1:length(doc$dataset$dataTable)) { # sequence part\n    doc$dataset$dataTable[[i]]$attributeList <- attributeList # body part\n}"
  },
  {
    "objectID": "workflows/adc_web_submissions/list_submissions_to_adc.html",
    "href": "workflows/adc_web_submissions/list_submissions_to_adc.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "If the arctic bot fails to pick up a new submission, due to an invalid token or another error, we can check for submissions using a solr query. The list_submissions() function in the arcticdatautils package is a convenience wrapper around the solr query. If the function doesn’t specifically fit your needs it’s fairly straight-forward to modify the source code. Note: the function can be applied to any node, although it filters out results if the submitter is in the Arctic Data Center Admins group. Refer to the function documentation for more information.\n\nView submissions to the Arctic Data Center from 10/1/18 to 10/7/18\n\n\ncn <- dataone::CNode('PROD')\nadc <- dataone::getMNode(cn,'urn:node:ARCTIC')\nView(arcticdatautils::list_submissions(adc, '2018-10-01', '2018-10-07'))\n\n\nThe following code shows the underlying solr query\n\n\n  q = paste0('dateUploaded:[\"2018-10-01T00:00:00Z\" TO \"2018-10-08T00:00:00Z\"] AND formatType:*')\n  results <- dataone::query(adc, list(q = q,\n                                     fl = \"identifier AND submitter AND dateUploaded AND formatType AND fileName\",\n                                     rows = 10000),\n                            as = \"data.frame\")"
  },
  {
    "objectID": "workflows/adc_web_submissions/assess_attributes.html",
    "href": "workflows/adc_web_submissions/assess_attributes.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "New submissions made through the web editor should have attributes created by the submitter. If there are no attributes, or if they are incomplete, please use the email template to correspond with the submitter to ask for attributes.\nAdditionally, the web editor will not allow for the creation of custom units. submitters should select other/none in the editor unit dropdown if they cannot find their desired unit. In the EML document, these will result in dimensionless units. Accordingly, new submissions should be checked for dimensionless units. This can be done with the following code. Note that this only applies if there is more than one otherEntity.\n\ndim_units <- sapply(doc$dataset$otherEntity, function(x) {\n      i <- datamgmt::which_in_eml(x$attributeList$attribute, \"standardUnit\", as(\"dimensionless\", \"standardUnit\"))\n      \n      out <- sapply(i, function(j){\n         list(entityName = x$entityName,\n               attributeName = x$attributeList$attribute[[j]]$attributeName,\n               attributeLabel = x$attributeList$attribute[[j]]$attributeLabel,\n               attributeDefinition = x$attributeList$attribute[[j]]$attributeDefinition)\n      })\n      \n  return(out)  \n})\ndim_units <- data.frame(t(do.call(cbind, dim_units)))\n\ndim_units\n\nIf dimensionless units are found, first check the attributeDefinition, attributeLabel, and attributeName to see if dimensionless seems to be an appropriate unit for the attribute. If there is any question about whether or not dimensionless seems appropriate, ask another member of the data team. If there is still any question about what the unit should be, reach out to the submitter and ask for clarification."
  },
  {
    "objectID": "workflows/edit_data_packages/01_datapack_background.html",
    "href": "workflows/edit_data_packages/01_datapack_background.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "adapted from the dataone and datapack vingettes\ndatapack is written differently than most R packages you may have encountered in the past. This is because it uses the S4 system instead.\n\nlibrary(dataone)\nlibrary(datapack)\nlibrary(uuid)\n\nData packages\nData packages are a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata.\n\n\nNodes\nUsing this example on arcticdata.io\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nTo use the member node information, use the mn slot\n\nd1c_test@mn\n\n\n\n\n\n\n\nNote\n\n\n\nTo access the various slots using objects created by datapack and dataone (e.g. getSystemMetadata) requires the @ which is different from what you might have seen in the past. This is because these use the S4 system.\n\n\nGet an existing package from the Arctic Data Center. Make sure you know as you go through this training whether you are reading or writing to test or production. We don’t want to upload any of your test datasets to production!\n\nd1c <- dataone::D1Client(\"PROD\", \"urn:node:ARCTIC\")\ndp <- dataone::getDataPackage(d1c, \"resource_map_urn:uuid:1f9eee7e-2d03-43c4-ad7f-f300e013ab28\")\n\n\n\n\nCheck out the objects slot\n\ndp@objects\n\nGet the number for data and metadata files associated with this data package:\n\ngetSize(dp)\n\nGet the file names and corresponding pids\n\ngetValue(dp, name=\"sysmeta@fileName\")\n\nGet identifiers\nYou can search by any of the sysmeta slots such as fileName and formatId and get the corresponding identifier(s):\n\nmetadataId <- selectMember(dp, name=\"sysmeta@ADD THE NAME OF THE SLOT\", \n                           value=\"PATTERN TO SEARCH BY\")\n\nExample:\n\nselectMember(dp, name=\"sysmeta@formatId\", value=\"image/tiff\")\nselectMember(dp, name=\"sysmeta@fileName\", value=\"filename.csv\")\n\n\n\n\nView the provenance as a dataTable. We will get into detail in the Building provenance chapter.\n\ndp@relations$relations"
  },
  {
    "objectID": "workflows/edit_data_packages/add_a_pre-generated_identifier.html",
    "href": "workflows/edit_data_packages/add_a_pre-generated_identifier.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "When you pre-generate a UUID or DOI, the change is not automatically reflected in the packageId section of the EML. Use the code below to ensure that the EML lines up with the desired identifier:\n\n## Generate DOI and add to EML\n# Note that you cannot generate a DOI on test nodes\ndoiPid <- generateIdentifier(mn, \"DOI\")\ndoc$packageId <- doiPid\n\nBe sure to include the identifier= argument in your publish update command so the pre-generated identifier is applied."
  },
  {
    "objectID": "workflows/edit_data_packages/show_indexing_status.html",
    "href": "workflows/edit_data_packages/show_indexing_status.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Sometimes it takes awhile for the website to render with the updates you’ve made in R. To check whether a PID has been indexed yet, use:\n\nshow_indexing_status(mn, pid)\n\n\n\n\n\n\n\nNote\n\n\n\nThe status bar will either show 0% (not indexed) or 100% (should be online already)."
  },
  {
    "objectID": "workflows/edit_data_packages/03_update_package_datapack.html",
    "href": "workflows/edit_data_packages/03_update_package_datapack.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Once you have updated the data objects and saved the metadata to a file, we can update the metadata and add the new pid to the resource map using datapack::updateDataObject().\n\n\n\n\n\n\nNote\n\n\n\nThis part of the training assumes that you have an EML file created from earlier parts of the training\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou must use these R packages for data packages that have provenance or folder hierarchy (arcticdatatutils does not support those features)\n\n\n\n\nSetting the node is done slightly differently:\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nGet a pre-existing package:\n\npackageId <- \"the resource map\"\n\ndp <- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\n\n\n\n\nGet the metadata identifier:\n\nmetadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n\n\n\n\n\n\n\nNote\n\n\n\nTake note of the EML version. If it is EML 2.1.1 the value needs to be changed to -\"eml://ecoinformatics.org/eml-2.1.1\"\n\n\nRead the EML doc:\n\ndoc <- read_eml(getObject(d1c@mn, metadataId))\n\nEdit the EML as usual - see the documents in Edit EML for details\nOnce you are happy with your changes, you can update your data package to include the new metadata file using replaceMember:\n\neml_path <- \"path/to/your/saved/eml.xml\"\nwrite_eml(doc, eml_path)\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path)\n\n\n\n\nremove zip files:\n\nzipId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"application/vnd.shp+zip\")\nremoveMember(dp, zipId, removeRelationships = T)\n\nadd an existing data object:\n\ndataObj <- getDataObject(d1c_test, id=\"urn:uuid: here\", lazyLoad=T, limit=\"1TB\")\ndp <- addMember(dp, dataObj, mo=metadataId)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to change the formatId please use updateSystemMetadata\n\n\n\n\n\n\n\nIf you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), (a) you need to change the public argument to TRUE and generate a DOI identifier.\nThis should only be done after the package is finalized and has been thoroughly reviewed!\n\nUpdating a package with a new DOI:\n\nA Digital Object Identifier (DOI) may be assigned to the metadata DataObject, using the generateIdentifier:\n\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\n\nUpdating a package with a pre-issued DOI:\n\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=\"your pre-issued doi previously generated\")\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\nRefresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest."
  },
  {
    "objectID": "workflows/edit_data_packages/update_a_package.html",
    "href": "workflows/edit_data_packages/update_a_package.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Once you have updated the data objects and saved the metadata to a file, we can update the metadata and use replaceMember to update the package with the new metadata.\nMake sure you have the package you want to update loaded into R using dataone::getDataPackage().\n\n\nNow we can update your data package to include the new data object. Assuming you have updated your data package earlier something like the below:\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\npackageId <- \"the resource map\"\n\ndp <- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\nmetadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n\n#some modification to the EML here\n\neml_path <- \"path/to/your/saved/eml.xml\"\nwrite_eml(doc, eml_path)\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path)\n\nYou can then upload your data package:\n\nmyAccessRules <- data.frame(subject=\"CN=arctic-data-admins,DC=dataone,DC=org\", permission=\"changePermission\") \npackageId <- uploadDataPackage(d1c_test, dp, public=FALSE, accessRules=myAccessRules, quiet=FALSE)\n\nIf a package is ready to be public, you can change the public argument in the datapack::uploadDataPackage() call to TRUE.\nIf you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you need to do this when replacing the metadata. This should only be done after the package is finalized and has been thoroughly reviewed!\n\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\nIf there is a pre-issued DOI (researcher requested the DOI for the publication first), please do the following:\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=\"your pre-issued doi previously generated\")\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\nIf the package has children, see how to do this using arcticdatautils::publish_update in the nesting section of the reference manual.\nRefresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest."
  },
  {
    "objectID": "workflows/edit_data_packages/set_rights_and_access.html",
    "href": "workflows/edit_data_packages/set_rights_and_access.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access() will set both, and arcticdatautils::set_access() will just set access. There are two functions for this because a rightsHolder should always have access, but not all people who need access are rightsHolders. The rightsHolder of the data package is typically the submitter (if the data set is submitted through the web form (“editor”)), but if a data team member is publishing objects for a PI, the rightsHolder should be the main point of contact for the data set (i.e. the person who requested that we upload the data for them).\nTo set the rights and access for all of the objects in a package, first get the ORCiD of the person to whom you are giving rights and access. You can set this manually, or grab it from one of the creators in an EML file. You can look up ORCID iDs here\n\n# Manually set ORCiD\n subject <- 'http://orcid.org/PUT-YOUR-ORCD-HERE'\n\n# Set ORCiD from EML creator\n# if only 1 creator exists\n subject <- doc$dataset$creator$userId$userId\n # if more than 1 creator exists and you want the first one\n subject <- doc$dataset$creator[[1]]$userId$userId\n \n # As a convention we use `http:` instead of `https:` in our system metadata\n subject <- sub(\"^https://\", \"http://\", subject)\n\nNote, when setting metadata, the ORCiD must start with http://. ORCiDs in EML should start with https://. The sub() command above will change this formatting for you.\nNext, set the rights and access using the following command:\n\nset_rights_and_access(mn, \n                      pids = c(pkg$metadata, pkg$data, pkg$resource_map),\n                      subject = subject,\n                      permissions = c('read','write','changePermission'))\n\nIf you ever need to remove/add public access to your package or object, you can use remove_public_read() or set_public_read(), respectively.\n\nremove_public_read(mn, c(pkg$metadata, pkg$data, pkg$resource_map))\n\n\n\nThe datasets that render under a user’s profile page like here are added if one of the following three System Metadata fields exists. The subject is the rightsHolder or the subject has one of either write or changePermission in the accessPolicy.\nIf you ever need to remove a subject from the accessPolicy or update the rightsHolder you can use arcticdatautils::remove_access and arcticdatautils::set_rightsHolder, respectively."
  },
  {
    "objectID": "workflows/edit_data_packages/set_dataone_nodes.html",
    "href": "workflows/edit_data_packages/set_dataone_nodes.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "DataONE is a network of data repositories that is structured with coordinating nodes (CN) and member nodes (MN). The network tree looks something like this:\n\nAt the top level is DataONE itself. Within DataONE there are several coordinating nodes, including nodes for both production material and testing material. Within these coordinating nodes are many member nodes, including ones for both the Arctic Data Center and the KNB. To set the environment in which you want to publish data, you need to set both the coordinating node and the member node. For example, if you are publishing to the Arctic Data Center test site, you would want to set the coordinating node to STAGING and the member node to mnTestArctic.\n\n\n\n\n\n\nWarning\n\n\n\nA note on nodes - be very careful about what you publish on production nodes (PROD, or arcticdata.io). These nodes should NEVER be used to publish test or training data sets.\n\n\nThe primary nodes we work on, and how to set them in R, are below:\n\n\n\n# ADC (test.arcticdata.io)\ncn_staging <- CNode('STAGING')\nadc_test <- getMNode(cn_staging,'urn:node:mnTestARCTIC')\n\n# KNB (dev.nceas.ucsb.edu)\ncn_staging2 <- CNode(\"STAGING2\")\nknb_test <- getMNode(cn_staging2, \"urn:node:mnTestKNB\")\n\n\n\n\n\n# ADC (arcticdata.io)\ncn <- CNode('PROD')\nadc <- getMNode(cn,'urn:node:ARCTIC')\n\n# KNB (knb.ecoinformatics.org)\nknb <- getMNode(cn, \"urn:node:KNB\")\n\n# GOA \ngoa <- getMNode(cn, \"urn:node:GOA\")\n\n# You can also use the datamgmt::guess_membernode function to set a member node\n# Note: this pid looks like a URL - it's really a unique identifier\ndryad <- datamgmt::guess_member_node('https://doi.org/10.5061/dryad.k6gf1tf/15?ver=2018-09-18T03:54:10.492+00:00')\n\nMore DataONE STAGING nodes can be found here More DataONE PROD nodes can be found here"
  },
  {
    "objectID": "workflows/edit_data_packages/edit_sysmeta.html",
    "href": "workflows/edit_data_packages/edit_sysmeta.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "To edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command:\n\nsysmeta <- getSystemMetadata(d1c@mn, pid)\n\nThen edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command:\n\nsysmeta@fileName <- 'NewFileName.csv'\n\nNote that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule() (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access(), which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata() to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats.\n\n# Example of setting the formatId slot\nsysmeta@formatId <- \"eml://ecoinformatics.org/eml-2.1.1\"\n\nAfter you have changed the necessary slot, you can update the system metadata using the following command:\n\nupdateSystemMetadata(d1c@mn, pid, sysmeta)\n\n\n\nImportantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published).\n\n\n\nFor a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation:\n\nSystem Metadata\nData Types in CICore"
  },
  {
    "objectID": "workflows/edit_data_packages/obsolescence_chain.html",
    "href": "workflows/edit_data_packages/obsolescence_chain.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "You can obsolete a dataset using the function datamgmt::obsolete_package(). Use the documentation for instructions on using the function. The following workflow explains how the functions operate.\nThis chunk is to obsolete one data set. If there are more to add to the chain, more steps can be added. Be very careful. Make sure to fill in obsoletes and obsoletedBy slots for each one. The obsoletes and obsoletedBy fields must be NA, once they are populated they can’t be modified.\n\n# get oldest version of the file you want to be visible. Use get_all_versions and look at the latest.\n# urn:uuid:...\n\n# PID for data set to be obsoleted (hidden): doi:10…\n\n# adding data set to obsolete (hide) in the slot before the first version of the visible data set\nsysmeta1 <- getSystemMetadata(mn, \"urn:uuid:example_pid\")\nsysmeta1@obsoletes <- \"doi:10.example_doi\"\nupdateSystemMetadata(mn, \"urn:uuid:example_pid\", sysmeta1)\n\n# adding first version to obsolescence chain after obsoleted (hidden) version\nsysmeta0 <- getSystemMetadata(mn, \"doi:10.example_doi\")\nsysmeta0@obsoletedBy <- \"urn:uuid:example_pid\"\nupdateSystemMetadata(mn, \"doi:10.example_doi\", sysmeta0)\n\nThe following code is equivalent to the code chunk above. This method is recommended, however it is necessary to read the function documentation first.\n\ndatamgmt::obsolete_package(mn, metadata_obsolete = \"doi:10.example_doi\", \n                           metadata_new = \"urn:uuid:example_pid\")"
  },
  {
    "objectID": "workflows/edit_data_packages/02_create_package_data_pack.html",
    "href": "workflows/edit_data_packages/02_create_package_data_pack.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "adapted from the dataone and datapack vingettes\n\nlibrary(dataone)\nlibrary(datapack)\nlibrary(uuid)\n\nTo create a new data package, follow the code below. Remember, a data package is a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata.\n\ndp <- new(\"DataPackage\")\n\n\n\n\n\nIn this example we will use this previously written EML metadata. Here we are getting the file path from the dataone package and saving that as the object emlFile.\nThis is a bit of an unusual way to reference a local file path, but all this does is looks within the R package dataone and grabs the path to a metadata document stored within that package. If you print the value of emlFile you’ll see it is just a file path, but it points to a special place on the server where that package is installed. Usually you will just reference EML paths that are stored within your user file system.\n\nemlFile <- system.file(\"extdata/strix-pacific-northwest.xml\", \n                       package = \"dataone\")\n\nCreate a new DataObject and add it to the package. In the case below, our new DataObject will be a metadata file.\n\nmetadataObj <- new(\"DataObject\", \n                   format = \"https://eml.ecoinformatics.org/eml-2.2.0\", \n                   filename = emlFile)\n\ndp <- addMember(dp, metadataObj)\n\nCheck the dp object to see if the DataObject was added correctly.\n\ndp\n\n\n\n\n\nsourceData <- system.file(\"extdata/OwlNightj.csv\", package = \"dataone\")\n\nsourceObj <- new(\"DataObject\", format = \"text/csv\", filename = sourceData)\n\ndp <- addMember(dp, sourceObj, metadataObj)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to change the formatId please use updateSystemMetadata (more on this later in the book)\n\n\n\n\n\n\n\nd1c <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\nMake sure to give access privileges to the ADC admins. Although you may be tempted to edit the format of the string in the subject argument, you must keep it exactly as is. Otherwise you’ll run into error messages!\n\nmyAccessRules <- data.frame(subject = \"CN=arctic-data-admins,DC=dataone,DC=org\", \n                            permission = \"changePermission\") \n\nGet necessary token from test.arcticdata.io to upload the dataset prior uploading the datapackage:\n\npackageId <- uploadDataPackage(d1c, dp, public = TRUE, \n                               accessRules = myAccessRules, quiet = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to preserve folder structures, you can use this method\nIn this example, adding the csv files to a folder named data and scripts:\n\noutputData <- system.file(\"extdata/Strix-occidentalis-obs.csv\", package=\"dataone\") \n\noutputObj <- new(\"DataObject\", format = \"text/csv\", filename = outputData,\n                 targetPath = \"data\")\n\ndp <- addMember(dp, outputObj, metadataObj)\n\nprogFile <- system.file(\"extdata/filterObs.R\", package = \"dataone\")\n\nprogObj <- new(\"DataObject\", format = \"application/R\", filename = progFile, \n               targetPath = \"scripts\", mediaType = \"text/x-rsrc\")\n\ndp <- addMember(dp, progObj, metadataObj)"
  },
  {
    "objectID": "workflows/edit_data_packages/update_an_object.html",
    "href": "workflows/edit_data_packages/update_an_object.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "To update a data file associated with a data package, you need to do three things:\n\nupdate the object itself,\nupdate the resource map (which affiliates the object with the metadata), and\nupdate the metadata that describes that object\n\nThe datapack::replaceMember function takes care of the first two of these tasks. First you need to get the pid of the file you want to replace by using datapack::selectMember:\n\nmetadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n\nThen use replaceMember:\n\ndp <- replaceMember(dp, metadataId, replacement=file_path)\n\nIf you want to remove some files from the data package we can use datapack::removeMember. If we wanted to remove all the zip files associated with this data package, we can use datapack::removeMember:\n\nzipId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"application/vnd.shp+zip\")\nremoveMember(dp, zipId, removeRelationships = T)\n\n\n\n\n\n\n\nNote\n\n\n\nYou will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character.\n\n\nTo accomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information.\nOnce you have updated your metadata and saved it, you can update the package itself."
  },
  {
    "objectID": "workflows/solr_queries/query_solr_via_browser.html",
    "href": "workflows/solr_queries/query_solr_via_browser.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Solr is queried via what’s called an HTTP API (Application Program Interface). Practically, what this means it that you can execute a query in your browser by tacking a query onto a base URL.\nThis is similar to the way Google handles your searches. If I search “soil science” in Google, for example, the URL becomes:\nhttps://www.google.com/search?q=soil+science&oq=soil+science&aqs=chrome.0.69i59.1350j0j1&sourceid=chrome&ie=UTF-8\nIf I break it down into pieces, I get:\n\nthe base URL - https://www.google.com/search\n?, after which the query parameters are listed\nthe query - q=soil+science\nother parameters, which are separated by & - oq=soil+science&aqs=chrome.0.69i59.1350j0j1&sourceid=chrome&ie=UTF-8\n\nMost of the time, you’ll query either the Arctic Data Center member node or the PROD coordinating node, which have the following base URLs:\n\nArctic Data Center member node: https://arcticdata.io/metacat/d1/mn/v2/query/solr\nPROD coordinating node: https://cn.dataone.org/cn/v2/query/solr\n\nYou can then append your query parameters to your base URL:\nhttps://arcticdata.io/metacat/d1/mn/v2/query/solr/?q={QUERY}&fl={FIELDS}&rows={ROWS}\n\n\n\n\n\n\nNote\n\n\n\nVisit the base URL to see a list of fields Solr is storing for the objects it indexes.\nThere is a large set of queryable fields, though not all types of objects will have values set for all of the possible fields because some fields do not make sense for some objects (e.g., title for a CSV)."
  },
  {
    "objectID": "workflows/solr_queries/use_stats.html",
    "href": "workflows/solr_queries/use_stats.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "With stats, we can have Solr calculate statistics on numerical values (such as size).\n\nhttps://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:DATA&stats=true&stats.field=size&rows=0\n\nThis query calculates a set of summary statistics for the size field on data objects that Solr has indexed. In this case, Solr’s size field indexes the size field in the system metadata for each object in Metacat."
  },
  {
    "objectID": "workflows/solr_queries/construct_a_query.html",
    "href": "workflows/solr_queries/construct_a_query.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Each Solr query is comprised of a number of parameters. These are like arguments to a function in R, but they are entered as parts of a URL.\nThe most common parameters are:\n\nq: The query. This is like subset() or dplyr::filter() in R.\nfl: What fields are returned for the documents that match your query (q). If not set, all fields are returned.\nrows: The maximum number of documents to return. Solr will truncate your result if the result size is greater than rows.\nsort: Sorts the result by the values in the given Solr field (e.g., sort by date uploaded).\n\nThe query (q) parameter uses a syntax that looks like field:value, where field is one of the Solr fields and value is an expression. The expression can match a specific value exactly, e.g., q=identifier:arctic-data.7747.1 or q=identifier:\"doi:10.5065/D60P0X4S\", which finds the Solr document for a specific Object by PID (identifier).\n\n\n\n\n\n\nNote\n\n\n\nIn the second example, the DOI PID is surrounded in double quotes. This is because Solr has reserved characters, of which : is one, so we have to help Solr by surrounding values with reserved characters in them in quotes or by escaping them.\n\n\nTo view the list of query-able parameters on the Arctic Data Center and their descriptions, you can visit https://arcticdata.io/metacat/d1/mn/v2/query/solr. The list of parameters is also provided below:\n\n\n  [1] \"solr\"                        \"_root_\"                     \n  [3] \"_text_\"                      \"_version_\"                  \n  [5] \"abstract\"                    \"archived\"                   \n  [7] \"attribute\"                   \"attributeDescription\"       \n  [9] \"attributeLabel\"              \"attributeName\"              \n [11] \"attributeUnit\"               \"author\"                     \n [13] \"authorGivenName\"             \"authorGivenNameSort\"        \n [15] \"authoritativeMN\"             \"authorLastName\"             \n [17] \"authorSurName\"               \"authorSurNameSort\"          \n [19] \"awardNumber\"                 \"awardTitle\"                 \n [21] \"beginDate\"                   \"blockedReplicationMN\"       \n [23] \"changePermission\"            \"checksum\"                   \n [25] \"checksumAlgorithm\"           \"class\"                      \n [27] \"collectionQuery\"             \"contactOrganization\"        \n [29] \"contactOrganizationText\"     \"datasource\"                 \n [31] \"dataUrl\"                     \"dateModified\"               \n [33] \"datePublished\"               \"dateUploaded\"               \n [35] \"decade\"                      \"documents\"                  \n [37] \"eastBoundCoord\"              \"edition\"                    \n [39] \"endDate\"                     \"family\"                     \n [41] \"fileID\"                      \"fileName\"                   \n [43] \"formatId\"                    \"formatType\"                 \n [45] \"funderIdentifier\"            \"funderName\"                 \n [47] \"funding\"                     \"fundingText\"                \n [49] \"gcmdKeyword\"                 \"genus\"                      \n [51] \"geoform\"                     \"geohash_1\"                  \n [53] \"geohash_2\"                   \"geohash_3\"                  \n [55] \"geohash_4\"                   \"geohash_5\"                  \n [57] \"geohash_6\"                   \"geohash_7\"                  \n [59] \"geohash_8\"                   \"geohash_9\"                  \n [61] \"hasPart\"                     \"id\"                         \n [63] \"identifier\"                  \"investigator\"               \n [65] \"investigatorText\"            \"isDocumentedBy\"             \n [67] \"isPartOf\"                    \"isPublic\"                   \n [69] \"isService\"                   \"isSpatial\"                  \n [71] \"keyConcept\"                  \"keywords\"                   \n [73] \"keywordsText\"                \"kingdom\"                    \n [75] \"label\"                       \"language\"                   \n [77] \"logo\"                        \"LTERSite\"                   \n [79] \"mediaType\"                   \"mediaTypeProperty\"          \n [81] \"namedLocation\"               \"noBoundingBox\"              \n [83] \"northBoundCoord\"             \"numberReplicas\"             \n [85] \"obsoletedBy\"                 \"obsoletes\"                  \n [87] \"ogcUrl\"                      \"order\"                      \n [89] \"origin\"                      \"originator\"                 \n [91] \"originatorText\"              \"originText\"                 \n [93] \"parameter\"                   \"parameterText\"              \n [95] \"phylum\"                      \"placeKey\"                   \n [97] \"preferredReplicationMN\"      \"presentationCat\"            \n [99] \"project\"                     \"projectText\"                \n[101] \"prov_generated\"              \"prov_generatedByExecution\"  \n[103] \"prov_generatedByProgram\"     \"prov_generatedByUser\"       \n[105] \"prov_hasDerivations\"         \"prov_hasSources\"            \n[107] \"prov_instanceOfClass\"        \"prov_used\"                  \n[109] \"prov_usedByExecution\"        \"prov_usedByProgram\"         \n[111] \"prov_usedByUser\"             \"prov_wasDerivedFrom\"        \n[113] \"prov_wasExecutedByExecution\" \"prov_wasExecutedByUser\"     \n[115] \"prov_wasGeneratedBy\"         \"prov_wasInformedBy\"         \n[117] \"pubDate\"                     \"purpose\"                    \n[119] \"readPermission\"              \"relatedOrganizations\"       \n[121] \"replicaMN\"                   \"replicationAllowed\"         \n[123] \"replicationStatus\"           \"replicaVerifiedDate\"        \n[125] \"resourceMap\"                 \"rightsHolder\"               \n[127] \"scientificName\"              \"sem_annotated_by\"           \n[129] \"sem_annotates\"               \"sem_annotation\"             \n[131] \"sem_comment\"                 \"sensor\"                     \n[133] \"sensorText\"                  \"seriesId\"                   \n[135] \"serviceCoupling\"             \"serviceDescription\"         \n[137] \"serviceEndpoint\"             \"serviceInput\"               \n[139] \"serviceOutput\"               \"serviceTitle\"               \n[141] \"serviceType\"                 \"site\"                       \n[143] \"siteText\"                    \"size\"                       \n[145] \"sku\"                         \"source\"                     \n[147] \"sourceText\"                  \"southBoundCoord\"            \n[149] \"species\"                     \"submitter\"                  \n[151] \"term\"                        \"termText\"                   \n[153] \"text\"                        \"title\"                      \n[155] \"titlestr\"                    \"topic\"                      \n[157] \"topicText\"                   \"updateDate\"                 \n[159] \"webUrl\"                      \"westBoundCoord\"             \n[161] \"writePermission\""
  },
  {
    "objectID": "workflows/solr_queries/y_example_solr_queries.html",
    "href": "workflows/solr_queries/y_example_solr_queries.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "For additional examples and explanations, check out the Apache Lucene Query Parser Syntax page.\n\n\n\n\n\n\nresult <- query(adc, list(q=\"*:*\",\n                               fl=\"*\",\n                               rows=\"20\"),\n                as = \"data.frame\")\n\n\n\n\n\n#find any id that starts with arctic-data.6\nresult <- query(adc, list(q=\"id:arctic-data.6*\",\n                               rows=\"5000\"),\n                as = \"data.frame\")\n\n\n\n\nMultiple fields can be queried at once by using either the AND or OR arguments.\n\nresult <- query(adc, list(q=\"title:soil*+AND+origin:Ludwig\",\n                               rows=\"5000\"),\n                as = \"data.frame\")\n\n\nresult <- query(adc, list(q=\"title:soil* OR origin:Ludwig\",\n                               rows=\"5000\"),\n                as = \"data.frame\")\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use either spaces or + to separate query parameters. When typing queries in R, it’s often easier to read if you use spaces. However, when using the browser, you may want to use + to keep the query clean. (The browser will replace spaces with %20.)\n\n\n\n\n\nObjects are queryable via their formatType which is one of DATA, METADATA, or RESOURCE.\n\nresult <- query(adc, list(q = 'formatType:RESOURCE AND submitter:\"http://orcid.org/0000-0002-2561-5840\"',\n                          fl = 'identifier,submitter,fileName',\n                          sort = 'dateUploaded+desc',\n                          rows='10'),\n                as = \"data.frame\")\n\n\nresult <- query(adc, list(q = 'formatType:METADATA AND title:*Toolik*',\n                          fl = 'identifier,submitter,fileName',\n                          sort = 'dateUploaded+desc',\n                          rows='10'),\n                as = \"data.frame\")\n\n\n\n\n\nresult <- query(adc, list(q = 'submitter:\"http://orcid.org/0000-0003-4703-1974\"',\n               fl = 'identifier,submitter,fileName, size',\n               sort = 'dateUploaded+desc',\n               rows='1000'),\n      as = \"data.frame\")\n\n\n\n\n\n# Wrap the pid with special characters with escaped backslashes\ndataone::query(adc, list(q = paste0('id:', '\\\"', 'doi:10.18739/A20R9M36V', '\\\"'),\n                         fl = 'dateUploaded AND identifier',\n                         rows = 5000),\n               as = \"data.frame\")\n\n\n\n\n\nresult <- query(adc, list(q=\"title:(soil* AND carbo*)\",\n                               rows=\"5000\"),\n                as = \"data.frame\") \n\n\n\n\n\nresult <- query(adc, list(q = \"rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)\",\n                          fl = \"identifier,rightsHolder,formatId\",\n                          start =\"0\",\n                          rows = \"1500\"),\n                     as=\"data.frame\")\n\n\n\n\nJust add - before a query parameter!\n\nresult <- query(adc, list(q=\"title:(soil AND -carbon)\",\n                               rows=\"5000\"),\n                as = \"data.frame\")\n\n\n\n\n\nresult <- query(cn, list(q=\"title:soil* AND origin:Ludwig\",\n                              rows=\"5000\"),\n                as = \"data.frame\")\n\n\n\n\n\n# Wrap the pid in escaped quotation marks if it contains special characters \nquery(adc, list(q = paste0('documents:', '\\\"', 'urn:uuid:f551460b-ce36-4dd3-aaa6-3a6c6e338ec9', '\\\"'),\n                fl = \"identifier\",\n                rows=\"20\"),\n      as = \"data.frame\")\n\n# Alternatively we can use a wildcard expression in place of \"urn:uuid:\"\n# This way we don't need to escape the colon special characters \nquery(adc, list(q = paste0(\"documents:*f551460b-ce36-4dd3-aaa6-3a6c6e338ec9\"),\n                fl = \"identifier\",\n                rows=\"20\"),\n      as = \"data.frame\")\n\n\n\n\n\nquery(adc, list(q=\"dateUploaded:[2020-05-06T00:00:00Z TO NOW]\",\n                fl=\"title,identifier,resourceMap,dateUploaded,dateModified\",\n                sort = \"dateUploaded+desc\",\n                rows=\"200\"), \n      as = \"data.frame\")\n\n\n\n\nAll resource maps with > 100 data objects that are not on the Arctic Data Center:\n\nhttps://cn.dataone.org/cn/v2/query/solr/?q=resourceMap:*+AND+-datasource:*ARCTIC*&rows=0&facet=true&facet.field=resourceMap&facet.mincount=100"
  },
  {
    "objectID": "workflows/solr_queries/query_solr_via_R.html",
    "href": "workflows/solr_queries/query_solr_via_R.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Though it’s possible to query Solr directly through its HTTP API, we typically run our queries through R, for two main reasons:\n\nThe result is returned in a more useful way to R without extra work on your part\nWe can more easily pass our authentication token with the query\n\nWhy does #2 matter? Well by default, all of those URLs above only return publicly-readable Solr documents. If a private document matches any of those queries, Solr won’t tell you that. It will act like the non-public-readable documents don’t exist. So we must pass an authentication token to access non-public-readable content. This bit is crucial for working with the ADC, so you’ll very often want to use R instead of visiting those URLs in a web browser.\n\ncn <- CNode(\"PROD\")\nadc <- getMNode(cn, \"urn:node:ARCTIC\")\n# Set your token if you need/want!\n\n#string your parameters together like this: dataone::query(mn, \"q=title:*soil*&fl=title&rows=10\")\n#or alternatively, list them out:\nquery(adc, list(q=\"title:*soil*\",\n                fl=\"title\",\n                rows=\"10\"))\n\nBy default, query returns the result as a list, but a data.frame can be a more useful way to work with the result. To get a data.frame instead, just set the as argument to ‘data.frame’ to get a data.frame:\n\nquery(adc, list(q=\"title:*soil*\",\n                fl=\"title\",\n                rows=\"10\"),\n      as = \"data.frame\")"
  },
  {
    "objectID": "workflows/solr_queries/use_facets.html",
    "href": "workflows/solr_queries/use_facets.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "We can also summarize what’s in Solr with faceting, which lets us group Solr documents together and count them. This is like table in R. Faceting can do a query within a query, but more commonly it’s used to summarize unique values in a field. For example, we can find the unique formatIds on data objects:\n\nhttps://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=:&fq=formatType:DATA&facet=true&facet.field=formatId&rows=0\n\nTo facet, we usually do a few things:\n\nAdd the parameter facet=true\nAdd the parameter facet.field={FIELD} with the field we want to facet (group) on\nSet rows=0 because we don’t care about the matched Solr documents\nOptionally specify fq={expression} which filters out Solr documents before faceting. In the above example, we have to do this to count only data objects. Without it, the facet result would include formatIds for metadata and resource maps, which we don’t want.\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently, the dataone::query() function does not support faceting, so you’ll have to run your queries as a URL.\nFor additional ways to use faceting (such as pivot faceting), check out the Solr documentation."
  },
  {
    "objectID": "workflows/solr_queries/z_solr_resources.html",
    "href": "workflows/solr_queries/z_solr_resources.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Solr’s The Standard Query Parser docs (high level of detail)\nAnother quick reference: https://wiki.apache.org/solr/SolrQuerySyntax\nhttp://www.solrtutorial.com/"
  },
  {
    "objectID": "workflows/edit_eml/set_parties.html",
    "href": "workflows/edit_eml/set_parties.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The address, creator, contact, and associatedParty classes can easily be created using functions from the EML package. However it is often easier to just edit this through the webform.\nTo add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associatedParty classes.\n\nNCEASadd <- eml$address( \n                deliveryPoint = \"735 State St #300\", \n                city = \"Santa Barbara\", \n                administrativeArea = 'CA', \n                postalCode = '93101')\n\nHere, we use eml_creator() to set our data set creator.\n\nJC_creator <- eml$creator(individualName = list(givenName = \"Jeanette\", \n                                                surName = \"Clark\"), \n                          organization = \"NCEAS\", \n                          electronicMailAddress = \"jclark@nceas.ucsb.edu\", \n                          phone = \"123-456-7890\", \n                          userId = \"https://orcid.org/WWWW-XXXX-YYYY-ZZZZ\",\n                          address = NCEASadd)\ndoc$dataset$creator <- JC_creator\n\nSimilarly, we can set a single contact or multiple. In this case, there are two, so we set doc$dataset$contact as a list containing both of them.\n\nJC_contact <- eml$contact(individualName = list(givenName = \"Jeanette\", \n                                                surName = \"Clark\"), \n                          organization = \"NCEAS\", \n                          electronicMailAddress = \"jclark@nceas.ucsb.edu\", \n                          phone = \"123-456-7890\",  \n                          userId = \"https://orcid.org/WWWW-XXXX-YYYY-ZZZZ\",id = eml$userId(directory = eml$dir)\n                          address = NCEASadd)\n\nJG_contact <- eml$contact(individualName = list(givenName = \"Jesse\", \n                                                surName = \"Goldstein\"), \n                          organization = \"NCEAS\", \n                          electronicMailAddress = \"jgoldstein@nceas.ucsb.edu\", \n                          phone = \"123-456-7890\",  \n                          userId = \"https://orcid.org/WWWW-XXXX-YYYY-ZZZZ\",\n                          address = NCEASadd)\n\ndoc$dataset$contact <- list(JC_contact, JG_contact)\n\nFinally, the associatedPartys are set. Note that associatedPartys MUST have a role defined, unlike creator or contact.\n\nJG_ap <- eml$associatedParty(individualName = list(givenName = \"Jesse\", \n                                                   surName = \"Goldstein\"),\n                             organization = \"NCEAS\", \n                             email = \"jgoldstein@nceas.ucsb.edu\",\n                             electronicMailAddress = \"123-456-7890\",  \n                             address = NCEASadd, \n                             userId = \"https://orcid.org/WWWW-XXXX-YYYY-ZZZZ\",\n                             role = \"metadataProvider\")\ndoc$dataset$associatedParty <- JG_ap"
  },
  {
    "objectID": "workflows/edit_eml/edit_attributelists.html",
    "href": "workflows/edit_eml/edit_attributelists.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Attributes are descriptions of variables, typically columns or column names in tabular data. Attributes are stored in an attributeList. When editing attributes in R, you need to create one to three objects:\n\nA data.frame of attributes\nA data.frame of custom units (if applicable)\n\nThe attributeList is an element within one of 4 different types of entity objects. An entity corresponds to a file, typically. Multiple entities (files) can exist within a dataset. The 4 different entity types are dataTable (most common for us), spatialVector, spatialRaster, and otherEntity\nPlease note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector object for spatial data.\nTo edit or examine an existing attribute table already in an EML file, you can use the following commands, where i represents the index of the series element you are interested in. Note that if there is only one item in the series (ie there is only one dataTable), you should just call doc$dataset$dataTable, as in this case doc$dataset$dataTable[[1]] will return the first sub-element of the dataTable (the entityName)\n\n# If they are stored in an otherEntity (submitted from the website by default)\nattributeList <- EML::get_attributes(doc$dataset$otherEntity[[i]]$attributeList)\n\n# Or if they are stored in a dataTable (usually created by a datateam member)\nattributeList <- EML::get_attributes(doc$dataset$dataTable[[i]]$attributeList)\n\n# Or if they are stored in a spatialVector (usually created by a datateam member)\nattributeList <- EML::get_attributes(doc$dataset$spatialVector[[i]]$attributeList)\n\nattributes <- attributeList$attributes\nprint(attributes)\n\n\n\nAttribute information should be stored in a data.frame with the following columns:\n\nattributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp”\nattributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)”\nattributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.”\nmeasurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required.\n\nnominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River)\nordinal: ordered categories. e.g.: Low, Medium, High\ndateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001\nratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second.\ninterval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius.\n\ndomain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required.\n\ntextDomain: text that is free-form, or matches a pattern\nenumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake\ndateTimeDomain: dateTime attributes\nnumericDomain: attributes that are numbers (either ratio or interval)\n\nformatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”.\ndefinition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases.\nunit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found using: standardUnits <- EML::get_unitList() then running View(standardUnits$units).\nnumberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer.\n\nreal: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…)\nnatural: non-zero positive integers (1,2,3…)\nwhole: positive integers and zero (0,1,2,3…)\ninteger: positive and negative integers and zero (…-2,-1,0,1,2…)\n\nmissingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’\nmissingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists.\n\nYou can create attributes manually by typing them out in R following a workflow similar to the one below:\n\nattributes <- data.frame(\n    \n    attributeName = c('Date', 'Location', 'Region','Sample_No', 'Sample_vol', \n                      'Salinity', 'Temperature', 'sampling_comments'),\n    \n    attributeDefinition = c('Date sample was taken on', \n                            'Location code representing location where sample was taken',\n                            'Region where sample was taken', 'Sample number', 'Sample volume', \n                            'Salinity of sample in PSU', 'Temperature of sample', \n                            'comments about sampling process'),\n    \n    measurementScale = c('dateTime', 'nominal','nominal', 'nominal', 'ratio', \n                         'ratio', 'interval', 'nominal'),\n    \n    domain = c('dateTimeDomain', 'enumeratedDomain','enumeratedDomain', \n               'textDomain', 'numericDomain', 'numericDomain', \n               'numericDomain', 'textDomain'),\n    \n    formatString = c('MM-DD-YYYY', NA,NA,NA,NA,NA,NA,NA),\n    \n    definition = c(NA,NA,NA,'Sample number', NA, NA, NA, \n                   'comments about sampling process'),\n    \n    unit = c(NA, NA, NA, NA,'milliliter', 'dimensionless', 'celsius', NA),\n    \n    numberType = c(NA, NA, NA,NA, 'real', 'real', 'real', NA),\n    \n    missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, 'NA'),\n    \n    missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, \n                                    'no sampling comments'))\n\nHowever, typing this out in R can be a major pain. Luckily, there’s a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you) to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes (these commands will launch a Shiny app in your web browser):\n\n#first download the CSV in your data package from Exercise #2\ndata_pid <- selectMember(dp, name = \"sysmeta@fileName\", value = \".csv\")\ndata <- read.csv(text=rawToChar(getObject(d1c_test@mn, data_pid)))\n\n\n# From data (recommended)\nEML::shiny_attributes(data = data)\n\n# From an existing attribute table\nattributeList <- get_attributes(doc$dataset$dataTable[[i]]$attributeList)\nEML::shiny_attributes(data = NULL, attributes = attributeList$attributes)\n\n# From scratch\natts <- EML::shiny_attributes()\n\nOnce you are done editing a table in the app, quit the app and the tables will be assigned to the atts variable as a list of data frames (one for attributes, factors, and units). Alternatively, each table can be to exported to a csv file by clicking the Download button.\nIf you downloaded the table, read the table back into your R session and assign it to a variable in your script (e.g. attributes <- data.frame(...)), or just use the variable that shiny_attributes returned.\nFor simple attribute corrections, datamgmt::edit_attribute() allows you to edit the slots of a single attribute within an attribute list. To use this function, pass an attribute through datamgmt::edit_attribute() and fill out the parameters you wish to edit/update. An example is provided below where we are changing attributeName, domain, and measurementScale in the first attribute of a dataset. After completing the edits, insert the new version of the attribute back into the EML document.\n\nnew_attribute <- datamgmt::edit_attribute(doc$dataset$dataTable[[1]]$attributeList$attribute[[1]], \n                          attributeName = 'date_and_time', \n                          domain = 'dateTimeDomain', \n                          measurementScale = 'dateTime')\n\ndoc$dataset$dataTable[[1]]$attributeList$attribute[[1]] <- new_attribute\n\n\n\n\nEML has a set list of units that can be added to an EML file. These can be seen by using the following code:\n\nstandardUnits <- EML::get_unitList()\nView(standardUnits$units)\n\nSearch the units list for your unit before attempting to create a custom unit. You can search part of the unit you can look up part of the unit ie meters in the table to see if there are any matches.\nIf you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields:\n\nid: The unit id (ids are camelCased)\nunitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes)\nparentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”)\nmultiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000)\nname: Unit abbreviation (e.g. for kilometer name = “km”)\ndescription: Text defining the unit (e.g. for kilometer description = “1000 meters”)\n\nTo manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template:\n\ncustom_units <- data.frame(   \n  id = c('siemensPerMeter', 'decibar'),\n  unitType = c('resistivity', 'pressure'),\n  parentSI = c('ohmMeter', 'pascal'),\n  multiplierToSI = c('1','10000'),\n  abbreviation = c('S/m','decibar'),\n  description = c('siemens per meter', 'decibar'))\n\nUsing EML::get_unit_id for custom units will also generate valid EML unit ids. Custom units are then added to additionalMetadata using the following command:\n\nunitlist <- set_unitList(custom_units, as_metadata = TRUE)\ndoc$additionalMetadata <-  list(metadata = list(unitList = unitlist))\n\n\n\n\nFor attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition.\n\nattributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute.\ncode should contain all unique values of the given attributeName that exist within the actual data.\ndefinition should contain a plain text definition that describes each code.\n\nTo build factors by hand, you use the named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”.\n\nLocation <- c(CASC = 'Cascade Lake', CHIK = 'Chikumunik Lake', \n              HEAR = 'Heart Lake', NISH = 'Nishlik Lake' )\n\nRegion <- c(W_MTN = 'West region, locations West of Eagle Mountain', \n            E_MTN = 'East region, locations East of Eagle Mountain')\n\nThe definitions are then written into a data.frame using the names of the named character vectors and their definitions.\n\nfactors <- rbind(data.frame(attributeName = 'Location', \n                            code = names(Location), \n                            definition = unname(Location)),\n                  data.frame(attributeName = 'Region', code = names(Region), \n                             definition = unname(Region)))\n\n\n\n\nOnce you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using the following command:\n\nattributeList <- EML::set_attributes(attributes = attributes,\n                                     factors = factors) \n\nThis attributeList must then be added to a dataTable."
  },
  {
    "objectID": "workflows/edit_eml/series_identifier.html",
    "href": "workflows/edit_eml/series_identifier.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "A series identifier is a system metadata field that represents a single identifier across multiple versions of a data package, a feature often requested by submitters. These are useful for maintaining a single identifier when a data package is expected to receive updates (usually additional data) in the future. For example, this dataset has a series id: https://doi.org/10.18739/A2154DQ22 (the doi and urn:uuid are both present at the top of the dataset).\n\n\nAdding a series identifier should be the last step. In most cases the SID will be in DOI format. Once the data package is complete (peer-reviewd and approved by the PI) the series identifier can be added by updating the seriesId field of the system metadata of the metadata object.\n\n# The metadata identifier we're assigning an SID to\nmetadata_pid <- 'metadata_pid'\n\nsys <- getSystemMetadata(mn, metadata_pid)\nsys@seriesId <- generateIdentifier(mn, scheme = 'DOI') #update the scheme argument if it should not be a DOI\nupdateSystemMetadata(mn, metadata_pid, sys)\n\n\n\n\nWhen updating a parent package with a series identifier you need to use update_resource_map rather than publish_update. In this case our goal is add nested data packages to an existing parent package.\n\n# Call get_package on the parent package that has a `seriesId` in its system metadata \nparent_package <- get_package(mn, 'resource_map_pid')\n# The resource map of the package we want to add to the 'child_pids' of the parent\nresource_map_new_child <- '' \n\nupdate_resource_map(mn, parent_package$resource_map, parent_package$metadata, parent_package$data,\n                    c(parent_package$child_packages, resource_map_new_child))\n\n\n\n\nWhen updating the metadata (xml) of a package with a series identifier use publish_update. It’s important that you pass the metadata pid to the metadata_pid argument rather than seriesId. The metadata pid will usually have the text “version:” ahead of it, however it’s best to use get_package first to avoid mistakes.\n\npkg <- get_package(mn, 'resource_map_pid')\npkg <- publish_update(mn, pkg$metadata, pkg$resource_map, pkg$data, \n                      child_pids = pkg$child_pids) # + any additional arguments to \"publish_update\""
  },
  {
    "objectID": "workflows/edit_eml/format_text.html",
    "href": "workflows/edit_eml/format_text.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Currently, only certain fields (abstracts, methods) support text formatting in EML. Check out this demo for a full example. Additional info is also available here.\n\n\n\n\n\n\nNote\n\n\n\nMany of these formatting functions only work when enclosed by <para></para>\n\n\nYou can insert these tags directly into the EML document using syntax that looks like this:\n\ndoc$dataset$abstract <- eml$abstract(para = \"Some abstracts require subscripts like CO<subscript>2</subscript>\")\n\n\n\nSubscripts, superscripts, and italics:\n<subscript>You can do subscripts</subscript>\n<superscript>or superscipts</superscript>\n<emphasis>or even italics.</emphasis> \n\n\n\nBe sure to include the “https://” before the link or it will redirect incorrectly. Also, always check that your links go through to the correct page. Please be aware that most links are inherently unstable so always default to archiving files over pointing to websites when possible and appropriate.\n<ulink url=\"https://some_url.com\">\n    <citetitle>some text</citetitle>\n</ulink>\n\n\n\nUnordered (bulleted) lists:\n<itemizedlist>\n    <listitem>\n        <para>Paragraphs</para>\n    </listitem>\n    <listitem>\n        <para>Sections w/ subsections (w/ titles)</para>\n    </listitem>\n</itemizedlist>\nOrdered lists (1, 2, 3)…\n<orderedlist>\n    <listitem>\n        <para>something</para>\n    </listitem>\n    <listitem>\n        <para>something else</para>\n    </listitem>\n</orderedlist>"
  },
  {
    "objectID": "workflows/edit_eml/set_physical.html",
    "href": "workflows/edit_eml/set_physical.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "To set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io!\n\n\n\n\n\n\nNote\n\n\n\nThe word ‘physical’ derives from database systems, which distinguish the ‘logical’ model (e.g., what attributes are in a table, etc) from the physical model (how the data are written to a physical hard disk (basically, the serialization). so, we grouped metadata about the file (eg. dataformat, file size, file name) as written to disk in physical.\n\n\n\ndata_pid <- selectMember(dp, name = \"sysmeta@fileName\", \n                         value = \"your_file_name.csv\")\n\nphysical <- arcticdatautils::pid_to_eml_physical(d1c@mn, data_pid)\n\nThe physical must then be assigned to the data object.\nNote that the above workflow only works if your data object already exists on the member node."
  },
  {
    "objectID": "workflows/edit_eml/edit_spatialdata.html",
    "href": "workflows/edit_eml/edit_spatialdata.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Occasionally, you may encounter a third type of data object: spatialVector and spatialRaster. These objects contains spatial data (ie maps), such as a shapefile or geodatabase.\nEditing a spatialVector or spatialRaster is similar to editing a dataTable or an otherEntity. A physical and attributeList should be present. We will focus on how to get the information unique to spatialData and how to create the spatialVector/spatialRaster\n\n\nFile extensions to look for that might be spatial data: kml, geoJSON, geoTIFF, .dbf, .shp, and .shx\nAdditionally, spatial data that involve multiple files should typically be archived within a .zip file to ensure all related and interdependent files stay together (ie . a geodatabase). This is one of the exceptions to our rule regarding .zip files.\nFor example, a spatial dataset for a shapefile should, at a minimum, consist of separate .dbf, .shp, and .shx files with the same prefix in the same directory. All these files are required in order to use the data. Also note that shapefiles limit attribute names to 10 characters, so attribute names in the metadata may not match exactly to attribute names in the data. Some spatial raster data come as standalone files (.tiff or .nc) and some come as a group of files. If you aren’t sure whether to unzip a file, ask Jasmine or Jeanette.\n\n\n\n\n\n\nNote\n\n\n\nThere are specific formatIds for these kinds of zipped files: application/vnd.shp+zip image/geotiff+zip. Remember to check that the files have the correct formatId\n\n\n\n\n\nRead in the files to (1) help you in creating your attributes table and (2) sometimes also figure out the coordinate reference system.\nlibrary(sf)\nspatial_file <- sf::read_sf(\"example.kml\")\n\n\n\n\n\n\nNote\n\n\n\nWhen you read kml files, read_sf() sometimes shows additional columns that aren’t in the actual file. Always open kml files in text editor to check if the columns actually exist.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf it is a zipped shapefile there is a handy function you can use arcticdatautils::read_zip_shapefile(mn, pid)\n\n\n\n\nThe coordinate system allow to work with spatial data using the same frame of reference (a Datum). A common coordinate system is “GCS_WGS_1984 (used in Google Maps!) which is suitable for plotting points distributed globally. There are many others that may be better suited for certain areas in the world.\nAll latitudes and longitude coordinates should have a coordinate system (like a frame of reference).\nThere are horizontal coordinate systems (earth’s surface) and vertical coordinate systems (depth). More information can be found here.\nTo find the horizCoordSysName you can use:\nsf::st_crs(spatial_file)\nTake the Datum and add GCS (Geographic Coordinate System) in front. For example: “GCS_WGS_1984”\n\n\n\n\n\n\nOne important difference is that a spatialVector object should also have a geometry slot that describes the geometry features of the data. The possible values include one or more (in a list) of ‘Point’, ‘LineString’, ‘LinearRing’, ‘Polygon’, ‘MultiPoint’, ‘MultiLineString’, ‘MultiPolygon’, or ‘MultiGeometry’. You will likely have to open the file itself within QGIS or R (ie . the sf package) to get the correct geometry value.\nTo add just a geometry slot use:\n\ndoc$dataset$spatialVector[[1]]$geometry <- \"Polygon\"\n\nTo add it using the data pid: 1. Get the geometry and spatialReference 2. Use pid_to_eml_entity() to generate the spatialVector\n spatialVector <- pid_to_eml_entity(adc, \n                                    pkg$data[n], \n                                    entity_type = \"spatialVector\",\n                                    entityName = \"filename.kml\",\n                                    entityDescription = \"some desciption\",\n                                    attributeList = attributeList,\n                                    geometry = \"Point\",\n                                    spatialReference = list(horizCoordSysName = \"GCS_WGS_1984\"))\n\nAdd the spatialVector to the doc\n\ndoc$dataset$spatialVector[[1]] <- spatialVector\n\n\n\n\nMost often these come in GeoTiff or Tiff files. The data is presented as a grid of “pixels”. For more information ESRI has a indepth article here.\nTo use the helper function get:\n\nthe path of your raster file\nan attribute table\na coordinate system\n\nTo get a coordinate system name, you can use the output of the function on your first try (which will print the coordinate reference system, if it is defined). You can use the return value of get_coord_list() (a large data.frame) to find the correct coordinate system name.\nAnother way to get the coordinate system name is using rgdal::GDALinfo(path). This function can provide many details for your GeoTiff or Tiff files including the coordinate system name. More information can be found here here.\nrgdal::GDALinfo(path)\n\neml_get_raster_metadata(path, coord_name, attributes)"
  },
  {
    "objectID": "workflows/edit_eml/edit_otherentities.html",
    "href": "workflows/edit_eml/edit_otherentities.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable.\n\ndoc$dataset$otherEntity[[i]] <- NULL\n\n\n\n\nIf you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity.\n\notherEntity <- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]])\n\nAlternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID.\n\notherEntity <- arcticdatautils::pid_to_eml_entity(mn, \"your_data_pid\", entityType = \"otherEntity\", entityName = \"Entity Name\", entityDescription = \"Description about entity\")\n\nThe otherEntity must then be set to the EML, like so:\n\ndoc$dataset$otherEntity <- otherEntity\n\nIf you have more than one otherEntity object in the EML already, you can add the new one like this:\n\ndoc$dataset$otherEntity[[i]] <- otherEntity\n\nWhere i is set to the number of existing entities plus one. Remember the warning from the last section, however. If you only have one otherEntity, and you are trying to add another, you have to run:\n\ndoc$dataset$otherEntity <- list(otherEntity, doc$dataset$otherEntity)"
  },
  {
    "objectID": "workflows/edit_eml/use_references.html",
    "href": "workflows/edit_eml/use_references.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "References are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including:\n\nMaking it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes)\nReducing the size on disk of EML records with highly redundant information\nFaster read/write/validate with the R EML package\n\nYou may want to use EML references if you have the following scenarios (not exhaustive):\n\nOne person has multiple roles in the dataset (creator, contact, etc)\nOne or more entities shares all or some attributes\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not use references for creators as it is used for the citation information. The creators will not show up on the top of the dataset if it is a reference. Until this issue is resolved in NCEAS/metacat#926 we will need to keep this in account.\n\n\nIt’s very common to see the contact and creator referring to the same person with XML like this:\n<eml packageId=\"my_test_doc\" system=\"my_system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1 eml.xsd\">\n  <dataset>\n    <creator>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </contact>\n  </dataset>\n</eml>\nSo you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt.\n\ndoc$dataset$creator[[1]]$id  <- \"reference_id\"\ndoc$dataset$contact <- list(references = \"reference_id\") \nprint(doc)\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<eml:eml xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stmml=\"http://www.xml-cml.org/schema/stmml-1.1\" packageId=\"id\" system=\"system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1/ eml.xsd\">\n  <dataset>\n    <title>A Minimal Valid EML Dataset</title>\n    <creator id=\"reference_id\">\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <references>reference_id</references>\n    </contact>\n  </dataset>\n</eml:eml>\n\n\n\n\n\n\nNote\n\n\n\nThe reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that.\n\n\n\n\n\nTo use references with attributes:\n\nAdd an attribute list to a data table\nAdd a reference id for that attribute list\nUse references to add that information into the attributeLists of the other data tables\n\nFor example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest:\n\ndoc$dataset$dataTable[[1]]$attributeList <- attribute_list\ndoc$dataset$dataTable[[1]]$attributeList$id <- \"shared_attributes\" # use any unique name for your id\n\nfor (i in 2:length(doc$dataset$dataTable)) {\n  doc$dataset$dataTable[[i]]$attributeList <- list(references = \"shared_attributes\") # use the id you set above\n}"
  },
  {
    "objectID": "workflows/edit_eml/edit_datatables.html",
    "href": "workflows/edit_eml/edit_datatables.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable using the eml$dataTable() helper function as below:\n\ndataTable <- eml$dataTable(entityName = \"A descriptive name for the data (does not need to be the same as the data file)\",\n                           entityDescription = \"A description of the data\",\n                           physical = physical,\n                           attributeList = attributeList)\n\nThe dataTable must then be added to the EML. How exactly you do this will depend on whether there are dataTable elements in your EML, and how many there are. To replace whatever dataTable elements already exist, you could write:\n\ndoc$dataset$dataTable <- dataTable\n\nIf there is only one dataTable in your dataset, the EML package will usually “unpack” these, so that it is not contained within a list of length 1 - this means that to add a second dataTable, you cannot use the syntax doc$dataset$dataTable[[2]], since when unpacked this will contain the entityDescription as opposed to pointing to the second in a series of dataTable elements. Confusing - I know. Not to fear though - this syntax will get you on your way, should you be trying to add a second dataTable.\n\ndoc$dataset$dataTable <- list(doc$dataset$dataTable, dataTable)\n\nIf there is more than one dataTable in your dataset, you can return to the more straightforward construction of:\n\ndoc$dataset$dataTable[[i]] <- dataTable \n\nWhere i is the index that you wish insert your dataTable into.\nTo add a list of dataTables to avoid the unpacking problem above you will need to create a list of dataTables\n\ndts <- list() # create an empty list\nfor(i in seq_along(tables_you_need)){\n  # your code modifying/creating the dataTable here\n  dataTable <- eml$dataTable(entityName = dataTable$entityName,\n                             entityDescription = dataTable$entityDescription,\n                             physical = physical,\n                             attributeList = attributeList)\n  \n  dts[[i]] <- dataTable # add to the list\n}\n\nAfter getting a list of dataTables, assign the resulting list to dataTable EML.\n\ndoc$dataset$dataTable <- dts\n\nBy default, the online submission form adds all entities as otherEntity, even when most should probably be dataTable. You can use eml_otherEntity_to_dataTable to easily move items in otherEntity over to dataTable. Most tabular data or data that contain variables should be listed as a dataTable. Data that do not contain variables (eg: plain text readme files, pdfs, jpegs) should be listed as otherEntity.\n\neml_otherEntity_to_dataTable(doc, \n                             1, # which otherEntities you want to convert, for multiple use - 1:5\n                             validate_eml = F) # set this to False if the physical or attributes are not added"
  },
  {
    "objectID": "workflows/edit_eml/edit_semantic_annotation.html",
    "href": "workflows/edit_eml/edit_semantic_annotation.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "For a brief overview of what a semantic annotation is, and why we use them check out this video.\nEven more information on how to add semantic annotations to EML 2.2.0 can be found here. Currently metacatUI does not support the editing of semantic annotations on the website so all changes will have to be done in R.\nThere are several elements in the EML 2.2.0 schema that can be annotated:\n\ndataset\nentity (eg: otherEntity or dataTable)\nattribute\n\nOn the datateam, we will only be adding annotations to attributes for now.\n\n\nThis is a dataset that has semantic annotations included.\nOn the website you can see annotations in each of the attributes.\n\n\nYou can click on any one of them to search for more datasets with that same annotation.\n\n\n\nTo add annotations to the attributeList you will need information about the propertyURI and valueURI\nAnnotations are essentially composed of a sentence, which contains a subject (the attribute), predicate (propertyURI), and object (valueURI). Because of the way our search interface is built, for now we will be using attribute annotations that have a propertyURI label of “contains measurements of type”.\nHere is what an annotation for an attribute looks like in R. Note that both the propertyURI and valueURI have both a label, and the URI itself.\n\ndoc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$annotation\n\n$id\n[1] \"ODBcOyaTsg\"\n\n$propertyURI\n$propertyURI$label\n[1] \"contains measurements of type\"\n\n$propertyURI$propertyURI\n[1] \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType\"\n\n\n$valueURI\n$valueURI$label\n[1] \"Distributed Biological Observatory region identifier\"\n\n$valueURI$valueURI\n[1] \"http://purl.dataone.org/odo/ECSO_00002617\"\n\n\n\n\n\n\nNote\n\n\n\nSemantic attribute annotations can be applied to spatialRasters, spatialVectors and dataTables\n\n\n\n\n\n\n1. Decide which variable to annotate\nThe goal for the datateam is to start annotating every dataset that comes in. Please make sure to add semantic annotations to spatial and temporal features such as latitude, longitude, site name and date and aim to annotate as many attributes as possible.\n2. Find an appropriate valueURI\nThe next step is to find an appropriate value to fill in the blank of the sentence: “this attribute contains measurements of _____.”\nThere are several ontologies to search in. In order of most to least likely to be relevant to the Arctic Data Center they are:\n\nThe Ecosystem Ontology (ECSO)\n\nthis was developed at NCEAS, and has many terms that are relevant to ecosystem processes, especially those involving carbon and nutrient cycling\n\nThe Environment Ontology (EnVO)\n\nthis is an ontology for the concise, controlled description of environments\n\nNational Center for Biotechnology Information (NCBI) Organismal Classification (NCBITAXON)\n\nThe NCBI Taxonomy Database is a curated classification and nomenclature for all of the organisms in the public sequence databases.\n\nInformation Artifact Ontology (IAO)\n\nthis ontology contains terms related to information entities (eg: journals, articles, datasets, identifiers)\n\n\nTo search, navigate through the “classes” until you find an appropriate term. When we are picking terms, it is important that we not just pick a similar term or a term that seems close - we want a term that is totally “right”. For example, if you have an attribute for carbon tetroxide flux and an ontology with a class hierarchy like this:\n– carbon flux\n|—- carbon dioxide flux\nOur exact attribute, carbon tetroxide flux is not listed. In this case, we should pick “carbon flux” as it’s completely correct and not “carbon dioxide flux” because it’s more specific but not quite right.\n\n\n\n\n\n\nNote\n\n\n\nFor general attributes (such as ones named depth or length), it is important to be as specific as possible about what is being measured.\ne.g. selecting the lake area annotation for the area attribute in this dataset\n\n\n3. Build the annotation in R\n\n\nthis method is great for when you are inserting 1 annotation, fixing an existing annotation or programmatically updating annotations for multiple attributeLists\nFirst you need to figure out the index of the attribute you want to annotate.\n\neml_get_simple(doc$dataset$dataTable[[3]]$attributeList, \"attributeName\")\n\n [1] \"prdM\"         \"t090C\"        \"t190C\"        \"c0mS/cm\"      \"c1mS/cm\"      \"sal00\"        \"sal11\"        \"sbeox0V\"      \"flECO-AFL\"\n[10] \"CStarTr0\"     \"cpar\"         \"v0\"           \"v4\"           \"v6\"           \"v7\"           \"svCM\"         \"altM\"         \"depSM\"    \n[19] \"scan\"         \"sbeox0ML/L\"   \"sbeox0dOV/dT\" \"flag\"    \nNext, assign an id to the attribute. It should be unique within the document, and it’s nice if it is human readable and related to the attribute it is describing. One format you could use is entity_x_attribute_y which should be unique in scope, and is nice and descriptive.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$id <- \"entity_ctd_attribute_salinity\"\n\nNow, assign the propertyURI information. This will be the same for every annotation you build.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$propertyURI <- list(label = \"contains measurements of type\",\n                                                                                       propertyURI = \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType\")\n\nFinally, add the valueURI information from your search.\n You should see an ID on the Bioportal page that looks like a URL - this is the valueURI. Use the value to populate the label element.\n\ndoc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$valueURI <- list(label = \"Water Salinity\",\n     valueURI = \"http://purl.dataone.org/odo/ECSO_00001164\")\n\n\n\n\nthis method is great for when you are updating many attributes\nOn the far right of the table of shiny_attributes there are 4 columns: id, propertyURI, propertyLabel, valueURI, valueLabel that can be filled out.\n\n\n\n\nSensitive datasets that might cover protected characteristics (human subjects data, endangered species locations, etc) should be annotated using the data sensitivity ontology: https://bioportal.bioontology.org/ontologies/SENSO/?p=classes&conceptid=root.\n\n\nAs a final step in the data processing pipeline, we will categorize the dataset. We are trying to categorize datasets so we can have a general idea of what kinds of data we have at the Arctic Data Center.\nDatasets will be categorized using the Academic Ontology. These annotations will be seen at the top of the landing page, and can be thought of as “themes” for the dataset. In reality, they are dataset-level annotations.\nBe sure to ask your peers in the #datateam slack channel whether they agree with the themes you think best fit your dataset. Once there is consensus, use the following line of code:\n\ndoc <- datamgmt::eml_categorize_dataset(doc, c(\"list\", \"of\", \"themes\"))"
  },
  {
    "objectID": "workflows/edit_eml/edit_custom_units.html",
    "href": "workflows/edit_eml/edit_custom_units.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "EML has a set list of units that can be added to an EML file. These can be seen by using the following code:\n\nstandardUnits <- EML::get_unitList()\nView(standardUnits$units)\n\nIf you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields:\n\nid: The unit id (ids are camelCased)\nunitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes)\nparentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”)\nmultiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000)\nname: Unit abbreviation (e.g. for kilometer name = “km”)\ndescription: Text defining the unit (e.g. for kilometer description = “1000 meters”)\n\nTo manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template:\n\ncustom_units <- data.frame(\n    \n  id = c('partsPerThousand', 'decibar', 'wattsPerSquareMeter', 'micromolesPerGram', 'practicalSalinityUnit'),\n  unitType = c('dimensionless', 'pressure', 'power', 'amountOfSubstanceWeight'， 'dimensionless'),\n  parentSI = c(NA, 'pascal', 'watt', 'molesPerKilogram'， NA),\n  multiplierToSI = c(NA, '10000', '1', '1000000000', NA),\n  abbreviation = c('ppt', 'decibar', 'W/m^2', 'umol/g', 'PSU'),\n  description = c('parts per thousand', 'decibar', 'watts per square meter', 'micro moles per gram', 'used to describe the concentration of dissolved salts in water, the UNESCO Practical Salinity Scale of 1978 (PSS78) defines salinity in terms of a conductivity ratio'))\n\nUsing EML::get_unit_id for custom units will also generate valid EML unit ids.\nCustom units are then added to additionalMetadata using the following command:\n\nunitlist <- set_unitList(custom_units, as_metadata = TRUE)\ndoc$additionalMetadata <- unitlist\n\nIf units that should be standardUnit are added as customUnit, you can use the following code to fix this issue:\n\n# add standard unit\ndoc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$measurementScale$ratio$unit$standardUnit <- \"your standard unit\"\n\n# get rid of custom unit\ndoc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$measurementScale$ratio$unit$customUnit <- NULL\n\nIf you want to find all of the positions of a certain custom unit that should be standard, try this code:\n\nls <- purrr::map(doc$dataset$dataTable[[i]]$attributeList$attribute, \n                  ~str_detect(.x$measurementScale$ratio$unit$customUnit, \"your standard unit\"))\n                  \nwhich(ls == TRUE)"
  },
  {
    "objectID": "workflows/edit_eml/set_project.html",
    "href": "workflows/edit_eml/set_project.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The project section in an EML document is automatically filled out by the metacatUI editor. It sets the project title and project personnel to the submission’s title and creators. Most of the time at least some of this information is incorrect and we need to update it.\nStart by searching for the funding information using NSF’s award search. This will give us the project title, abstract, and personnel - along with some additional metadata.\nUsing this information we will set the title, personnel, and funding number. For NSF funded projects prepend the funding number with “NSF”. If there are multiple awards associated with one dataset then additional funding, title, and personnel elements should be added to reflect the additional awards.\ndoc$dataset$project$funding$para[[1]] <- 'NSF 1503846'\n\ndoc$dataset$project$title[[1]] <- 'Collaborative Research: Reconciling conflicting Arctic temperature and fire reconstructions using multi-proxy records from lake sediments north of the Brooks Range, Alaska\n\ndoc$dataset$project$personnel[[1]] <- eml$personnel(individualName = eml$individualName(givenName = 'Yongsong', surName = 'Huang'),\n                                                    role = 'Principal Investigator')\ndoc$dataset$project$personnel[[2]] <- eml$personnel(individualName = eml$individualName(givenName = 'James', surName = 'Russell'),\n                                                    role = 'co Principal Investigator')\nThere is also a helper function eml_nsf_to_project() that can help do the searching for you. Just verify that the information retrieved is correct.\n# update NSF awards data\nawards <- c(\"1311655\", \"1417987\", \"1417993\") # list of award numbers\nproj <- eml_nsf_to_project(awards) #helper function\n\ndoc$dataset$project <- proj\nOn the web form right now only fills in the funding field. We need instructions on how to convert that to the new awards formatting until the form is updated to include these fields:\nproject$award$funderName # required\nproject$award$title # required\nproject$award$awardNumber\nproject$award$funderIdentifier\nInformation can be found in using the Open Funder Registry. If a award title cannot be found you can use the dataset title."
  },
  {
    "objectID": "workflows/edit_eml/edit_an_eml_element.html",
    "href": "workflows/edit_eml/edit_an_eml_element.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "There are multiple ways to edit an EML element.\n\n\nThe most basic way to edit an EML element would be to navigate to the element and replace it with something else. Easy!\nFor example, to change the title one could use the following command:\n\ndoc$dataset$title <- \"New Title\"\n\nIf the element you are editing allows for multiple values, you can pass it a list of character strings. Since a dataset can have multiple titles, we can do this:\n\ndoc$dataset$title <- list(\"New Title\", \"Second New Title\")\n\nHowever, this isn’t always the best method to edit the EML, particularly if the element has sub-elements.\n\n\n\nTo edit a section where you are not 100% sure of the sub-elements, using the eml$elementName() helper functions from the EML package will pre-populate the options for you if you utilize the RStudio autocomplete functionality. The arguments in these functions show the available slots for any given EML element. For example, typing doc$dataset$abstract <- eml$abstract()<TAB> will show you that the abstract element can take either the section or para sub-elements.\n\n\n\n\ndoc$dataset$abstract <- eml$abstract(para = \"A concise but thorough description of the who, what, where, when, why, and how of a dataset.\")\n\nThis inserts the abstract with a para element in our dataset, which we know from the EML schema is valid.\nNote that the above is equivalent to the following generic construction:\n\ndoc$dataset$abstract <- list(para = \"A concise but thorough description of the who, what, where, when, why, and how of a dataset.\")\n\nThe eml() family of functions provides the sub-elements as arguments, which is extremely helpful, but functionally all it is doing is creating a named list, which you can also do using the list function.\n\n\n\nA final way to edit an EML element would be to build a new object to replace the old object. To begin, you might create an object using an eml helper function. Let’s take keywords as an example. Sometimes keyword lists in a metadata record will come from different thesauruses, which you can then add in series (similar to the way we added multiple titles) to the element keywordSet.\nWe start by creating our first set of keywords and saving it to an object.\n\nkw_list_1 <- eml$keywordSet(keywordThesaurus = \"LTER controlled vocabulary\",\n                            keyword = list(\"bacteria\", \"carnivorous plants\", \"genetics\", \"thresholds\"))\n\nWhich returns:\n$keyword\n$keyword[[1]]\n[1] \"bacteria\"\n\n$keyword[[2]]\n[1] \"carnivorous plants\"\n\n$keyword[[3]]\n[1] \"genetics\"\n\n$keyword[[4]]\n[1] \"thresholds\"\n\n\n$keywordThesaurus\n[1] \"LTER controlled vocabulary\"\nWe create the second keyword list similarly:\n\nkw_list_2 <- eml$keywordSet(keywordThesaurus = \"LTER core area\", \n                            keyword =  list(\"populations\", \"inorganic nutrients\", \"disturbance\"))\n\nFinally, we can insert our two keyword lists into our EML document just like we did with the title example above, but rather than passing character strings into list(), we will pass our two keyword set objects.\n\ndoc$dataset$keywordSet <- list(kw_list_1, kw_list_2)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you must use the function list here and not the c() function. The reasons for this are complex, and due to some technical subtlety in R - but the gist of the issue is that the c() function can behave in unexpected ways with nested lists, and frequently will collapse the nesting into a single level, resulting in invalid EML."
  },
  {
    "objectID": "workflows/edit_eml/set_coverages.html",
    "href": "workflows/edit_eml/set_coverages.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering doc$dataset$coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage().\n\ncoverage <- EML::set_coverage(beginDate = '2012-01-01', \n                              endDate = '2012-01-10', \n                              sci_names = c('exampleGenus exampleSpecies1', 'exampleGenus ExampleSpecies2'),\n                              geographicDescription = \"The geographic region covers the lake region near Eagle Mountain, Alaska.\",\n                              west = -154.6192, \n                              east = -154.5753, \n                              north = 68.3831, \n                              south = 68.3619)\ndoc$dataset$coverage <- coverage\n\n\n\nYou can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. Note that we use nested eml function helpers in this construction.\n\ngeocov1 <- eml$geographicCoverage(geographicDescription = \"The geographich region covers area 1\",\n                                  boundingCoordinates = eml$boundingCoordinates(\n                                         northBoundingCoordinate = 68,\n                                         eastBoundingCoordinate = -154,\n                                         southBoundingCoordinate = 67,\n                                         westBoundingCoordinate = -155))\n\ngeocov2 <- eml$geographicCoverage(geographicDescription = \"The geographich region covers area 2\",\n                                  boundingCoordinates = eml$boundingCoordinates(\n                                         northBoundingCoordinate = 65,\n                                         eastBoundingCoordinate = -151,\n                                         southBoundingCoordinate = 62,\n                                         westBoundingCoordinate = -153))\n\ncoverage <- EML::set_coverage(beginDate = '2012-01-01', \n                              endDate = '2012-01-10', \n                              sci_names = list('exampleGenus exampleSpecies1', 'exampleGenus ExampleSpecies2'))\n\ndoc$dataset$coverage$geographicCoverage <- list(geocov1, geocov2)\n\n\n\n\n\n\nFor arctic circle geographic coverage, we only have the starting vertical line of the circle shown in the projection. Here is an example with arctic circle geographic coverage.\n\n\n\nExample dataset with geologic coverages set using the following:\n\ngeo_time_start <- EML::eml$alternativeTimeScale(timeScaleName = \"Absolute\",\n                                                timeScaleAgeEstimate = \"7.5 Myr\")\n\ncoverage <- EML::set_coverage(beginDate = '2012-01-01', \n                              endDate = '2012-01-10', \ndoc$dataset$coverage <- coverage"
  },
  {
    "objectID": "workflows/edit_eml/set_methods.html",
    "href": "workflows/edit_eml/set_methods.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the eml helpers.\nAnother simple and potentially useful way to add methods to an EML that has no methods at all is by adding them via a MS Word document. An example is shown below:\n\nmethods1 <- set_methods('methods_doc.docx')\ndoc$dataset$methods <- methods1\n\nIf you want to make minor changes to existing methods information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing methods information.\n\n\n\n# add method steps as new variables\nstep1 <- eml$methodStep(description = \"text describing the methods used\")\n\nstEx <- eml$studyExtent(description = \"study extent description\")\n\nsamp <- eml$sampling(studyExtent = stEx,\n                     samplingDescription = \"sampling description text\")\n\n\n# combine all methods steps and sampling info \nmethods1 <- eml$methods(methodStep = step1, \n                        sampling = samp)\n\ndoc$dataset$methods <- methods1"
  },
  {
    "objectID": "workflows/explore_eml/access_specific_elements.html",
    "href": "workflows/explore_eml/access_specific_elements.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The eml_get() function is a powerful tool for exploring EML (more on that here ). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples:\n\ndoc <- read_eml(system.file(\"example-eml.xml\", package = \"arcticdatautils\"))\neml_get(doc, \"creator\")\n\nindividualName:\n  givenName: Bryce\n  surName: Mecum\norganizationName: National Center for Ecological Analysis and Synthesis\n\neml_get(doc, \"boundingCoordinates\")\n\neastBoundingCoordinate: '-134'\nnorthBoundingCoordinate: '59'\nsouthBoundingCoordinate: '57'\nwestBoundingCoordinate: '-135'\n\neml_get(doc, \"url\")\n\n'':\n  function: download\n  url: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\n'': ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\neml_get_simple() is a simplified alternative to eml_get() that produces a list of the desired EML element.\n\neml_get_simple(doc$dataset$otherEntity, \"entityName\")\n\nTo find an eml element you can use either a combination of which_in_emlfrom the arcticdatautils package or eml_get_simple and which to find the index in an EML list. Use which ever workflow you see fit.\nAn example question you may have: Which creators have a surName “Mecum”?\nExample using which_in_eml:\n\nn <- which_in_eml(doc$dataset$creator, \"surName\", \"Mecum\")\n# Answer: doc$dataset$creator[[n]]\n\nExample using eml_get_simple and which:\n\nent_names <- eml_get_simple(doc$dataset$creator, \"surName\")\ni <- which(ent_names == \"Mecum\")\n# Answer: doc$dataset$creator[[i]]"
  },
  {
    "objectID": "workflows/explore_eml/navigate_through_eml.html",
    "href": "workflows/explore_eml/navigate_through_eml.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "The first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function View allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file.\n\n\n\n\n# Need to be in this member node to explore file\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\ndoc <- read_eml(getObject(d1c_test@mn, \n                          \"urn:uuid:558eabf1-1e91-4881-8ba3-ef8684d8f6a1\"))\n\n\nView(doc)\n\n\nThe complex EML document is represented in R as as series of named, nested lists. We use lists all the time in R! A data.frame is one example of a special kind of list that we use all the time. You may be familiar with the syntax dataframe$column_name which allows us to select a particular column of a data.frame. Under the hood, a data.frame is a named list of vectors with the same length. You select one of those vectors using the $ operator, which is called the “list selector operator.”\nJust like you navigate in a data.frame, you can use the $ operator to navigate through the EML structure. The $ operator allows you to go deeper into the EML structure and to see what elements are nested within other elements. However, you have to tell R where you want to go in the structure when you use the $ symbol. For example, if you want to view the dataset element of your EML you would use the command doc$dataset. If you want to view the creators of your data set you would use doc$dataset$creator. Note here that creator is contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing $ and a list of available elements in the structure will appear (e.g., doc$<TAB>):\n\nNote that if you hit tab, and nothing pops up, this most likely implies that you are trying to go into an EML element that can take a series items. For example doc$dataset$creator$<TAB> will not show a pop-up menu. This is because creator is a series-type object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now doc$dataset$creator[[1]]$<TAB> will give you many more options. Note, an empty autocomplete result sometimes means you have reached the end of a branch in the EML structure.\nAt this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering doc$dataset into your console, and print it. Now make the search more specific, for instance: doc$dataset$abstract."
  },
  {
    "objectID": "workflows/explore_eml/understand_eml_schema.html",
    "href": "workflows/explore_eml/understand_eml_schema.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Another great resource for navigating the EML structure is looking at the schema which defines the structure. The schema diagrams on this page are interactive. Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it.\nFor example, let’s take a look at eml-party. To start off, notice that some elements have bolded lines leading to them.\n\nA bold line indicates that the element is required if the element above it (to the left in the schema) is used, otherwise the element is optional.\nNotice also that next to the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname.\nYou will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema.\nThe eml schema sections you may find particularly helpful include eml-party, eml-attribute and eml-physical.\nFor a more detailed description of the EML schema, see the reference section on exploring EML."
  },
  {
    "objectID": "workflows/misc_file_types/scan_rar_files.html",
    "href": "workflows/misc_file_types/scan_rar_files.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This section is for rar files which support data compression and file scanning. Here is how to scan rar files:\n\nDownload the rar file to datateam\nIn the terminal run the following on your files:\n\n\nmkdir to-scan && mv <file.rar> to-scan && cd to-scan\nunrar e <file.rar>\n\nThat should scan the compressed file and each individual file it contains:\n\nclamscan *\n\nClean up by deleting the to-scan folder - ’rm -r to-scan`."
  },
  {
    "objectID": "workflows/misc_file_types/datalogger_files.html",
    "href": "workflows/misc_file_types/datalogger_files.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": ".hobo files are raw data files offloaded from a sensor, and can include real-time graphs of measured variables, such as temperature and soil moisture conditions.\n.hobo files can be opened using HOBOware, which can be downloaded from here."
  },
  {
    "objectID": "workflows/misc_file_types/convert_xls_to_csv.html",
    "href": "workflows/misc_file_types/convert_xls_to_csv.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This uses the workflow in here: https://readxl.tidyverse.org/articles/articles/readxl-workflows.html\nThis is preferred to converting Excel files by hand to avoid human error.\n\n\n\n\n\n\nNote\n\n\n\nIf files are in the visitor folder you might need to be complete the following while logged into visitor\n\n\nLoad the necessary packages\n\nlibrary(magrittr)\nlibrary(readxl)\nlibrary(readr)\n\nGet a list of files that we want to convert\n\nxlsx <- dir(\"directory/of/files/here\", pattern = \"*xls\", recursive = T, full.names = T)\n\nCreate a function that iterates through the excel files and saves it as a csv in the same folder the excel file was in\n\nread_then_csv <- function(sheet, path) {\n  #remove the file extension\n  pathbase <- path %>%\n    tools::file_path_sans_ext()\n  \n  #change file extension to csv and save with the sheet name\n  for(i in 1:length(sheet)){\n    path %>%\n      read_excel(sheet = i) %>%\n      write_csv(paste0(pathbase, \"_\", sheet[i], \".csv\")) \n  }\n  \n}\n\nGet the sheet name(s)\n\nsheets <- xlsx %>%\n  map(excel_sheets) %>%\n  set_names()\n\nIterate over all the xlsx files for each of the sheets\n\ndf <- map2(sheets, xlsx, ~read_then_csv(.x, path = .y))\n\nCheck the files afterwards to make sure everything was converted properly\n\ncsv <- dir(\"directory/of/files/here\", pattern = \"*csv\", recursive = T, full.names = T)\n\nCheck to see if there are any empty tables and get a list of files that were empty\n\nfiles <- map(csv, read_csv)\nn <- which(map(files, nrow) == 0)\n\nRemove those empty table files\n\nlapply(csv[n], file.remove)\n\nRemove xlsx files if everything looks in order\n\nlapply(xlsx, file.remove)"
  },
  {
    "objectID": "workflows/misc_file_types/reorganize_files.html",
    "href": "workflows/misc_file_types/reorganize_files.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Useful commands to use when reorganizing large datasets to be prepped to be uploaded using datapack\n\n\n\n\n\n\nNote\n\n\n\nThis is run in the terminal\n\n\n\n\nMoving around\n{bash, eval = F} cd directory_name\nGo up one directory using ..\n{bash, eval = F} cd ..\nGet the contents of the current directory\n{bash, eval = F} ls\nGet the directory structure for this folder\n\n-d flag shows only the directories\n\n```{bash, eval = F} tree\ntree -d\n\n\n\n### Rename file based on file path\nFor example you have some files organized like this:\n../../visitor/Lastname └── RU_ALN_TR1_FL007R │ └── rgb_images | └──image.png └── RU_ALN_TR1_FL008B └── micasense_preflight_calibration_images └──image.png\n\nWe want to rename the files.\n\n1. Find all the files with the same file name\n\n\n```{bash, eval = F}\nfind . -name 'image.png' \n\nFirst extract the file name in parts ((.*)/(.*)/(.*)/(.*)) by using the. / as a separator.\n\n\nEach part is represented by $#. If you want the last part of the file name:$4. Rearrange this section as needed:$2/$3/$2-$3-$4\nSet the flag as -n while testing to see the result of the renaming first before committing to it using -v\n\n```{bash, eval = F} rename -n – ‘s|(.)/(.)/(.)/(.)$|$2/$3/$2-$3-$4|’\nrename -v – ‘s|(.)/(.)/(.)/(.)$|$2/$3/$2-$3-$4|’\n\n\n\n3. Putting it together by taking the results from find and renaming those files\n\n- `|` the pipe is like the `%>%` in R\n\n\n```{bash, eval = F}\nfind . -name 'image.png' | rename -v -- 's|(.*)/(.*)/(.*)/(.*)$|$2/$3/$2-$3-$4|'\n\n\n\n{bash, eval = F} unzip '*zip'\n\n\n\n\nFor all the files in a directory, create a folder and move those files\n\n{bash, eval = F} for file in *; do dir=$(echo $file | cut -d. -f1); mkdir -p $dir; mv $file $dir; done\n\nZip the folders\n\n{bash, eval = F} for file in *;do zip -r $file $file; done\n\nRemove the directories after zipping\n\n-r means to recursively remove files\n{bash, eval = F} rm -r */"
  },
  {
    "objectID": "workflows/misc_file_types/netCDF_chunk.html",
    "href": "workflows/misc_file_types/netCDF_chunk.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "This section is for dealing with NetCDF (.nc) files. These files require data tables but since they can not be simply opened on the computer using a text editor or Excel, you can use Panoply to explore them or the R commands. However, notice that not all NetCDF files have the attribute table information.\n\nlibrary(ncdf4)\n# gets attribute info\natts <- arcticdatautils::get_ncdf4_attributes('filepath')\n# preview of View(atts)\natts[1:10,]\n\n\nThe “long_name” can be used to build a definition.\nThe “units” can be used for either units or the enumerated domain codes.\nThe “_FillValue” is a missing value code it looks like.\n\n# returns the actual values for a specified attribute\nt <- ncdf4::nc_open(filepath)\ntest <- ncdf4::ncvar_get(t, 'attributeName')\n# preview of View(test)\ntest[1:10]\n\n\nThe formatId in the sysmeta will most likely be netCDF-4, but could be netCDF-3."
  },
  {
    "objectID": "workflows/misc_file_types/spatial_data.html",
    "href": "workflows/misc_file_types/spatial_data.html",
    "title": "NCEAS Data Team Training",
    "section": "",
    "text": "Shapefiles and their associated files can be kept in a single grouping using *.zip (or another similar format). To extract metadata, simply run:\n\nlibrary(sf)\n\npid <- \"pid of a zipped shapefile on the ADC\"\n\nspatial_obj <- arcticdatautils::read_zip_shapefile(adc, pid)\n\nst_geometry(spatial_obj)\n\nThis will return the shapefile as an sf style data.frame so that you can examine the attributes, and print the projection and geometry information. Note that the column geometry should not be considered an attribute for documentation purposes. This information is represented in the spatialVector$geometry element."
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction to open science",
    "section": "",
    "text": "These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. It also provides an overview on the tools we will be using (remote servers, Rstudio, R, Troubleshooting, Exercises) throughout the training. This document is meant to take multiple days to complete depending on your previous knowledge on some of the topics.\nIf you see anything that needs fixing, submit a issue in the  github issues"
  },
  {
    "objectID": "01_introduction.html#background-reading",
    "href": "01_introduction.html#background-reading",
    "title": "Introduction to open science",
    "section": "Background reading",
    "text": "Background reading\nRead the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC. We encourage you to follow the links within these pages to gain a deeper understanding.\n\nabout\nsubmission\npreservation\nhistory"
  },
  {
    "objectID": "01_introduction.html#effective-data-management",
    "href": "01_introduction.html#effective-data-management",
    "title": "Introduction to open science",
    "section": "Effective data management",
    "text": "Effective data management\nRead Matt Jones et al.’s paper on effective data management to learn how we will be organizing datasets prior to archival.\n(Please note that while the tips outlined in this article are best practices, we often do not reformat data files submitted to our repositories unless necessary. It is best to be conservative and not alter other people’s data without good reason.)\nYou may also want to explore the DataONE education resources related to data management."
  },
  {
    "objectID": "01_introduction.html#using-dataone",
    "href": "01_introduction.html#using-dataone",
    "title": "Introduction to open science",
    "section": "Using DataONE",
    "text": "Using DataONE\nData Observation Network for Earth (DataONE) is a community driven initiative that provides access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data.\nRead more about what DataONE is here and about DataONE member node (MN) guidelines here. Please feel free to ask Jeanette any questions you have about DataONE.\nWe will be applying these concepts in the next chapter."
  },
  {
    "objectID": "01_introduction.html#working-on-a-remote-server",
    "href": "01_introduction.html#working-on-a-remote-server",
    "title": "Introduction to open science",
    "section": "Working on a remote server",
    "text": "Working on a remote server\nAll of the work that we do at NCEAS is done on our remote server, datateam.nceas.ucsb.edu. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet.\nWe access RStudio on our server through this link. This is the same as your desktop version of RStudio with one main difference is that files are on the server. Please do all your work here. This way you can share your code with the rest of us.\n\n\n\n\n\n\nNote\n\n\n\nIf you R session is frozen and unresponsive check out the guide on how to fix it."
  },
  {
    "objectID": "01_introduction.html#a-note-on-paths",
    "href": "01_introduction.html#a-note-on-paths",
    "title": "Introduction to open science",
    "section": "A note on paths",
    "text": "A note on paths\nOn the servers, paths to files in your folder always start with /home/yourusername/....\nNote - if you are a more advanced user, you may use the method you prefer as long as it is evident where your file is from.\nWhen you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another data team member needs to run your script, it is not dependent on a working directory."
  },
  {
    "objectID": "01_introduction.html#a-note-on-r",
    "href": "01_introduction.html#a-note-on-r",
    "title": "Introduction to open science",
    "section": "A note on R",
    "text": "A note on R\nThis training assumes basic knowledge of R and RStudio. If you want a quick R refresher, walk through Jenny Bryan’s excellent materials here.\nThroughout this training we will occasionally use the namespace syntax package_name::function_name() when writing a function. This syntax denotes which package a function came from. For example dataone::getSystemMetadata selects the getSystemMetadata function from the dataone R package. More detailed information on namespaces can be found here."
  },
  {
    "objectID": "01_introduction.html#a-note-on-effective-troubleshooting-in-r",
    "href": "01_introduction.html#a-note-on-effective-troubleshooting-in-r",
    "title": "Introduction to open science",
    "section": "A note on effective troubleshooting in R",
    "text": "A note on effective troubleshooting in R\nWe suggest using a combination of Minimal Reproducible Examples (MRE) and the package reprex to create reproducible examples. This will allow others to better help you if we can run the code on our own computers.\nA MRE is stripping down your code to only the parts that cause the bug.\nHow to generate a reprex:\n\ncopy the code you want to ask about\ncall reprex()\nfix until everything runs smoothly\ncopy the result to ask your question\n\nWhen copy and paste code slack message or github issues, use three backticks for code blocks and two backticks for a small piece of code will prevent issues with slack formats quotation.\nFor more information and examples check out more of Jenny Bryan’s slides or watch the video starting at about the 10 min mark.\nNote for EML related MREs:\n\nGenerating a reprex for these situations (ie. tokens) might be complicated but you can should still follow the MRE principles even if the reprex won’t render fully.\nYou can include a minimal EML to avoid some get_package issues:\n\n\nme <- list(individualName = list(givenName = \"Jeanette\", surName = \"Clark\"))\n\nattributes <- data.frame(attributeName = 'length_1',\n                         attributeDefinition = 'def1',\n                         measurementScale = 'ratio',\n                         domain = 'numericDomain',\n                         unit = 'meter',\n                         numberType = 'real')\n\natt_list <- set_attributes(attributes)\n\n\ndoc_ex <- list(packageId = \"id\", system = \"system\", \n            dataset = list(title = \"A Mimimal Valid EML Dataset\",\n                           creator = me,\n                           contact = me,\n                           dataTable = list(entityName = \"data table\", \n                                            attributeList = att_list)))"
  },
  {
    "objectID": "01_introduction.html#a-note-on-exercises",
    "href": "01_introduction.html#a-note-on-exercises",
    "title": "Introduction to open science",
    "section": "A note on Exercises",
    "text": "A note on Exercises\nThe rest of the training has a series of exercises. These are meant to take you through the process as someone submitting a dataset from scratch. This is slightly different than the usual workflow but important in understanding the underlying system behind the Arctic Data Center.\nPlease note that you will be completing everything on the test site for the training. In the future if you are unsure about doing anything with a dataset, the test site is a good place to try things out!"
  },
  {
    "objectID": "01_introduction.html#exercise-1",
    "href": "01_introduction.html#exercise-1",
    "title": "Introduction to open science",
    "section": "Exercise 1",
    "text": "Exercise 1\nThis part of the exercise walks you through submitting data through the web form on the development version of our website: test.arcticdata.io\n\nPart 1\n\nDownload the csv of Table 1 from this paper.\nReformat the table to meet the guidelines outlined in the journal article on effective data management (this might be easier to do in an interactive environment like Excel).\nNote - we usually don’t edit the content in data submissions so don’t stress over this part too much\n\n\n\nPart 2\n\nGo to “test.arcticdata.io” and submit your reformatted file with appropriate metadata that you derive from the text of the paper:\n\nlist yourself as the first ‘Creator’ so your test submission can easily be found,\nfor the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine,\ndirectly copying and pasting sections from the paper (abstract, methods, etc.) is also fine,\nattributes (column names) should be defined, including correct units and missing value codes.\nsubmit the dataset"
  },
  {
    "objectID": "09_first_ticket.html",
    "href": "09_first_ticket.html",
    "title": "First Ticket",
    "section": "",
    "text": "After completing the previous chapters, Daphne or Jeanette will assign a ticket from RT. Login using your LDAP credentials got get familiarized with RT."
  },
  {
    "objectID": "09_first_ticket.html#navigate-rt",
    "href": "09_first_ticket.html#navigate-rt",
    "title": "First Ticket",
    "section": "Navigate RT",
    "text": "Navigate RT\nThe RT ticketing system is how we communicate with folks interacting with the Arctic Data Center.\nWe use it for managing submissions, accessing issues, etc. It consists of three separate interfaces:\nFront Page\nAll Tickets\nTicket Page\n\nFront page\n\nThis is what you see first\n\nHome - brings you to this homepage\n\nTickets - to search for tickets (also see number 5)\n\nTools - not needed\n\nNew Ticket - create a new ticket\n\nSearch - Type in the ticket number to quickly navigate to a ticket\n\nQueue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses\n\n\n\nNew = unopened tickets that require attention\n\nOpen = tickets currently open and under investigation and/or being processed by a support team member\n\nStalled = tickets awaiting responses from the PI/ submitter\n\n\n\nTickets I Own - These are the current open tickets that are claimed by me\n\nUnowned Tickets - Newest tickets awaiting claim\n\nTicket Status - Status and how long ago it was created\n\nTake - claim the ticket as yours\n\n\n\nAll tickets\n\nThis is the queue interface from number 6 of the Front page\n1. Ticket number and title\n2. Ticket status\n3. Owner - who has claimed the ticket\n\n\nExample ticket\n\n\nTitle - Include the PI’s name for reference\n\nDisplay - homepage of the ticket\n\nHistory - Comment/Email history, see bottom of Display page\n\nBasics - edit the title, status, and ownership here\n\nPeople - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters\n\nLinks - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number\n\n\n\n\n\n\n\nWarning\n\n\n\nVerify that this is indeed the two tickets you want to merge. It is non-reversible.\n\n\n\nActions\n\n\n\nReply - message the submitter/ PI/ all watchers\n\nComment - attach internal message (no submitters, only Data Teamers)\n\nOpen It - Open the ticket\n\nStall - submitter has not responded in greater than 1 month\n\nResolve - ticket completed\n\n\n\nHistory - message history and option to reply (to submitter and beyond) or comment (internal message)\n\n\n\nNew data submission\nWhen notified by Arcticbot about a new data submission, here are the typical steps:\n\nUpdate the Requestor under the People section based on the email given in the submission (usually the user/ PI/ submitter). You may have to google for the e-mail address if the PI did not include it in the metadata record.\nTake the ticket (Actions > Take)\nReview the submission based on the checklist\nDraft an email using the template and let others review it via Slack\nSend your reply via Actions\n\nBefore opening a R script first look over the initial checklist first to identify what you will need to update in the metadata."
  },
  {
    "objectID": "09_first_ticket.html#initial-review-checklist",
    "href": "09_first_ticket.html#initial-review-checklist",
    "title": "First Ticket",
    "section": "Initial review checklist",
    "text": "Initial review checklist\nBefore responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed.\n\nSensitive Data\nIf any of the below is in the dataset, please alert the #arctica team know before proceeding.\n\nCheck if there is any sensitive information or personal identifying information in the data (eg. Names)\nCan the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers)\nDryad Human Subject data guidelines can be a good place to start\n\nCommon Cases:\n\nSocial Science: Any dataset involving human subjects (may include awards awarded by ASSP and topics such as COVID-19)\nArchaeology: archaeological site location information, which is protected from public access by law\nBiology: protected species location coordinates\n\n\n\nData citations\n\nIf the dataset appears to be in a publication please (might be in the abstract) make sure that those citations are registered.\n\n\n\nTitle\n\nWHAT, WHERE, and WHEN:\n\nIs descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage\nProvides a location of the work from the local to state or country level\nProvides a time frame of the work\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\n\n\n\n\nAbstract\n\nDescribes the DATA as well as:\n\nThe motivation (purpose) of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nNO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out\nAt least 100 words total\n\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\nAny citations to papers can be registered with us\n\n\n\n\nKeywords\n\nSome keywords are included\n\n\n\nData\n\nData is normalized (if not suggest to convert the data if possible)\nAt least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program)\nNo xls/xlsx files (or other proprietary files)\nFile contents and relationships among files are clear\nEach file is well NAMED and DESCRIBED and clearly differentiated from all others\nAll attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. Quality control all dimensionless units.\nMissing value codes are explained (WHY are the data absent?)\nIf it is a .rar file  -> scan the file\nIf there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already\n\n\n\nPeople & Parties\n\nAt least one contact and one creator with a name, email address, and ORCID iD\n\n\n\nCoverages\n\nIncludes coverages that make sense\n\nTemporal coverage - Start date BEFORE end date\nGeologic time scales are added if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic)\nSpatial coverage matches geographic description (check hemispheres)\nGeographic description is from the local to state or country level, at the least\nTaxonomic coverage if appropriate\n\n\n\n\nProject Information\n\nAt least one FUNDING number\nTitle, personnel, and abstract match information from the AWARD (not from the data package)\n\n\n\nMethods\n\nThis section is REQUIRED for ALL NSF-FUNDED data packages\nEnough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources\n\n\n\nPortals\n\nIf there are multiple submissions from the same people/project let them know about the portals feature\nIf this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette and see the data portals section."
  },
  {
    "objectID": "09_first_ticket.html#processing-templates",
    "href": "09_first_ticket.html#processing-templates",
    "title": "First Ticket",
    "section": "Processing templates",
    "text": "Processing templates\nWe have developed some partially filled R scripts to get you started on working on your first dataset. They outline common functions used in processing a dataset. However, it will differ depending on the dataset.\nYou can use this template where you can fill in the blanks to get familiar with the functions we use and workflow at first. We also have a more minimal example A filled example as a intermediate step. You can look at the filled example if you get stuck or message the #datateam.\nOnce you have updated the dataset to your satisfaction and reviewed the Final Checklist, post the link to the dataset on #datateam for peer review."
  },
  {
    "objectID": "09_first_ticket.html#final-checklist",
    "href": "09_first_ticket.html#final-checklist",
    "title": "First Ticket",
    "section": "Final Checklist",
    "text": "Final Checklist\nYou can click on the assessment report on the website to for a general check. Fix anything you see there.\nSend the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive):\n\nSpecial Datasets\nPlease refer to the dedicated pages for instructions to handle these cases:\n\nMOSAiC\nDBO\n\n\n\nSystem Metadata\nThe format ids are correct\n\n\nGeneral EML\nIncluded lines for FAIR:\n\ndoc <- eml_add_publisher(doc)\ndoc <- eml_add_entity_system(doc)\n\n\n\nTitle\n\nNo abbreviations, should include geographic and temporal coverage\n\n\n\nAbstract\n\nlonger than 100 words\nno abbreviations or garbled text\ntags such as <superscript>2</superscript> and <subscript>2</subscript> can be used for nicer formatting\n\n\n\nDataTable / OtherEntity / SpatialVectors\n\nin the correct one: DataTable / OtherEntity / SpatialVector / SpatialRaster for the file type\nentityDescription - longer than 5 words and unique\nphysical present and format correct\n\n\nAttribute Table\n\ncomplete\nattributeDefinitions longer than 3 words\nVariables match what is in the file\nMeasurement domain - if appropirate (ie dateTime correct)\nMissing Value Code - accounted for if applicable\nSemantic Annotation - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc.\n\n\n\n\nPeople\n\ncomplete information for each person in each section\n\nincluding ORCID and e-mail address for all contacts\npeople repeated across sections should have consistent information\n\n\n\n\nGeographic region\n\nthe map looks correctand matches the geographic description\ncheck if negatives (-) are missing\n\n\n\nProject\n\nif it is an NSF award you can use the helper function:\n\ndoc$dataset$project <- eml_nsf_to_project(awards)\n\nfor other awards that need to be set manually, see the set project page\n\n\n\nMethods\n\npresent\nno garbled text\n\n\n\nCheck EML Version\n\ncurrently using: eml-2.2.0 (as of July 30 2020)\nreview to see if the EML version is set correctly by reviewing the doc$`@context` that it is indeed 2.2.0 under eml\nRe-run your code again and have the lineemld::eml_version(\"eml-2.2.0\") at the top\nMake sure the system metadata is also 2.2.0\n\n\n\nAccess\n\nGranted access to PI using set_rights_and_access()\n\nmake sure it is http:// (no s)\n\nnote if it is a part of portals there might be specific access requirements for it to be visible using set_access()\n\n\n\nSFTP Files\n\nif there are files transferred to us via SFTP, delete those files when the ticket is resolved\n\n\n\nUpdated datasets\nAll the above applies. These are some areas to do a closer check when users update with a new file:\n\nNew data was added\n\nTemporal Coverage and Title\nand follow usual protocols\n\nFiles were replaced\n\nupdate physical and entityName\ndouble-check attributes are the same\ncheck for any new missing value codes that should be accounted for\n\nWas the dataset published before 2021?\n\nupdate project info , annotations\n\nGlance over entire page for any small mistakes (ie. repeated additionalMetadata, any missed &amps, typos)\n\nAfter all the revisions send the link to the PI in an email through RT. Send the draft of the email to Daphne or Jeanette on Slack."
  },
  {
    "objectID": "09_first_ticket.html#email-templates",
    "href": "09_first_ticket.html#email-templates",
    "title": "First Ticket",
    "section": "Email templates",
    "text": "Email templates\nThis section covers new data packages submitted. For other inquiries see the PI FAQ templates\nPlease think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible.\nIn your first few months, please run email drafts by the #datateam Slack and get approval before sending.\nRemember to consult the submission guidelines for details of what is expected.\nQuick reference:\n\nInitial email template\nFinal email templates\nAdditional email template\n\n\n\n\nInitial email template\n\nHello [NAME OF REQUESTOR], Thank you for your recent submission to the NSF Arctic Data Center!\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n[COMMENTS HERE]\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\nBest,\n[YOUR NAME]\n\n\n\nComment templates based on what is missing\n\nPortals\nMultiple datasets under the same project - suggest data portal feature\n\nI would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project.\n\nIf they ask to nest the dataset\n\nWe are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email.\n\n\n\nDataset citations\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point.\n\n\n\nTitle\nProvides the what, where, and when of the data\n\nWe would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’.\n\nDoes not use acronyms\n\nWe wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE]\n\n\n\nAbstract\nDescribes DATA in package (ideally > 100 words)\n\nWe would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING]\n\n\nThe motivation of the study\nWhere and when the research took place\nAt least one sentence summarizing general methodologies\nAll acronyms are defined\nAt least 100 words long\n\nOffer this if submitter is reluctant to change:\n\nIf you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL].\n\n\n\nKeywords\n\nWe noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others.\n\n\n\nData\nSensitive Data\nWe will need to ask these questions manually until the fields are added to the webform.\n\nData sensitivity categories\n\nOnce we have the ontology this question can be asked:\n\nBased on our Data sensitivity categories, which of the 3 does your dataset align with most:\n\n\nNon-sensitive data - None of the data includes sensitive or protected information.\nSome or all data is sensitive with minimal risk - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized.\nSome or all data is sensitive with significant risk - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body.\n\n\nEthical research proceedures\n\n\nWe were wondering if you could also address this question specifically on Ethical Research Procedures: Describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. We can help add your answers to the question to the metadata.\n\nAdding provenance\n\nIs the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2\n\nAt least one data file\n\nWe noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process.\n\nOpen formats\nExample using xlsx. Tailor this reponse to the format in question.\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nThe data files can be replaced by going to the green Edit button > Click the black triangle by the Describe button for the data file > Select Replace (attached is also a screenshot on how to get there). \n\nZip files\n\nExcept for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way.\n\nFile contents and relationships among files are clear\n\nCould you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use.\n\nData layout\n\nWould you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent?\n\nWe try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat.\n\nWe would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\n\nAttributes\nIdentify which attributes need additional information. If they are common attributes like date and time we do not need further clarification.\nChecklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets):\n\nA name (often the column or row header in the file).\nA complete definition.\nAny missing value codes along with explanations for those codes.\nFor all numeric data, unit information is needed.\nFor all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”).\nFor text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes.\n\nHelpful templates: > We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] > Could you describe ____? > Please define “XYZ”, including the unit of measure. > What are the units of measurement for the columns labeled “ABC” and “XYZ”?\nMissing value codes\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf.\n\n\nWe noticed that the data files contain [blank cells - replace with missing values found]. What do these represent?\n\n\n\nFunding\nAll NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization.\n\nWe noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations.\n\n\n\nMethods\nWe noticed that methods were missing from the submission. Submissions should include the following:\n\nprovide instrument names (if applicable)\nspecify how sampling locations were chosen\nif citations for sampling methods are used, please provide a brief summary of the methods referenced\nany software used to process the data\n\nNote - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software)\n\nYour methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.]\n\n\nComprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable.\n\nA full example - New Submission: methods, excel to csv, and attributes\n\nThank you for your recent submission to the NSF Arctic Data Center!\n\n\nFrom my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below:\n\n\nIf there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point.\n\n\nWe would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records.\n\n\nWe noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term.\n\n\nWe also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. \n\n\nOur data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help.\n\n\nWhat do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice.\n\n\nWe noticed that methods were missing from the submission. Submissions should: - provide instrument names (if applicable) - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail - any software used to process the data\n\n\nAfter we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface.\n\n\nBest,\n\n\nName\n\n\n\n\nFinal email templates\n\nAsking for approval\n\nHi [submitter],\n\n\nI have updated your data package and you can view it here after logging in: [URL]\n\n\nPlease review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI.\n\n\nAfter publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version.\n\n\nPlease let us know if you have any questions.\n\n\n\nDOI and data package finalization comments\nReplying to questions about DOIs\n\nWe attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version.\n\n\nDOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists.\n\nClarification of updating with a DOI and version control\n\nWe definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it).\n\n\n\nResolve the ticket\nSending finalized URL and dataset citation before resolving ticket\n[NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)]\n\nHere is the link and citation to your finalized data package:\n\n\nhttps://doi.org/10.18739/A20X0X\n\n\nFirst Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X.\n\n\nIf in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\nPlease let us know if you need any further assistance.\n\n\n\n\nAdditional email templates\n\nDeadlines\nIf the PI is checking about dates/timing: > [give rough estimate of time it might take] > Are you facing any deadlines? If so, we may be able to expedite publication of your submission.\n\n\nPre-assigned DOI\nIf the PI needs a DOI right away:\n\nWe can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center.\n\n\n\nSensitive Data\n\nWhich of the following categories best describes the level of sensitivity of your data?\n\n\nA. Non-sensitive data None of the data includes sensitive or protected information. Proceed with uploading data. B. Some or all data is sensitive but has been made safe for open distribution Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Proceed with uploading data, but ensure that only data that are safe for public distribution are uploaded. Address questions about anonymization, aggregation, de-identification, and data embargoes with the data curation support team before uploading data. Describe these approaches in the Methods section. C. Some or all data is sensitive and should not be distributed The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Do NOT upload sensitive data. You should still upload a metadata description of your dataset that omits all sensitive information to inform the community of the dataset’s existence. Contact the data curation support team about possible alternative approaches to safely preserve sensitive or protected data.\n\n\n\nEthical Research Procedures. Please describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Institutional Review Board approvals, consent waivers, procedures for co-production, data sovereignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution.\n\n\n\n\nAsking for dataset access\n\nAs a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets.\n\n\n\nNo response from the researcher\nPlease email them before resolving a ticket like this:\n\nWe are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email.\n\n\n\nRecovering Dataset submissions\n\nTo recover dataset submissions that were not successful please do the following:\n\n\n\nGo to https://arcticdata.io/catalog/drafts\nFind your dataset and download the corresponding file\nSend us the file in an email\n\n\n\n\nCustom Search Link\n\nYou could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=“your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER\n\n\n\nAdding metadata via R\n\nKNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R.\n\n\nHere are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")\n\n\nIf you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well.\n\n\n\nFinding multiple data packages\n\nIf linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533\n\n\n\nNSF ARC data submission policy\n\nPlease find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp.\n\n\nInvestigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used.\n\n\n\nLinking ORCiD and LDAP accounts\n\nFirst create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage.\n\n\nAfter completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface.\n\n\nOr, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage.\n\nOnce the dataset is approved by the PI and there are no further changes, publish the dataset with a doi.\n\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement = eml_path, newId = doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public = TRUE, quiet = FALSE)"
  },
  {
    "objectID": "09_first_ticket.html#categorize-datasets",
    "href": "09_first_ticket.html#categorize-datasets",
    "title": "First Ticket",
    "section": "Categorize datasets",
    "text": "Categorize datasets\nAs a final step we will categorize the dataset you processed. We are trying to categorize datasets so we can have a rough idea of what kinds of datasets we have at the Arctic Data Center. We will grant you access to the google sheet that has all of the categorized datasets\nWe will categorize each dataset into one of the predefined themes (ie. biology, ecology etc.). Definition of the themes can be found in the google sheet\nRun the following line with your doi and themes as a list.\n\ndatamgmt::categorize_dataset(\"your_doi\", c(\"list\", \"of\", \"themes\"), \"your name\")"
  },
  {
    "objectID": "09_first_ticket.html#congrats",
    "href": "09_first_ticket.html#congrats",
    "title": "First Ticket",
    "section": "Congrats!",
    "text": "Congrats!\nCongratulations on finishing your first ticket! You can head over to the the repository, data-processing to get your ticket processing code reviewed by the team so we can learn from each other!"
  },
  {
    "objectID": "07_building_provenance.html",
    "href": "07_building_provenance.html",
    "title": "Building provenance",
    "section": "",
    "text": "Note - It is rare for a dataset to have provenance - though we would like to see that change by encouraging researchers to submit scripts whenever it is reasonable. When processing datasets if you notice that provenance is needed let Daphne or Jeanette know.\nThe provenance chain describes the origin and processing history of data. Provenance (or “prov”) can exist on a continuum, ranging from prose descriptions of the history, to formal provenance traces, to fully executable environments. In this section we will describe how to build provenance using formal provenance traces in DataONE.\nProvenance is becoming increasingly important in the face of what is being called a reproducibility crisis in science. J. P. A. Ioannidis (2005) wrote that “Most Research Findings Are False for Most Research Designs and for Most Fields”. Ioannidis outlined ways in which the research process has lead to inflated effect sizes and hypothesis tests that codify existing biases.\nThe first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available.\nAnd yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)).\nAt NCEAS and in the datateam, not only do we archive data and code openly, but we also describe the workflows that involve that data and code using provenance, formalizing the provenance trace for a workflow that might look like this  into an easily understandable trace including archived data objects, such as what is shown here.\nThere are two ways that we add provenance in the datateam - the prov editor and the R datapack package."
  },
  {
    "objectID": "07_building_provenance.html#using-the-prov-editor",
    "href": "07_building_provenance.html#using-the-prov-editor",
    "title": "Building provenance",
    "section": "Using the prov editor",
    "text": "Using the prov editor\nProvenance can easily be added to production Arctic Data Center packages using the provenance editor on the Arctic Data Center. On the landing page of a data package within beta, in the dataTable or otherEntity section where you would like to add a provenance relationship, you can choose to add either a “source” or a “derivation”, to the left or right of the object pane, respectively.\n\nTo add a source data file, click on the circle with the “+ add” text. Similarly, a source script would be added by selecting the arrow. Selecting the circle to add a source file pulls up the following screen, where you can select the source from other data objects within the same data package. \nA data package with an object that has multiple sources added will look like this. \nFor simple packages on the Arctic Data Center, adding prov through the prov editor is super easy!"
  },
  {
    "objectID": "07_building_provenance.html#understanding-resource-maps",
    "href": "07_building_provenance.html#understanding-resource-maps",
    "title": "Building provenance",
    "section": "Understanding resource maps",
    "text": "Understanding resource maps\nBefore we dive further into constructing prov in R, we need to talk more about resource maps (or “resmaps”).\nAll data packages have a single resource map. But what is a resource map and how do we use one to find out what objects are in a particular data package? This document is a short introduction but a more complete guide can be found here.\nA resource map is a special kind of XML document that describes (among other things) an “aggregation”. The aggregation describes the members of a data package (metadata and data, usually). We can use the dataone R package to download a resource map if we know its PID:\n\nlibrary(dataone)\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\npid <- \"urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556\" # A resource map PID\n\npath <- tempfile(fileext = \".xml\") # We're saving to a temporary file but you can save elsewhere\nwriteLines(rawToChar(getObject(d1c_test@mn, pid)), path) # Write the object to `path`\n\nIf we open that file up in a text editor, we see this:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<rdf:RDF xmlns:cito=\"http://purl.org/spar/cito/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dcterms=\"http://purl.org/dc/terms/\" xmlns:foaf=\"http://xmlns.com/foaf/0.1/\" xmlns:ore=\"http://www.openarchives.org/ore/terms/\" xmlns:prov=\"http://www.w3.org/ns/prov#\" xmlns:provone=\"http://purl.dataone.org/provone/2015/01/15/ontology#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\">\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27\">\n    <cito:isDocumentedBy rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\">\n    <rdf:type rdf:resource=\"http://www.openarchives.org/ore/terms/Aggregation\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\">\n    <cito:isDocumentedBy rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\">\n    <cito:documents rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\">\n    <cito:documents rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27\">\n    <ore:isAggregatedBy rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\">\n    <dc:title>DataONE Aggregation</dc:title>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556\">\n    <dcterms:identifier rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556</dcterms:identifier>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556\">\n    <ore:describes rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\">\n    <ore:isAggregatedBy rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\">\n    <ore:aggregates rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\">\n    <ore:aggregates rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556\">\n    <rdf:type rdf:resource=\"http://www.openarchives.org/ore/terms/ResourceMap\"/>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27\">\n    <dcterms:identifier rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27</dcterms:identifier>\n  </rdf:Description>\n  <rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\">\n    <dcterms:identifier rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5</dcterms:identifier>\n  </rdf:Description>\n</rdf:RDF>\nWhoa! What is this thing and how do you read it to find the members of the data package? The short answer is to look for lines like this:\n<rdf:Description rdf:about=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation\">\n    <ore:aggregates rdf:resource=\"https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5\"/>\nThis line says “The aggregation aggregates urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5” so that means urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 is in our data package! The key bit is the <rdf:Description rdf:about=\"...#aggregation part. If you look for another similar statement, you’ll also see that urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 is part of our data package.\nNow we know which objects are in our data package but we don’t know which one contains metadata and which one contains data. For that, we need to get a copy of the system metadata for each object:\n\ngetSystemMetadata(d1c_test@mn, \"urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5\")@formatId\ngetSystemMetadata(d1c_test@mn, \"urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27\")@formatId\n\nFrom the formatIds, we can see the first PID is the EML (formatId: [1]\"eml://ecoinformatics.org/eml-2.1.1\") and the second PID is a data object (formatId: [1]\"application/octet-stream\"). Now we know enough to know what’s in the data package:\n\nResource map: urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556\nMetadata: urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5\nData: urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27\n\nNow that you’ve actually seen a resource map, we can dive further into prov."
  },
  {
    "objectID": "07_building_provenance.html#using-the-datapack-package",
    "href": "07_building_provenance.html#using-the-datapack-package",
    "title": "Building provenance",
    "section": "Using the datapack package",
    "text": "Using the datapack package\nFor packages not on the ADC, or packages that are extremely complicated, it may be best to upload prov relationships using R. The datapack package has several functions which help add relationships in a very simple way. These relationships are stored in the resource map. When you update a package to only add prov, the package will not be assigned any new identifiers with the exception of the resource map.\nFirst, we set the environment, in a similar, but slightly different way than what you may be used to. Here the function D1Client() sets the DataONE client with the coordinating node instance as the first argument, and member node as the second argument.\n\nlibrary(dataone)\nlibrary(datapack)\nd1c <- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\")\n\nNext, get the PID of the resource map of the data package you are adding prov to, and load that package into R using the getDataPackage() function.\n\npackageId <- \"urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b\"\ndp <- getDataPackage(d1c, id=packageId, lazyLoad=TRUE, limit=\"0MB\", quiet=FALSE)\n\nPrinting pkg in your console shows you the contents of the data package, including all of the objects and their names:\n> pkg\nMembers:\n\nfilename   format         mediaType  size     identifier                                    modified local \nesc...er.R application/R  NA         888      knb.92049.1                                   n        n     \nPWS....csv text/csv       NA         1871469  knb.92050.1                                   n        n     \nPWS....csv text/csv       NA         1508128  knb.92051.1                                   n        n     \nNA         eml:/...-2.1.1 NA         15658    urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b n        y     \n\nPackage identifier: resource_map_urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b\nRightsHolder: http://orcid.org/0000-0002-2192-403X\nIt will also show the existing relationships in the resource map, which in this case are mostly the “documents” relationships that specify that the metadata record is describing all of these data files.\nRelationships:\n                                   subject           predicate                                   object\n2          esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b\n4                        PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b\n1                PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b\n3 urn:uuid:8f501606-...4-b22d-050a4176a97b     dcterms:creator                    _r1515542097r415842r1\n5 urn:uuid:8f501606-...4-b22d-050a4176a97b      cito:documents          esc_reformatting_PWSweirTower.R\n6 urn:uuid:8f501606-...4-b22d-050a4176a97b      cito:documents                        PWS_weirTower.csv\n7 urn:uuid:8f501606-...4-b22d-050a4176a97b      cito:documents                PWS_Weir_Tower_export.csv\n8 urn:uuid:8f501606-...4-b22d-050a4176a97b      cito:documents urn:uuid:8f501606-...4-b22d-050a4176a97b\n9 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b\nIn this example above, the data package has two .csv files, with an R script that converts one to the other. To create our provenance trace, first we need to select the source object, and save the PID to a variable. We do this using the selectMember() function, and we can query part of the system metadata to select the file that we want. This function takes the data package (pkg), the name of the sysmeta field to query (in this case we use the fileName), and the value that you want to match that field to (in this case, ‘PWS_Weir_Tower_export.csv’).\n\nsourceObjId <- selectMember(dp, name=\"sysmeta@fileName\", value='PWS_Weir_Tower_export.csv')\n\nThis returns a list of the source object PIDs that match the query (in this case only one object matches).\n> sourceObjId\n[1] \"knb.92051.1\"\nNow we need to select our output object. Here, we use the selectMember() function again, and save the result to a new variable.\n\noutputObjId <- selectMember(dp, name=\"sysmeta@fileName\", value='PWS_weirTower.csv')\n\nNow we query for the R script. In this case, we query based on the value of the formatId as opposed to the fileName. This can be useful if you wish to select a large list of PIDs that are all similar.\n\nprogramObjId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"application/R\")\n\nNext, you use these lists of PIDs and a function called describeWorkflow() to add these relationships to the data package. Note that if you do not have a program in the workflow, or a source file, you can simply leave those arguments blank.\n\ndp <- describeWorkflow(dp, sources=sourceObjId, program=programObjId, derivations=outputObjId)\n\nViewing pkg again confirms that these relationships have been inserted into the data package, as shown by the wasDerivedFrom and wasGeneratedBy statements. It is always a good idea to print pkg to confirm that your PID selection process worked as expected, and your prov relationships make sense.\nRelationships (updated):\n\n                                subject                 predicate                               object\n15 _1db49d06-ae98-4...9101-39f7c0b45a95                  rdf:type                     prov:Association\n14 _1db49d06-ae98-4...9101-39f7c0b45a95              prov:hadPlan      esc_reformatting_PWSweirTower.R\n1       esc_reformatting_PWSweirTower.R       cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b\n16      esc_reformatting_PWSweirTower.R                  rdf:type                      provone:Program\n8                     PWS_weirTower.csv       cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b\n11                    PWS_weirTower.csv                  rdf:type                         provone:Data\n20                    PWS_weirTower.csv       prov:wasDerivedFrom            PWS_Weir_Tower_export.csv\n19                    PWS_weirTower.csv       prov:wasGeneratedBy urn:uuid:3dd59b0...bc38-3b5d8fa644ac\n6             PWS_Weir_Tower_export.csv       cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b\n10            PWS_Weir_Tower_export.csv                  rdf:type                         provone:Data\n9                 _r1515544826r415842r1                 foaf:name                     DataONE R Client\n17 urn:uuid:3dd59b0...bc38-3b5d8fa644ac        dcterms:identifier urn:uuid:3dd59b0...bc38-3b5d8fa644ac\n13 urn:uuid:3dd59b0...bc38-3b5d8fa644ac                  rdf:type                    provone:Execution\n12 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:qualifiedAssociation _1db49d06-ae98-4...9101-39f7c0b45a95\n18 urn:uuid:3dd59b0...bc38-3b5d8fa644ac                 prov:used            PWS_Weir_Tower_export.csv\n5  urn:uuid:8f50160...b22d-050a4176a97b            cito:documents      esc_reformatting_PWSweirTower.R\n4  urn:uuid:8f50160...b22d-050a4176a97b            cito:documents                    PWS_weirTower.csv\n3  urn:uuid:8f50160...b22d-050a4176a97b            cito:documents            PWS_Weir_Tower_export.csv\n2  urn:uuid:8f50160...b22d-050a4176a97b            cito:documents urn:uuid:8f50160...b22d-050a4176a97b\n7  urn:uuid:8f50160...b22d-050a4176a97b       cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b\nFinally, you can upload the data package using the uploadDataPackage() function, which takes the DataONE client d1c we set in the beginning, the updated pkg variable, some options for public read, and whether or not informational messages are printed during the upload process. First, you will need to run a token obtained from https://dev.nceas.ucsb.edu/ to publish to that node.\n\nresmapId_new <- uploadDataPackage(d1c, dp, public = TRUE, quiet = FALSE)\n\nIf successful you should be able to navigate to the landing page of your dataset, and icons should show up where the sources and derivations are, such as in this example: \n\nFixing mistakes\nIf you messed up updating a data package using datapack, there unfortunately isn’t a great way to undo your work, as the describeWorkflow() only adds prov relationships, it does not replace them. If you messed up, the best course of action is to update the resource map with a clean version that does not have prov using update_resource_map(), and then go through the steps outlined above again.\nNote: this has not been thoroughly tested, and more extreme actions may be necessary to fully nuke the prov relationships. Consult Peter or Jeanette if things do not work as expected."
  },
  {
    "objectID": "07_building_provenance.html#references",
    "href": "07_building_provenance.html#references",
    "title": "Building provenance",
    "section": "References",
    "text": "References\nIoannidis, John P A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\nHampton, Stephanie E, Sean Anderson, Sarah C Bagby, Corinna Gries, Xueying Han, Edmund Hart, Matthew B Jones, et al. 2015. “The Tao of Open Science for Ecology.” Ecosphere 6 (July). https://doi.org/10.1890/ES14-00402.1.\nMunafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021."
  },
  {
    "objectID": "10_advanced.html",
    "href": "10_advanced.html",
    "title": "Beyond your first ticket",
    "section": "",
    "text": "This section is meant to be read after you have processed a couple of tickets and you are comfortable with the workflow with a relatively simple dataset with 1-2 files and want to expand your skills and workflows further."
  },
  {
    "objectID": "10_advanced.html#working-with-large-data-packages",
    "href": "10_advanced.html#working-with-large-data-packages",
    "title": "Beyond your first ticket",
    "section": "Working with large data packages",
    "text": "Working with large data packages"
  },
  {
    "objectID": "10_advanced.html#add-physicals-to-submissions",
    "href": "10_advanced.html#add-physicals-to-submissions",
    "title": "Beyond your first ticket",
    "section": "Add physicals to submissions",
    "text": "Add physicals to submissions\nNew submissions made through the web editor will not have any physical sections within the otherEntitys. Add them to the EML with the following script:\n\nfor (i in seq_along(doc$dataset$otherEntity)) {\n    otherEntity <- doc$dataset$otherEntity[[i]]\n    id <- otherEntity$id\n    \n    if (!grepl(\"urn-uuid-\", id)) {\n        warning(\"otherEntity \", i, \" is not a pid\")\n        \n    } else {\n        id <- gsub(\"urn-uuid-\", \"urn:uuid:\", id)\n        physical <- arcticdatautils::pid_to_eml_physical(mn, id)\n        doc$dataset$otherEntity[[i]]$physical <- physical\n    }\n}\n\nAs you can see from code above, we use a for loop here to add physical sections. The for loop is a very useful tool to iterate over a list of elements. With for loop, you can repeat a specific block of code without copying and pasting the code over and over again. When processing datasets in Arctic Data Center, there are many places where for loop can be used, such as publishing a bunch of objects with pids, updating formatID for pkg$data, adding physical section like above code, etc.\nA loop is composed of two parts: the sequence and the body. The sequence usually generates indices to locate elements and the body contains the code that you want to iterate for each element.\nHere is an example of adding the same attributeList for all the dataTables in the metadata using for loop. ::: {.cell}\nattributes <- read.csv('attributes.csv')  # attribute table in csv format\nattributeList <- EML::set_attributes(attributes = attributes)\n\nfor (i in 1:length(doc$dataset$dataTable)) { # sequence part\n    doc$dataset$dataTable[[i]]$attributeList <- attributeList # body part\n}\n:::"
  },
  {
    "objectID": "10_advanced.html#use-references",
    "href": "10_advanced.html#use-references",
    "title": "Beyond your first ticket",
    "section": "Use references",
    "text": "Use references\nReferences are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including:\n\nMaking it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes)\nReducing the size on disk of EML records with highly redundant information\nFaster read/write/validate with the R EML package\n\nYou may want to use EML references if you have the following scenarios (not exhaustive):\n\nOne person has multiple roles in the dataset (creator, contact, etc)\nOne or more entities shares all or some attributes\n\n\nExample with parties\n\n\n\n\n\n\nNote\n\n\n\nDo not use references for creators as it is used for the citation information. The creators will not show up on the top of the dataset if it is a reference. Until this issue is resolved in NCEAS/metacat#926 we will need to keep this in account.\n\n\nIt’s very common to see the contact and creator referring to the same person with XML like this:\n<eml packageId=\"my_test_doc\" system=\"my_system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1 eml.xsd\">\n  <dataset>\n    <creator>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </contact>\n  </dataset>\n</eml>\nSo you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt.\n\ndoc$dataset$creator[[1]]$id  <- \"reference_id\"\ndoc$dataset$contact <- list(references = \"reference_id\") \nprint(doc)\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<eml:eml xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stmml=\"http://www.xml-cml.org/schema/stmml-1.1\" packageId=\"id\" system=\"system\" xsi:schemaLocation=\"eml://ecoinformatics.org/eml-2.1.1/ eml.xsd\">\n  <dataset>\n    <title>A Minimal Valid EML Dataset</title>\n    <creator id=\"reference_id\">\n      <individualName>\n        <givenName>Bryce</givenName>\n        <surName>Mecum</surName>\n      </individualName>\n    </creator>\n    <contact>\n      <references>reference_id</references>\n    </contact>\n  </dataset>\n</eml:eml>\n\n\n\n\n\n\nNote\n\n\n\nThe reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that.\n\n\n\n\nExample with attributes\nTo use references with attributes:\n\nAdd an attribute list to a data table\nAdd a reference id for that attribute list\nUse references to add that information into the attributeLists of the other data tables\n\nFor example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest:\n\ndoc$dataset$dataTable[[1]]$attributeList <- attribute_list\ndoc$dataset$dataTable[[1]]$attributeList$id <- \"shared_attributes\" # use any unique name for your id\n\nfor (i in 2:length(doc$dataset$dataTable)) {\n  doc$dataset$dataTable[[i]]$attributeList <- list(references = \"shared_attributes\") # use the id you set above\n}"
  },
  {
    "objectID": "10_advanced.html#annotations",
    "href": "10_advanced.html#annotations",
    "title": "Beyond your first ticket",
    "section": "Annotations",
    "text": "Annotations\nIf there are multiple tables with similar annotations you can try something like this:\n\n#go through each dataTable\nfor(i in 1:length(doc$dataset$dataTable)){\n  \n  #get all the attibute names\n  an <- eml_get_simple(doc$dataset$dataTable[[i]], \"attributeName\")\n  \n  #go through the attributes figure out what will match\n  for(a in 1:length(an)){\n    annotation <- dplyr::case_when(\n      # the attributeName to match ~ valueLabel\n      an[[a]] == \"Sample ID\" ~ \"Identity\",\n      an[[a]] == \"Location\" ~ \"study location name\",\n      an[[a]] == \"Latitude\" ~ \"latitude coordinate\",\n      an[[a]] == \"Longitude\" ~ \"longitude coordinate\",\n      an[[a]] == \"Elevation (m)\" ~ \"elevation\",\n      str_detect(an[[a]], \"Depth|depth\") ~ \"Depth\")\n    \n    #only run this code when the annotations match\n    if(!is.na(annotation)){\n      #based on the entity Name create a unique id\n      entity <- str_split(doc$dataset$dataTable[[i]]$entityName, \"_\")\n      doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$id <- paste0(entity[[1]][[1]], \"_\", an[[a]])\n      \n      #add the annotation\n      doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$annotation <- eml_ecso_annotation(annotation)\n    }\n  }\n  \n}"
  },
  {
    "objectID": "10_advanced.html#streamlining-your-workflow",
    "href": "10_advanced.html#streamlining-your-workflow",
    "title": "Beyond your first ticket",
    "section": "Streamlining your workflow",
    "text": "Streamlining your workflow"
  },
  {
    "objectID": "10_advanced.html#code-snippets",
    "href": "10_advanced.html#code-snippets",
    "title": "Beyond your first ticket",
    "section": "Code Snippets",
    "text": "Code Snippets\nCode snippets help with templating portions of code that you will be using regularly. To add your own, go to the toolbar ribbon at the top of your Rstudio screen and select:\nTools > Global Options... > Code > Edit Snippets > Add these chunks to the end of the file\nMore info can be found in this blog post by Mara Averick on how to add them: https://maraaverick.rbind.io/2017/09/custom-snippets-in-rstudio-faster-tweet-chunks-for-all/\nUsual arcticdatautils ticket workflow ::: {.cell}\nsnippet ticket\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    rm <- \"add your rm\"\n    pkg <- get_package(adc, rm)\n    doc <- EML::read_eml(getObject(adc, pkg\\$metadata))\n    \n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    eml_validate(doc)\n    eml_path <- \"eml.xml\"   \n    write_eml(doc, eml_path)\n\n    #update <- publish_update(adc,\n    #                                               metadata_pid = pkg\\$metadata,\n    #                                               resource_map_pid = pkg\\$resource_map,\n    #                                               metadata_path = eml_path,\n    #                                               data_pids = pkg\\$data,\n    #                                               public = F)\n                                                    \n    #datamgmt::categorize_dataset(update\\$metadata, c(\"theme1\"), \"Your Name\")\n:::\nThe datapack ticket workflow ::: {.cell}\nsnippet datapack\n    library(dataone)\n    library(datapack)\n    library(digest)\n    library(uuid)\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n    \n    d1c <- D1Client(\"PROD\", \"urn:node:ARCTIC\")\n    packageId <- \"id here\"\n    dp <- getDataPackage(d1c, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\n    \n    #get metadata id\n    metadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n    \n    #edit the metadata\n    doc <- read_eml(getObject(d1c@mn, metadataId))\n    \n    #add the publisher info\n    doc <- eml_add_publisher(doc)\n    doc <- eml_add_entity_system(doc)\n    \n    doc\\$dataset\\$project <- eml_nsf_to_project(\"nsf id here\")\n    \n    #check and save the metadata\n    eml_validate(doc)\n    eml_path <- arcticdatautils::title_to_file_name(doc\\$dataset\\$title)\n    write_eml(doc, eml_path)\n    \n    dp <- replaceMember(dp, metadataId, replacement=eml_path)\n    \n    #upload the dataset\n    myAccessRules <- data.frame(subject=\"CN=arctic-data-admins,DC=dataone,DC=org\", permission=\"changePermission\") \n    packageId <- uploadDataPackage(d1c, dp, public=F, accessRules=myAccessRules, quiet=FALSE)\n    #datamgmt::categorize_dataset(\"doi\", c(\"theme1\"), \"Jasmine\")\n:::\nQuick way to give access to submitters to their datasets: ::: {.cell}\nsnippet access\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n\n    rm <- \"rm here\"\n    pkg <- get_package(adc, rm)\n\n    set_access(adc, unlist(pkg), \"orcid here\")\n:::\nQuick access to the usual code for common Solr queries: ::: {.cell}\nsnippet solr\n    library(dataone)\n    library(arcticdatautils)\n    library(EML)\n\n    cn <- CNode('PROD')\n    adc <- getMNode(cn, 'urn:node:ARCTIC')\n    \n    result <- query(adc, list(q = \"rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)\",\n                              fl = \"identifier,rightsHolder,formatId, fileName, dateUploaded\",\n                              sort = 'dateUploaded+desc',\n                              start =\"0\",\n                              rows = \"1500\"),\n                         as=\"data.frame\")\n:::"
  },
  {
    "objectID": "10_advanced.html#resources-for-r",
    "href": "10_advanced.html#resources-for-r",
    "title": "Beyond your first ticket",
    "section": "Resources for R",
    "text": "Resources for R\n\nLearning\nThe following online books are useful for expanding your R knowledge and skills:\n\nthe most recent ADC training materials\n\nThe cleaning and data manipulation section is useful for working with attribute tables\n\nEfficient R Programming\n\nIn particular Chapter 3 Efficient Programming\n\nR for Data Science\n\nSection on Strings\n\nR Packages\n\ncontributing to arcticdatatutils, datamgmt and EML\n\nAdvanced R\n\nObject-oriented programming in R for S4 to understand how datapack and dataone packages are written\n\nbookdown: Authoring Books and Technical Documents with R Markdown\nformatting, troubleshooting and updating the training document\n\nOthers\n\nHands-On Programming with R\nR Programming for Data Science\nExploratory Data Analysis with R\nMastering Software Development in R\nGeocomputation with R\nR Markdown: The Definitive Guide\nThe Tidyverse Style Guide\n\nThe RStudio cheatsheets are also useful references for functions in tidyverse and other packages.\n\n\nPackages\nThe data team uses and develops a number of R packages. Here is a listing and description of the main packages:\n\ndataone\n\nreading and writing data at DataONE member nodes\nhttp://doi.org/10.5063/F1M61H5X\n\ndatapack\n\ncreating and managing data packages\nhttps://github.com/ropensci/datapack\n\nEML\n\ncreating and editing EML metadata documents\nhttps://ropensci.github.io/EML\n\narcticdatautils\n\nutility functions for processing data for the Arctic Data Center\nhttps://nceas.github.io/arcticdatautils/\n\ndatamgmt\n\ndata management utilities for curating, documenting, and publishing data (sandbox package)\nhttps://nceas.github.io/datamgmt/\n\nmetadig\n\nauthoring MetaDIG quality checks\nhttps://github.com/NCEAS/metadig-r\n\nmetajam\n\ndownloading and reading data and metadata from DataONE member nodes\nhttps://nceas.github.io/metajam/"
  },
  {
    "objectID": "08_using_git.html",
    "href": "08_using_git.html",
    "title": "Using Git",
    "section": "",
    "text": "We use git and Github to manage our packages (i.e. datamgmt, arcticdatautils) and even this document!"
  },
  {
    "objectID": "08_using_git.html#what-is-git",
    "href": "08_using_git.html#what-is-git",
    "title": "Using Git",
    "section": "What is Git?",
    "text": "What is Git?\nGit is a distributed version control system.\nImportant! If you have never used Git before, or only used it a little, or have no idea what it is, check out this Intro to Git put together by the ecodatascience group at UCSB. Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. After you have read through that presentation, come back to this chapter.\n\nSo why do I need to use this again?\nThere are several reasons why using the arctic-data GitHub repository is helpful, both for you and for the rest of the data team. Here are a few:\n\nVersioning: Did you accidentally make a change to your code and can’t figure out why it broke? Do you wish you could go back to the version that worked? If you add your code to the GitHub repo you can do this!\nReproducibility: Being able to reproduce how you accomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. As a data center, this is especially important for us with data team members that stay for 6-12 months because we may need to go back and figure out how something was done after the intern or fellow who wrote the code left the team.\nTroubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for others to help you troubleshoot your code if it is on the GitHub repo. We can view, troubleshoot, and fix bugs very easily when code is on the GitHub repo, with the added bonus of being able to go back a version if something should break.\nSolve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on GitHub, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch."
  },
  {
    "objectID": "08_using_git.html#setting-up-git",
    "href": "08_using_git.html#setting-up-git",
    "title": "Using Git",
    "section": "Setting up Git",
    "text": "Setting up Git\nBefore using git, you need to tell it who you are. The only way to do this is through the command line. When you open RStudio, you should see a Terminal tab located to the right of the Console tab. If the Terminal tab is not visible, you can open it by selecting Tools → Terminal → New Terminal at the top of your RStudio window.\nTo tell git who you are, you’re going to set the global options. This includes setting your name, email address, PAT (Personal Access Token), and caching the PAT. Make sure you run the following commands one at a time.\nType the following command in the Terminal window, with your actual name, and press enter. If you do this correctly, it will look as though nothing happened.\n\ngit config --global user.name \"Your Name\"\n\nNext, enter the following line, with the email address associated with your GitHub account.\n\ngit config --global user.email MyEmail@domain.com\n\nAfter running these commands, the Terminal window should look like this: \nNext, we will tell Git to store your PAT. We must do this because of the way our server operating system handles credentials. If you don’t run the next line, your PAT will expire immediately on the server, even though we will set it up on GitHub to be valid for 90 days.\n\ngit config --global credential.helper 'cache --timeout=10000000'\n\nFinally, check to make sure everything looks correct by entering the following command, which will return the options that you have set.\n\ngit config --global --list"
  },
  {
    "objectID": "08_using_git.html#github-authentication",
    "href": "08_using_git.html#github-authentication",
    "title": "Using Git",
    "section": "GitHub Authentication",
    "text": "GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. That’s where the PAT comes in.\nIn the tabs at the bottom of your RStudio window, make sure to switch from the Terminal to the Console. Now that, you’ve done that, follow these steps:\n\nRun usethis::create_github_token()\nIn the browser window that pops up, log into GitHub\nIn the “Expiration” drop down menu, select “90 days”\nClick “Generate token” in the green box at the bottom of the page\nOn the next page, copy the token from the green box\nBack in RStudio, run credentials::set_github_pat()\nPaste your token into the dialog box that pops up\n\nYou’ve now successfully set your PAT in RStudio, and connected R, Git, and GitHub. Great job! Now, you’re going to clone the repository we work in so that you can version control your code.\n\nCloning the arctic-data repo\nTo clone the arctic-data repository, navigate to the repository on GitHub: https://github.nceas.ucsb.edu/KNB/arctic-data/.\nIf you are not already logged on, this will be the enterprise account you use to log in when you go to RT. If you have trouble with this, send a message to Nick Outin (@Nick) in the datateam slack channel.\nOnce you are logged on and can view the above link, click on the green “Code” button, and copy the URL in the gray box.\nIn your RStudio window, click File → New Project → Version Control → Git. Paste the URL you just copied into the “Repository URL” box, and press tab on your keyboard. This should autofill the “Project directory name.” If it does not, then type in arctic-data. Fill it out as shown in the image below to clone the arctic-data repository into the top level of your home directory. Click “Create Project,” and now you should have a directory called arctic-data in your RStudio files window.\nIf you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save’."
  },
  {
    "objectID": "08_using_git.html#working-with-the-repository",
    "href": "08_using_git.html#working-with-the-repository",
    "title": "Using Git",
    "section": "Working with the repository",
    "text": "Working with the repository\n\nAdding a new script\nIf you have been working on a script that you want to put in the arctic-data GitHub repo, you first need to save it somewhere in the arctic-data folder you cloned to your account on the Datateam server. You can do this by either moving your script into the folder or using the save-as functionality. Note that Git will try and version anything that you save in this folder, so you should be careful about what you save here. For our purposes, things that probably shouldn’t be saved in this folder include:\n\nTokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token in GitHub.\nData files: Git does not version data files very well. You shouldn’t save any .csv files or any other data files (including metadata).\nWorkspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory.\nPlots/Graphics: For the same reasons as data files.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your Git tab, ask for help before committing.\n\n\nAfter you save your script in the appropriate place within the arctic-data folder, it will show up in your Git tab looking like this: \nBefore you commit your changes, you need to click the little box under “Staged”. Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), write your commit message. Always include a commit message. Remember that the commit message should be a concise description of the changes that were made to a file. Your window should look like this: \nPush ‘Commit’, and your commit will be saved to your local repository (this will not push it to the remote repository, yet). Now you want to merge the commits you made with the master version of the remote repository. You do this by using the command “Push.” But before you push, you always need to pull first to avoid merge conflicts. Pulling will merge the current version of the remote repository with your local repository, on your local machine. Click “Pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “Push”.\nAlways remember, the order is commit-pull-push.\n\n\nEditing a script\nIf you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the GitHub website. It is much easier to accidentally overwrite the history of a file this way.\nOne thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done”. Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40”.\nOne final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your Git tab in RStudio that looks like this: \n\n\nWhere do I commit?\nThe default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with sub-folders listed by project. Directories can be created as needed but please ask Dom or Jesse first so we can try and maintain some semblance of order in the file structure.\nThe Github workflow diagram from stamminator shows each step and their functions explicitly."
  },
  {
    "objectID": "08_using_git.html#my-git-tab-disappeared",
    "href": "08_using_git.html#my-git-tab-disappeared",
    "title": "Using Git",
    "section": "My Git tab disappeared",
    "text": "My Git tab disappeared\nSometimes R will crash so hard it loses your project information, causing your Git tab to disappear. Most likely, RStudio has just closed your “project” and all you need to do is reopen it. If your Git tab has disappeared, in the top right of your RStudio session, you should see a little R logo with “Project: (None)” next to it. This means you do not currently have a project open. Clicking the arrow should give you a dropdown menu of recent projects, where you can select “arctic-data” or “sasap-data.” Once you have opened your project, the Git tab should reappear! This is also a convenient way to switch between projects if you are working in multiple repositories."
  },
  {
    "objectID": "03_exploring_eml.html",
    "href": "03_exploring_eml.html",
    "title": "Exploring EML",
    "section": "",
    "text": "We use the Ecological Metadata Language (EML) to store structured metadata for all datasets submitted to the Arctic Data Center. EML is written in XML (extensible markup language) and functions for building and editing EML are in the EML R package.\nCurrently the Arctic Data Center website supports editing EML version 2.2.0. There are still some metadata in 2.1.1 that will be converted eventually.\nFor additional background on EML and principles for metadata creation, check out this paper.\nIf you aren’t too familiar with lists and how to navigate them yet take a look at the relevant sections in the Stat 545 class."
  },
  {
    "objectID": "03_exploring_eml.html#navigate-through-eml",
    "href": "03_exploring_eml.html#navigate-through-eml",
    "title": "Exploring EML",
    "section": "Navigate through EML",
    "text": "Navigate through EML\nThe first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function View allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file.\n\n\n\n\n# Need to be in this member node to explore file\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\n\ndoc <- read_eml(getObject(d1c_test@mn, \n                          \"urn:uuid:558eabf1-1e91-4881-8ba3-ef8684d8f6a1\"))\n\n\nView(doc)\n\n\nThe complex EML document is represented in R as as series of named, nested lists. We use lists all the time in R! A data.frame is one example of a special kind of list that we use all the time. You may be familiar with the syntax dataframe$column_name which allows us to select a particular column of a data.frame. Under the hood, a data.frame is a named list of vectors with the same length. You select one of those vectors using the $ operator, which is called the “list selector operator.”\nJust like you navigate in a data.frame, you can use the $ operator to navigate through the EML structure. The $ operator allows you to go deeper into the EML structure and to see what elements are nested within other elements. However, you have to tell R where you want to go in the structure when you use the $ symbol. For example, if you want to view the dataset element of your EML you would use the command doc$dataset. If you want to view the creators of your data set you would use doc$dataset$creator. Note here that creator is contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing $ and a list of available elements in the structure will appear (e.g., doc$<TAB>):\n\nNote that if you hit tab, and nothing pops up, this most likely implies that you are trying to go into an EML element that can take a series items. For example doc$dataset$creator$<TAB> will not show a pop-up menu. This is because creator is a series-type object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now doc$dataset$creator[[1]]$<TAB> will give you many more options. Note, an empty autocomplete result sometimes means you have reached the end of a branch in the EML structure.\nAt this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering doc$dataset into your console, and print it. Now make the search more specific, for instance: doc$dataset$abstract."
  },
  {
    "objectID": "03_exploring_eml.html#understand-the-eml-schema",
    "href": "03_exploring_eml.html#understand-the-eml-schema",
    "title": "Exploring EML",
    "section": "Understand the EML schema",
    "text": "Understand the EML schema\nAnother great resource for navigating the EML structure is looking at the schema which defines the structure. The schema diagrams on this page are interactive. Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it.\nFor example, let’s take a look at eml-party. To start off, notice that some elements have bolded lines leading to them.\n\nA bold line indicates that the element is required if the element above it (to the left in the schema) is used, otherwise the element is optional.\nNotice also that next to the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname.\nYou will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema.\nThe eml schema sections you may find particularly helpful include eml-party, eml-attribute and eml-physical.\nFor a more detailed description of the EML schema, see the reference section on exploring EML."
  },
  {
    "objectID": "03_exploring_eml.html#access-specific-elements",
    "href": "03_exploring_eml.html#access-specific-elements",
    "title": "Exploring EML",
    "section": "Access specific elements",
    "text": "Access specific elements\nThe eml_get() function is a powerful tool for exploring EML (more on that here ). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples:\n\ndoc <- read_eml(system.file(\"example-eml.xml\", package = \"arcticdatautils\"))\neml_get(doc, \"creator\")\n\nindividualName:\n  givenName: Bryce\n  surName: Mecum\norganizationName: National Center for Ecological Analysis and Synthesis\n\neml_get(doc, \"boundingCoordinates\")\n\neastBoundingCoordinate: '-134'\nnorthBoundingCoordinate: '59'\nsouthBoundingCoordinate: '57'\nwestBoundingCoordinate: '-135'\n\neml_get(doc, \"url\")\n\n'':\n  function: download\n  url: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\n'': ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456\neml_get_simple() is a simplified alternative to eml_get() that produces a list of the desired EML element.\n\neml_get_simple(doc$dataset$otherEntity, \"entityName\")\n\nTo find an eml element you can use either a combination of which_in_emlfrom the arcticdatautils package or eml_get_simple and which to find the index in an EML list. Use which ever workflow you see fit.\nAn example question you may have: Which creators have a surName “Mecum”?\nExample using which_in_eml:\n\nn <- which_in_eml(doc$dataset$creator, \"surName\", \"Mecum\")\n# Answer: doc$dataset$creator[[n]]\n\nExample using eml_get_simple and which:\n\nent_names <- eml_get_simple(doc$dataset$creator, \"surName\")\ni <- which(ent_names == \"Mecum\")\n# Answer: doc$dataset$creator[[i]]"
  },
  {
    "objectID": "05_updating_a_data_package.html",
    "href": "05_updating_a_data_package.html",
    "title": "Updating a data package",
    "section": "",
    "text": "This chapter will teach you how to edit and update an existing data package in R. Earlier, we updated metadata. In this section we will learn how to update a data file, and how to update a package by adding an additional data file."
  },
  {
    "objectID": "05_updating_a_data_package.html#update-a-data-object",
    "href": "05_updating_a_data_package.html#update-a-data-object",
    "title": "Updating a data package",
    "section": "Update a data object",
    "text": "Update a data object\nTo update a data file associated with a data package, you need to do three things:\n\nupdate the object itself,\nupdate the resource map (which affiliates the object with the metadata), and\nupdate the metadata that describes that object\n\nThe datapack::replaceMember function takes care of the first two of these tasks. First you need to get the pid of the file you want to replace by using datapack::selectMember:\n\nmetadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n\nThen use replaceMember:\n\ndp <- replaceMember(dp, metadataId, replacement=file_path)\n\nIf you want to remove some files from the data package we can use datapack::removeMember. If we wanted to remove all the zip files associated with this data package, we can use datapack::removeMember:\n\nzipId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"application/vnd.shp+zip\")\nremoveMember(dp, zipId, removeRelationships = T)\n\n\n\n\n\n\n\nNote\n\n\n\nYou will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character.\n\n\nTo accomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information.\nOnce you have updated your metadata and saved it, you can update the package itself."
  },
  {
    "objectID": "05_updating_a_data_package.html#update-a-package-with-a-new-data-object",
    "href": "05_updating_a_data_package.html#update-a-package-with-a-new-data-object",
    "title": "Updating a data package",
    "section": "Update a package with a new data object",
    "text": "Update a package with a new data object\nOnce you have updated the data objects and saved the metadata to a file, we can update the metadata and use replaceMember to update the package with the new metadata.\nMake sure you have the package you want to update loaded into R using dataone::getDataPackage().\n\nPublish update\nNow we can update your data package to include the new data object. Assuming you have updated your data package earlier something like the below:\n\nd1c_test <- dataone::D1Client(\"STAGING\", \"urn:node:mnTestARCTIC\")\npackageId <- \"the resource map\"\n\ndp <- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE)\nmetadataId <- selectMember(dp, name=\"sysmeta@formatId\", value=\"https://eml.ecoinformatics.org/eml-2.2.0\")\n\n#some modification to the EML here\n\neml_path <- \"path/to/your/saved/eml.xml\"\nwrite_eml(doc, eml_path)\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path)\n\nYou can then upload your data package:\n\nmyAccessRules <- data.frame(subject=\"CN=arctic-data-admins,DC=dataone,DC=org\", permission=\"changePermission\") \npackageId <- uploadDataPackage(d1c_test, dp, public=FALSE, accessRules=myAccessRules, quiet=FALSE)\n\nIf a package is ready to be public, you can change the public argument in the datapack::uploadDataPackage() call to TRUE.\nIf you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you need to do this when replacing the metadata. This should only be done after the package is finalized and has been thoroughly reviewed!\n\ndoi <- dataone::generateIdentifier(d1c_test@mn, \"DOI\")\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=doi)\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\nIf there is a pre-issued DOI (researcher requested the DOI for the publication first), please do the following:\n\ndp <- replaceMember(dp, metadataId, replacement=eml_path, newId=\"your pre-issued doi previously generated\")\n\nnewPackageId <- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE)\n\nIf the package has children, see how to do this using arcticdatautils::publish_update in the nesting section of the reference manual.\nRefresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest."
  },
  {
    "objectID": "05_updating_a_data_package.html#exercise-4",
    "href": "05_updating_a_data_package.html#exercise-4",
    "title": "Updating a data package",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat if the researcher notices that some information needed to be updated in the data file? We can use replaceMember to do just that!\nIf you haven’t already:\n\nLocate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/…\nLoad the package and EML into R using the above commands.\n\nMake a slightly different data file to upload to test.arcticdata.io for this exercise:\n\nLoad the data file associated with the package into R as a data.frame. (Hint: use read.csv() to upload the data file from your computer/the server.)\nMake an edit to the data in R (e.g. change one of the colnames to \"TEST\").\nSave the edited data. (Hint: use write.csv(data, row.names = FALSE).)\n\nUpload the new csv file, get a new pid and publish those updates:\n\nUpdate the data file in the package with the edited data file using replaceMember.\nUpdate your package using uploadDataPackage."
  }
]